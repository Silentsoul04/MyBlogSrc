<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 27 Dec 2017 21:18:43 +0800</pubDate>
    <lastBuildDate>Wed, 27 Dec 2017 21:18:43 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>json_shell</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;解析简单json&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;{\&amp;quot;rv\&amp;quot;:0,\&amp;quot;flag\&amp;quot;:1,\&amp;quot;url\&amp;quot;:\&amp;quot;http://www.jinhill.com\&amp;quot;,\&amp;quot;msg\&amp;quot;:\&amp;quot;test\&amp;quot;}&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;parse_json&lt;span class=&quot;o&quot;&gt;(){&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#echo &amp;quot;$1&amp;quot; | sed &amp;quot;s/.*\&amp;quot;$2\&amp;quot;:\([^,}]*\).*/\1/&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5 &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; sed &lt;span class=&quot;s2&quot;&gt;&amp;quot;s/.*&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$2&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:\([^,}]*\).*/\1/&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6 &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;7 &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$s&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;8 &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;parse_json &lt;span class=&quot;nv&quot;&gt;$s&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;url&amp;quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;9 &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$value&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;解析URL Query&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;http://www.zonetec.cn/WlanAuth/portal.do?appid=aaaa&amp;amp;apidx=0&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;parse&lt;span class=&quot;o&quot;&gt;(){&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; sed &lt;span class=&quot;s1&quot;&gt;&amp;#39;s/.*&amp;#39;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$2&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;=\([[:alnum:]]*\).*/\1/&amp;#39;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5 &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6 &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;parse &lt;span class=&quot;nv&quot;&gt;$s&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;appid&amp;quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;7 &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$value&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Wed, 27 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/12/27/json_shell.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/12/27/json_shell.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>ioctl</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;ioctl函数详细说明（网络）
ioctl 函数
本函数影响由fd 参数引用的一个打开的文件。
 #include&lt;unistd.h&gt;&lt;/unistd.h&gt;&lt;/p&gt;

&lt;p&gt;int ioctl( int fd, int request, …/* void *arg */ );&lt;/p&gt;

&lt;p&gt;返回0 ：成功    -1 ：出错&lt;/p&gt;

&lt;p&gt;第三个参数总是一个指针，但指针的类型依赖于request 参数。&lt;/p&gt;

&lt;p&gt;我们可以把和网络相关的请求划分为6 类：&lt;/p&gt;

&lt;p&gt;套接口操作&lt;/p&gt;

&lt;p&gt;文件操作&lt;/p&gt;

&lt;p&gt;接口操作&lt;/p&gt;

&lt;p&gt;ARP 高速缓存操作&lt;/p&gt;

&lt;p&gt;路由表操作&lt;/p&gt;

&lt;p&gt;流系统&lt;/p&gt;

&lt;p&gt;下表列出了网络相关ioctl 请求的request 参数以及arg 地址必须指向的数据类型：&lt;/p&gt;

&lt;p&gt;类别&lt;/p&gt;

&lt;p&gt;Request&lt;/p&gt;

&lt;p&gt;说明&lt;/p&gt;

&lt;p&gt;数据类型&lt;/p&gt;

&lt;p&gt;套&lt;/p&gt;

&lt;p&gt;接&lt;/p&gt;

&lt;p&gt;口&lt;/p&gt;

&lt;p&gt;SIOCATMARK&lt;/p&gt;

&lt;p&gt;SIOCSPGRP&lt;/p&gt;

&lt;p&gt;SIOCGPGRP&lt;/p&gt;

&lt;p&gt;是否位于带外标记&lt;/p&gt;

&lt;p&gt;设置套接口的进程ID 或进程组ID&lt;/p&gt;

&lt;p&gt;获取套接口的进程ID 或进程组ID&lt;/p&gt;

&lt;p&gt;int&lt;/p&gt;

&lt;p&gt;int&lt;/p&gt;

&lt;p&gt;int&lt;/p&gt;

&lt;p&gt;文&lt;/p&gt;

&lt;p&gt;件&lt;/p&gt;

&lt;p&gt;FIONBIN&lt;/p&gt;

&lt;p&gt;FIOASYNC&lt;/p&gt;

&lt;p&gt;FIONREAD&lt;/p&gt;

&lt;p&gt;FIOSETOWN&lt;/p&gt;

&lt;p&gt;FIOGETOWN&lt;/p&gt;

&lt;p&gt;设置/ 清除非阻塞I/O 标志&lt;/p&gt;

&lt;p&gt;设置/ 清除信号驱动异步I/O 标志&lt;/p&gt;

&lt;p&gt;获取接收缓存区中的字节数&lt;/p&gt;

&lt;p&gt;设置文件的进程ID 或进程组ID&lt;/p&gt;

&lt;p&gt;获取文件的进程ID 或进程组ID&lt;/p&gt;

&lt;p&gt;int&lt;/p&gt;

&lt;p&gt;int&lt;/p&gt;

&lt;p&gt;int&lt;/p&gt;

&lt;p&gt;int&lt;/p&gt;

&lt;p&gt;int&lt;/p&gt;

&lt;p&gt;接&lt;/p&gt;

&lt;p&gt;口&lt;/p&gt;

&lt;p&gt;SIOCGIFCONF&lt;/p&gt;

&lt;p&gt;SIOCSIFADDR&lt;/p&gt;

&lt;p&gt;SIOCGIFADDR&lt;/p&gt;

&lt;p&gt;SIOCSIFFLAGS&lt;/p&gt;

&lt;p&gt;SIOCGIFFLAGS&lt;/p&gt;

&lt;p&gt;SIOCSIFDSTADDR&lt;/p&gt;

&lt;p&gt;SIOCGIFDSTADDR&lt;/p&gt;

&lt;p&gt;SIOCGIFBRDADDR&lt;/p&gt;

&lt;p&gt;SIOCSIFBRDADDR&lt;/p&gt;

&lt;p&gt;SIOCGIFNETMASK&lt;/p&gt;

&lt;p&gt;SIOCSIFNETMASK&lt;/p&gt;

&lt;p&gt;SIOCGIFMETRIC&lt;/p&gt;

&lt;p&gt;SIOCSIFMETRIC&lt;/p&gt;

&lt;p&gt;SIOCGIFMTU&lt;/p&gt;

&lt;p&gt;SIOCxxx&lt;/p&gt;

&lt;p&gt;获取所有接口的清单&lt;/p&gt;

&lt;p&gt;设置接口地址&lt;/p&gt;

&lt;p&gt;获取接口地址&lt;/p&gt;

&lt;p&gt;设置接口标志&lt;/p&gt;

&lt;p&gt;获取接口标志&lt;/p&gt;

&lt;p&gt;设置点到点地址&lt;/p&gt;

&lt;p&gt;获取点到点地址&lt;/p&gt;

&lt;p&gt;获取广播地址&lt;/p&gt;

&lt;p&gt;设置广播地址&lt;/p&gt;

&lt;p&gt;获取子网掩码&lt;/p&gt;

&lt;p&gt;设置子网掩码&lt;/p&gt;

&lt;p&gt;获取接口的测度&lt;/p&gt;

&lt;p&gt;设置接口的测度&lt;/p&gt;

&lt;p&gt;获取接口MTU&lt;/p&gt;

&lt;p&gt;（还有很多取决于系统的实现）&lt;/p&gt;

&lt;p&gt;struct ifconf&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;struct ifreq&lt;/p&gt;

&lt;p&gt;ARP&lt;/p&gt;

&lt;p&gt;SIOCSARP&lt;/p&gt;

&lt;p&gt;SIOCGARP&lt;/p&gt;

&lt;p&gt;SIOCDARP&lt;/p&gt;

&lt;p&gt;创建/ 修改ARP 表项&lt;/p&gt;

&lt;p&gt;获取ARP 表项&lt;/p&gt;

&lt;p&gt;删除ARP 表项&lt;/p&gt;

&lt;p&gt;struct arpreq&lt;/p&gt;

&lt;p&gt;struct arpreq&lt;/p&gt;

&lt;p&gt;struct arpreq&lt;/p&gt;

&lt;p&gt;路&lt;/p&gt;

&lt;p&gt;由&lt;/p&gt;

&lt;p&gt;SIOCADDRT&lt;/p&gt;

&lt;p&gt;SIOCDELRT&lt;/p&gt;

&lt;p&gt;增加路径&lt;/p&gt;

&lt;p&gt;删除路径&lt;/p&gt;

&lt;p&gt;struct rtentry&lt;/p&gt;

&lt;p&gt;struct rtentry&lt;/p&gt;

&lt;p&gt;流&lt;/p&gt;

&lt;p&gt;I_xxx&lt;/p&gt;

&lt;p&gt;套接口操作：&lt;/p&gt;

&lt;p&gt;明确用于套接口操作的ioctl 请求有三个, 它们都要求ioctl 的第三个参数是指向某个整数的一个指针。&lt;/p&gt;

&lt;p&gt;SIOCATMARK:    如果本套接口的的度指针当前位于带外标记，那就通过由第三个参数指向的整数返回一个非0 值；否则返回一个0 值。POSIX 以函数sockatmark 替换本请求。&lt;/p&gt;

&lt;p&gt;SIOCGPGRP ：       通过第三个参数指向的整数返回本套接口的进程ID 或进程组ID ，该ID 指定针对本套接口的SIGIO 或SIGURG 信号的接收进程。本请求和fcntl 的F_GETOWN 命令等效，POSIX 标准化的是fcntl 函数。&lt;/p&gt;

&lt;p&gt;SIOCSPGRP ：     把本套接口的进程ID 或者进程组ID 设置成第三个参数指向的整数，该ID 指定针对本套接口的SIGIO 或SIGURG 信号的接收进程，本请求和fcntl 的F_SETOWN 命令等效，POSIX 标准化的是fcntl 操作。&lt;/p&gt;

&lt;p&gt;文件操作：&lt;/p&gt;

&lt;p&gt;以下5 个请求都要求ioctl 的第三个参数指向一个整数。&lt;/p&gt;

&lt;p&gt;FIONBIO ：        根据ioctl 的第三个参数指向一个0 或非0 值分别清除或设置本套接口的非阻塞标志。本请求和O_NONBLOCK 文件状态标志等效，而该标志通过fcntl 的F_SETFL 命令清除或设置。&lt;/p&gt;

&lt;p&gt;FIOASYNC ：      根据iocl 的第三个参数指向一个0 值或非0 值分别清除或设置针对本套接口的信号驱动异步I/O 标志，它决定是否收取针对本套接口的异步I/O 信号（SIGIO ）。本请求和O_ASYNC 文件状态标志等效，而该标志可以通过fcntl 的F_SETFL 命令清除或设置。&lt;/p&gt;

&lt;p&gt;FIONREAD ：     通过由ioctl 的第三个参数指向的整数返回当前在本套接口接收缓冲区中的字节数。本特性同样适用于文件，管道和终端。&lt;/p&gt;

&lt;p&gt;FIOSETOWN ：    对于套接口和SIOCSPGRP 等效。&lt;/p&gt;

&lt;p&gt;FIOGETOWN ：    对于套接口和SIOCGPGRP 等效。&lt;/p&gt;

&lt;p&gt;接口配置：&lt;/p&gt;

&lt;p&gt;得到系统中所有接口由SIOCGIFCONF 请求完成，该请求使用ifconf 结构，ifconf 又使用ifreq&lt;/p&gt;

&lt;p&gt;结构，如下所示：&lt;/p&gt;

&lt;p&gt;Struct ifconf{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int ifc_len;                 // 缓冲区的大小

union{

    caddr_t ifcu_buf;        // input from user-&amp;gt;kernel

    struct ifreq *ifcu_req;    // return of structures returned

}ifc_ifcu;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;};&lt;/p&gt;

&lt;p&gt;#define  ifc_buf  ifc_ifcu.ifcu_buf    //buffer address&lt;/p&gt;

&lt;p&gt;#define  ifc_req  ifc_ifcu.ifcu_req    //array of structures returned
 #define  IFNAMSIZ  16
struct ifreq{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char ifr_name[IFNAMSIZ];           // interface name, e.g., “le0”

union{

    struct sockaddr ifru_addr;

    struct sockaddr ifru_dstaddr;

    struct sockaddr ifru_broadaddr;

    short ifru_flags;

    int ifru_metric;

    caddr_t ifru_data;

}ifr_ifru;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;};&lt;/p&gt;

&lt;p&gt;#define ifr_addr     ifr_ifru.ifru_addr            // address
 #define ifr_dstaddr   ifr_ifru.ifru_dstaddr         // otner end of p-to-p link&lt;/p&gt;

&lt;p&gt;#define ifr_broadaddr ifr_ifru.ifru_broadaddr    // broadcast address&lt;/p&gt;

&lt;p&gt;#define ifr_flags     ifr_ifru.ifru_flags        // flags&lt;/p&gt;

&lt;p&gt;#define ifr_metric    ifr_ifru.ifru_metric      // metric&lt;/p&gt;

&lt;p&gt;#define ifr_data      ifr_ifru.ifru_data        // for use by interface&lt;/p&gt;

&lt;p&gt;再调用ioctl 前我们必须先分撇一个缓冲区和一个ifconf 结构，然后才初始化后者。如下图&lt;/p&gt;

&lt;p&gt;展示了一个ifconf 结构的初始化结构，其中缓冲区的大小为1024 ，ioctl 的第三个参数指向&lt;/p&gt;

&lt;p&gt;这样一个ifconf 结构。&lt;/p&gt;

&lt;p&gt;ifc_len&lt;/p&gt;

&lt;p&gt;Ifc_buf&lt;/p&gt;

&lt;p&gt;1024&lt;/p&gt;

&lt;p&gt;———————&amp;gt; 缓存&lt;/p&gt;

&lt;p&gt;假设内核返回2 个ifreq 结构，ioctl 返回时通过同一个ifconf 结构缓冲区填入了那2 个ifreq 结构，ifconf 结构的ifc_len 成员也被更新，以反映存放在缓冲区中的信息量&lt;/p&gt;

&lt;p&gt;一般来讲ioctl在用户程序中的调用是：
ioctl(int fd,int command, (char*)argstruct)
ioctl调用与网络编程有关（本文只讨论这一点），文件描述符fd实际上是由socket()系统调用返回的。参数command的取值由/usr/include/linux/sockios.h 所规定。这些command的由于功能的不同，可分为以下几个小类：
• 改变路由表 (例如 SIOCADDRT, SIOCDELRT), 
• 读/更新 ARP/RARP 缓存(如：SIOCDARP, SIOCSRARP), 
• 一般的与网络接口有关的(例如 SIOCGIFNAME, SIOCSIFADDR 等等) 
在 Gooodies目录下有很多样例程序展示了如何使用ioctl。当你看这些程序时，注意参数argstruct是与参数command相关的。例如，与 路由表相关的ioctl使用rtentry这种结构，rtentry定义在/usr/include/linux/route.h（参见例子 adddefault.c）。与ARP有关的ioctl调用使用arpreq结构，arpreq定义在/usr/include/linux /if_arp.h（参见例子arpread.c）
与网络接口有关的ioctl调用使用的command参数通常看起来像SIOCxIFyyyy的形式，这里x要 么是S（设定set，写write），要么是G（得到get，读read）。在getifinfo.c程序中就使用了这种形式的command参数来读 IP地址，硬件地址，广播地址和得到与网络接口有关的一些标志（flag）。在这些ioctl调用中，第三个参数是ifreq结构，它在/usr /include/linux/if.h中定义。在某些情况下， ioctrl调用可能会使用到在sockios.h之外的新的定义。&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/27/ioctl.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/27/ioctl.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>inetd</title>
        <description>&lt;p&gt;inetd是监视一些网络请求的守护进程，其根据网络请求来调用相应的服务进程来处理连接请求。它可以为多种服务管理连接，当 inetd 接到连接时，它能够确定连接所需的程序，启动相应的进程，并把 socket 交给它 （服务 socket 会作为程序的标准输入、 输出和错误输出描述符）。 使用 inetd 来运行那些负载不重的服务有助于降低系统负载，因为它不需要为每个服务都启动独立的服务程序。
&lt;!-- more --&gt;
inetd是通过rc系统启动的。inetd_enable选项默认设为NO，但可以在安装系统时，由用户根据需要sysinstall通过来打开。
inetd.conf则是inetd的配置文件。inetd.conf文件告诉inetd监听哪些网络端口，为每个端口启动哪个服务。在任何的网络环境中使用Linux系统，第一件要做的事就是了解一下服务器到底要提供哪些服务。不需要的那些服务应该被禁止掉，最好卸载掉，这样黑客就少了一些攻击系统的机会。查看“/etc/inetd.conf”文件，了解一下inetd提供哪些服务。用加上注释的方法（在一行的开头加上#号），禁止任何不需要的服务，再给inetd进程发一个SIGHUP信号。&lt;/p&gt;

&lt;p&gt;从理论上说，UNIX® 是内核，或者说低层软件，它控制对文件系统、内存和处理器等计算机资源的访问。但是，用更通俗的话来说，UNIX 是指在操作系统上运行的一整套软件。实际上，通常说的 “它是一台 UNIX 机器” 是指系统的基础功能：UNIX 机器通常提供 shell 界面、并行访问、强大的安全性和各种连网的服务。
实际上，UNIX（内核等）被选用的原因通常是它的连网应用。FTP、POP、SMTP 和 HTTP 最初都是在 UNIX 上实现的，而且一直在 UNIX 上使用。UNIX 系统还通过运行服务（常常称为守护进程 ）实现各种功能，包括与中心时钟执行同步（网络时间协议）、交换新闻（网络新闻传输协议）、把主机名解析为 IP 地址（DNS）等。在大多数 UNIX 机器上的 /etc/services 中可以找到常用的一部分服务。这个文件与 清单 1 相似。&lt;/p&gt;

&lt;p&gt;清单 1. /etc/services（UNIX 网络服务目录）中常见的条目
ftp             21/tcp
fsp             21/udp          fspd
ssh             22/tcp                       &lt;br /&gt;
ssh             22/udp
telnet          23/tcp
smtp            25/tcp          mail
/etc/services 中的每个条目列出服务的名称；服务使用的端口号和协议（TCP 或 UDP）；服务的别名（可能没有，也可能有多个别名）。每个系统守护进程都通过检查 /etc/services 寻找它提供服务时要使用的端口和协议。&lt;/p&gt;

&lt;p&gt;例如，处理入站电子邮件的守护进程会寻找 “smtp”（服务名称）或 “mail”（别名之一），在端口 25 上监听到达的 TCP 连接。类似地，远程登录守护进程在文件中搜索 “ssh”，在端口 22 上监听到达的 TCP 连接。&lt;/p&gt;

&lt;p&gt;小公司的服务器可能运行多个服务，分别负责与世界时钟同步、提供 Web 页面、传输电子邮件、支持远程 shell 访问、打印页面、传输文件、连接数据库、监视系统的稳定性、提供域名以及通过 NFS 共享文件。这种配置并不少见，这主要是因为守护进程的开销不大。守护进程通常设计为在空闲时休眠，等待请求。当服务请求出现时，守护进程醒来，响应并处理请求，然后继续休眠。&lt;/p&gt;

&lt;p&gt;尽管如此，大量休眠的进程仍然会影响系统性能。因此，如果预期会经常请求某一服务，比如有稳定的 Web 访问请求，那么有必要具有一个长期运行的守护进程。否则，最好把守护进程重新配置为根据需要执行。&lt;/p&gt;

&lt;p&gt;但是，系统如何提供随时可用的服务并在需要的时候启动？解决方案是使用代理服务，它预测到达的各种请求，根据后续处理的需要启动适当的服务。在 UNIX 和 Linux® 系统上，这个代理称为 inetd。
给定一个服务列表，inetd 会监视对这些服务的端口和协议的请求。当发生活动时，inetd 把入站请求映射到标准输入 (stdin)、标准输出 (stdout) 和标准错误 (stderr)，并启动适当的守护进程。服务处理数据并终止。inetd 把资源消耗保持在最低水平，并且让守护进程更容易编写。&lt;/p&gt;

&lt;p&gt;inetd 监听许多端口并在接收到请求时启动服务。服务处理请求并退出。有一些服务例外。例如，传输电子邮件的 SMTP 服务器通常独立地运行。&lt;/p&gt;

&lt;p&gt;根据它的作用，inetd 常常被称为 “超级服务员”。在近几年，inetd 已经被它的变体 xinetd 替代了。这两个软件的用途是相同的，但是后者更安全并提供许多特性，可以在系统负载过重时限制访问。inetd 和 xinetd 的配置相似，但是不完全相同。系统可以运行 inetd 或 xinetd，但是不能同时运行两者。因为后者更安全，它是首选的，所以本文后面一直使用它。&lt;/p&gt;

&lt;p&gt;xinetd 是开放源码的，很容易构建在 UNIX 以及 OpenBSD 和 Linux 等变体上。到 2009 年 10 月底，xinetd 的最新版本是 2.3.14，可以从 xinetd 主页获取它（参见 参考资料）。下载 xinetd 的源代码之后，解压压缩文件，运行配置脚本）并构建软件。在安装 xinetd 之前，一定要备份 inetd 配置（如果有的话），然后禁用和/或删除 inetd。禁用 inetd 的步骤取决于使用的 UNIX 变体；参见系统的 inetd 手册页。执行这个修改很可能需要超级用户访问权。&lt;/p&gt;

&lt;p&gt;无论如何安装和启用 xinetd，如果以前运行过 inetd，就必须把 inetd 配置文件 inetd.conf 转换为与 xinetd 兼容的文件。可以手工地执行转换，也可以使用 xinetd 提供的转换脚本替您修改文件：&lt;/p&gt;

&lt;p&gt;1
2
$ xconv.pl &amp;lt; /etc/inetd.conf &amp;gt; /etc/xinetd.conf
$ mv /etc/inetd.conf /etc/inetd.conf.sav
Xconv.pl 是 xinetd 提供的 Perl 脚本。后一个步骤（把 inetd 配置文件转移到标准位置之外）只是一项预防措施。&lt;/p&gt;

&lt;p&gt;可以完全在 /etc/xinetd.conf 中配置 xinetd。但是，按照惯例，通常在这个文件中提供默认设置，并在特殊目录 /etc/xinetd.d 中包含多个配置文件 — 每个服务一个文件。例如，下面是 Ubuntu 上安装的 xinetd 配置文件：&lt;/p&gt;

&lt;p&gt;1
2
3
4
5
6
defaults
{
    log_type = SYSLOG daemon info
}&lt;/p&gt;

&lt;p&gt;includedir /etc/xinetd.d
defaults 提供 xinetd 控制的所有 服务的值。服务可以覆盖这些全局默认值。在这里，log_type 的默认值指定每个守护进程应该把日志条目发送到哪里（如果启用日志的话）。SYSLOG 选项把输出发送到 syslog（中心系统日志）。info 要求只记录信息性消息。其他值包括 emerg、alert、crit、err、warning、notice 和 debug。第一个值 emerg 从 xinetd 生成最少的输出；最后一个值 debug 提供最详细的输出。如果在从 xinetd 启动某个服务时遇到了问题，可以启用更详细的日志选项以帮助判断问题的原因。&lt;/p&gt;

&lt;p&gt;/etc/xinetd.d 中的文件采用与 xinetd.conf 相同的格式。其中有一个操作，包含零个、一个或更多操作数，还有一组放在大括号 ({}) 中的变量和值。例如，清单 3 是 /etc/xinetd.d/imap，这是用于 IMAP 服务的条目。（IMAP 是用于读取和管理电子邮件的邮箱协议。它与 POP 相比有一个重要的优点：IMAP 邮箱可以跨任意数量的系统保持同步。）&lt;/p&gt;

&lt;p&gt;虽然有一些小差异，但是这个片段看起来应该很熟悉。这个脚本作为用户 martin 运行，因为它不需要特殊的特权。一般来说，应该提供尽可能少的特权 — 不仅是在这里，在授予对任何系统资源的访问权时都应该这样。对于 TCP 协议服务，必须设置 wait=no。server 指向要运行的脚本或可执行程序，log_type 指定更高的日志记录级别，这有助于解决服务中的任何问题。&lt;/p&gt;

&lt;p&gt;重新启动 xinetd，或者向它的进程发送一个重新设置信号。要想重新启动 xinetd，应该在 /etc/init.d 或系统保存启动脚本的地方寻找控制脚本。运行下面这样的命令：&lt;/p&gt;

&lt;p&gt;$ sudo /etc/init.d/xinetd restart
另一种方法是向 xinetd 守护进程发送重新设置信号。信号 SIGHUP 让 xinetd 重新读取它的配置，并且根据新的参数，可能会关闭连接。使用的命令是：&lt;/p&gt;

&lt;p&gt;$ sudo pkill -SIGHUP xinetd
如果系统没有 pkill（它根据进程名寻找进程 ID），那么使用 ps aux | grep xinetd 寻找进程号，然后使用 sudo kill -SIGHUP pid，其中的 pid 是进程 ID。&lt;/p&gt;

&lt;p&gt;为了测试这个新服务，创建一个名为 /tmp/xinetd 的目录，创建 Ruby 脚本并把它保存在 /tmp/xinetd/find.rb 中。用 chmod +x /tmp/xinetd/find.rb 把这个文件设置为可执行的。接下来，创建一些目录和文本文件：&lt;/p&gt;

&lt;p&gt;$ mkdir a b c
$ touch a/d.txt b/e.txt
现在可以测试新服务。当端口 11000 上出现入站连接时，xinetd 启动 Ruby 脚本。发送到标准输出的任何脚本输出会被发送到发出请求的机器上的标准输出。这个脚本不需要输入，但是如果需要，发出请求的机器上的标准输入会被传递给脚本。Telnet 提供一种连接任何服务的简便方法：&lt;/p&gt;

&lt;p&gt;$ telnet localhost 11000
Trying 127.0.0.1…
Connected to localhost.
Escape character is ‘^]’.
/tmp/xinetd/b/e.txt
/tmp/xinetd/a/d.txt
Connection closed by foreign host.
成功了！端口打开了，控制被传递给脚本，脚本生成了预期的输出。&lt;/p&gt;

&lt;p&gt;运行 xinetd 的更多原因
xinetd 有许多优点。它只在需要时运行守护进程，这可以节省资源。它提供一个额外的安全层，可以通过 “修改根目录” 把服务隔离在一个目录中。最重要的是，它实际上可以把任何脚本或程序转换为服务。但是要注意一点：如果您的服务非常受欢迎，应该考虑用 C 等高效的语言重写它。处理请求越快，性能就越好。&lt;/p&gt;

&lt;p&gt;inetd 是一个守护程序，通过一个集中的配置文件（inetd.conf）来管理大多数入网连接。xinetd 守护程序是 inetd 的替代，它提供许多改进的或新的特性，以及更容易的配置。Ted 解释了 inetd 背后的概念，并且给出了在您自己的站点上设置 xinetd 的示例。
经典的 inetd 守护程序已经存在很久了。有几种替换 inetd 的功能的方法，但是最灵活、最简便的方法似乎是 xinetd。inetd 能做的，xinetd 也能做，并且 xinetd 还能做更多的事情。譬如，TCP 封装、模块化配置、连接重定向和入站连接的负载限制，而这些只是使得 xinetd 成为系统管理员良好选择的部分特性。&lt;/p&gt;

&lt;p&gt;本文是为从初学者到中级系统管理员这样的读者而准备的，并且文中的说明和示例并不尝试假设您已经熟悉 inetd。在本文中，我们将研究 xinetd 的一些简单用法，从安装到安全性策略的实现。&lt;/p&gt;

&lt;p&gt;开始之前
为实现本文的目的，您的系统最好安装了最近的主流（2000 或更新）UNIX（Linux、Solaris、BSD）。这些示例在 Perl 和 UNIX（以及其它操作系统）的早期版本上也可以运行，但是它们功能方面的障碍应该由读者作为练习来解决。给定的特定示例是用于 Red Hat Linux 的，但是它们在其它系统上应该也可以运行（除 chkconfig 以外）。&lt;/p&gt;

&lt;p&gt;inetd 到底是什么
对于 UNIX 系统管理员，inetd 和 cp/rm/mv 命令一样基本。它总是存在，并准备着处理入站连接。但它到底是什么？它用来做什么？&lt;/p&gt;

&lt;p&gt;首先从 TCP/IP （它也包括 UDP，但我们目前还不考虑）开始回答。当您建立与一台主机的连接时，实际上是创建了一个 TCP/IP 连接（通常是一个套接字） — 这好象是在您和主机之间打了一个电话。TCP/IP 连接由起始主机和接收主机唯一地定义，但还有其它标识。如果我们都连接到一台服务器，它如何区分 webserver、telnet、SSH、FTP 和其它连接呢？套接字也通过建立连接所使用的端口来定义。例如，端口 21 是入站 FTP、端口 22 是 SSH、端口 23 是 TELNET（有关其它大多数端口，可以查看 UNIX 系统上的 /etc/services）。&lt;/p&gt;

&lt;p&gt;一旦建立了连接，某人就在另一端拿起了电话。这可以是接线员或直线。直线表示您直接连接到了服务器，而接线员是涉及 inetd 的方法。接线员实际上处理一组入站直线（主机上的端口），并亲自将它们交给负责的程序（服务器）。&lt;/p&gt;

&lt;p&gt;UDP 是另一种连接方法。象 TCP 一样，UDP 基本上是和某人的对话，但是不保证它是可靠的。UDP（继续使用电话的比喻）就象将消息扔到传送带上，让接收者站到另一端。您可以从传送带得到许多消息，但是如果消息太多（网络流量高）或者读取消息费时太久（服务器忙），则接收者可能会丢失一些消息。&lt;/p&gt;

&lt;p&gt;如果使用 inetd，在执行一些检查后，您被重定向到特定服务器。只有一个配置文件 — inetd.conf，管理所有入站连接。因而在系统上添加、删除、更改或复查服务变得更为简单。例如，在 Solaris 系统上使用 TCP 封装器将 ftp 定义如下：&lt;/p&gt;

&lt;p&gt;清单 1，FTP 服务的 inetd.conf 定义 ftp stream tcp nowait root /usr/sbin/tcpd in.ftpd&lt;/p&gt;

&lt;p&gt;这些是创建一个 FTP 连接所需的全部参数。简单地说，我们以面向流（stream）的方式使用 TCP/IP（tcp）时，同时允许多个 FTP 连接（nowait）、作为 root 运行以及调用 FTP（接下来，TCP 封装器将调用 FTP 守护程序）。&lt;/p&gt;

&lt;p&gt;用一上午的时间解析很困难吗？绝对困难。有必要这么复杂吗？不。xinetd 继承了 inetd 的设计并将它模块化，这意味着每个服务都可以存在于它自己的配置文件中。xinetd 还添加了一些象 TCP 封装器之类的功能部件，使得配置更加简单。&lt;/p&gt;

&lt;p&gt;xinetd 保持了中央配置（接线员）方法，将所有配置文件存储到单一位置，通常是 /etc/xinetd.conf 和 /etc/xinetd.d/*，使系统管理员可以更容易地获得。模块化配置意味着，您可以通过将服务复制到 xinetd.d 目录来向多台机器上分发该服务，也可以用同类的手段除去它。甚至可以指定额外的包含目录。&lt;/p&gt;

&lt;p&gt;最后，xinetd FAQ（请参阅本文后面的参考资料）声明了 RPC 程序在 xinetd 下运行得不太好。不过没问题，对 RPC 使用 inetd，并对其它所有服务使用 xinetd。这就象雇了两个接线员，一个说西班牙语，另一个说所有其它语言。&lt;/p&gt;

&lt;p&gt;xinetd 简介
那么 xinetd 是什么？一句话，它就是个程序。处理入站网络连接没什么神奇。可以使用 Perl、Python 或 Java 来处理。Xinetd 是用 C 编写的，而且它和它的前辈 inetd 一样快，如果不是更快的话（例如，TCP 封装器不必为每个入站连接而执行；它们在启动时装入内存）。&lt;/p&gt;

&lt;p&gt;xinetd 正在开发中。（您的版本可能过时了，所以请务必到主页上查找最新的版本；请参阅参考资料。）因为它正在开发中，所以 xinetd 的安全漏洞得以迅速弥补，而不象 inetd 那样薄弱，通常要很长时间才能弥补。当然，xinetd 是随源代码一起交付的，所以您可以复查源代码并自己找到可能存在弱点的地方。&lt;/p&gt;

&lt;p&gt;如何使用 xinetd 定义服务呢？编写一个服务文件，它除了指定 /etc/xinetd.conf 中所指定的一般参数之外，还指定特定配置。所以，如果 /etc/xinetd.conf 是这样的：&lt;/p&gt;

&lt;p&gt;清单 2，样本 xinetd.conf（标准的 Red Hat 7.1） defaults
{
instances = 60
log_type = SYSLOG authpriv
log_on_success = HOST PID
log_on_failure = HOST
cps = 25 30
}&lt;/p&gt;

&lt;p&gt;service telnet
{
flags = REUSE
socket_type = stream 
wait = no
user = root
server = /usr/sbin/in.telnetd
log_on_failure += USERID
disable = yes
}&lt;/p&gt;

&lt;p&gt;includedir /etc/xinetd.d&lt;/p&gt;

&lt;p&gt;您放到 /etc/xinetd.d 中的每个服务文件都会继承这些缺省值，并指定它自己的参数。这里，telnet 服务在顶级定义，而不是在子目录中定义。这太棒了，这种模块性允许复杂的配置。&lt;/p&gt;

&lt;p&gt;要使 xinetd 重新读取配置文件，不必重新启动它。只要向它发送 USR2 信号即可。&lt;/p&gt;

&lt;p&gt;那些参数表示什么意思？让我们通读整个清单。您也可以在命令行下使用 man xinetd.conf 来查看列表（如果那个帮助页面正确安装的话），但这个概述试图用更简单的术语来解释参数，并不假定您已经知道关于套接字和服务的所有信息。一些参数（rpc_version、rpc_number）被跳过。&lt;/p&gt;

&lt;p&gt;常规参数&lt;/p&gt;

&lt;p&gt;id 
该服务的唯一名称。服务名称在花括号之前指定，但是 ID 使逻辑上相同的服务可能拥有多个协议。这是对于临时用户的受限使用。例如，NFS 服务可以在 UDP 或 TCP 传输协议上运行。在 Red Hat Linux 7.1 上，TCP 版本（在 /etc/xinetd.d/time 中）和 UDP 版本（在 /etc/xinetd.d/time-udp中）中提供了对于 xinetd 来说内部的时间服务。&lt;/p&gt;

&lt;p&gt;type 
这实际上应该称为“特殊类型”，因为它只适用于特殊服务。它可以是以下几种类型的组合：“RPC”，用于 RPC 服务（由 SUN 引入的远程过程调用，导致了很多安全性问题，最好避免使用）；“INTERNAL”，用于构建到 xinetd 内部的服务，譬如时间服务；“UNLISTED”，用于在系统列表（/etc/services 或用于 RPC 服务的 /etc/rpc）中找不到的非标准服务。&lt;/p&gt;

&lt;p&gt;flags 
这里放置着所有额外标志。列表很长并且技术性很强；我们感兴趣的标志包括 REUSE（用于套接字重用，譬如 telnet）、NAMEINARGS/NOLIBWRAP（如果您希望手工调用 TCP 封装器或者完全地避免使用封装器）、NODELAY/KEEPALIVE（用于调整 TCP 套接字）、DISABLE（覆盖顶级“disable”参数）以及 SENSOR（用于检测和防止某些类型的“拒绝服务（denial-of-service）”网络攻击）。&lt;/p&gt;

&lt;p&gt;disable 
除非您希望禁用某项服务，否则总是把它设成“no”。Red Hat Linux 的 chkconfig 程序将为您打开或关闭“disable”参数；在 Red Hat 上，用 chkconfig 启用和禁用特定服务可能比手工方式简单些。请注意，chkconfig 预期在 /etc/xinetd.d/SERVICE 中找到服务文件。所以对于上面清单 2 中的示例，chkconfig 将不会在请求时打开或关闭 telnet。可以将它认为是一个错误或特性，取决于您的观点。&lt;/p&gt;

&lt;p&gt;socket_type 
通常您希望这个参数设置成“stream”，除非使用 UDP 服务，此时设置成“dgram”。该参数也可以设置成“raw”和“seqpacket”，但极少见。&lt;/p&gt;

&lt;p&gt;protocol 
这是连接所用的协议，通常是“tcp”或“udp”，但是在理论上您可以使用来自 /etc/protocols 的任何值。&lt;/p&gt;

&lt;p&gt;wait 
如果设置成“no”，xinetd 将为每个连接上的服务启动一个新的处理程序。如果是“yes”，xinetd 预期该处理程序处理所有后续连接直到它死亡。在大多数情况下，这个参数是“no”。&lt;/p&gt;

&lt;p&gt;server, server_args 
处理程序的程序名，以及它应当获得的参数。处理程序名不应该象在 inetd 环境下那样，出现在参数中。&lt;/p&gt;

&lt;p&gt;port 
服务的端口。通常不需要，因为端口通过 /etc/services 文件来映射到服务。&lt;/p&gt;

&lt;p&gt;redirect 
允许 xinetd 将所有服务的流量发送给另一台主机。因此，受防火墙保护的主机可以通过中央 xinetd 转发器接受安全流量，而不必建立与外部网络的连接。在某些工作中，可以采用这个特征来在两台主机间执行故障转移服务。&lt;/p&gt;

&lt;p&gt;banner, banner_success, banner_fail 
一个将要在“任意/一个成功/一个不成功”连接上打印的来自文件的定制文本块。&lt;/p&gt;

&lt;p&gt;enabled 
在全局级别上补充“disabled”参数和 DISABLE 标志。&lt;/p&gt;

&lt;p&gt;include, includedir 
告诉 xinetd 要包含文件或目录。&lt;/p&gt;

&lt;p&gt;环境参数&lt;/p&gt;

&lt;p&gt;user, group, umask, groups 
当启动服务处理程序时，xinetd 应该扮演的 UNIX 属性。这主要用于非安全服务。&lt;/p&gt;

&lt;p&gt;nice 
确定该服务对于系统有多重要的 UNIX 优先级级别。可以针对您的系统调整它，请查看“nice”的 man 页面。&lt;/p&gt;

&lt;p&gt;env 
用于服务处理程序的环境变量。&lt;/p&gt;

&lt;p&gt;passenv 
应该向下传递到服务处理程序的 xinetd 中的环境变量。&lt;/p&gt;

&lt;p&gt;资源管理参数&lt;/p&gt;

&lt;p&gt;instances 
可以同时启动的处理程序数。可以调整这个参数以防止拒绝服务攻击。如果您希望缺省（无限制）行为，将它设置成“UNLIMITED”。&lt;/p&gt;

&lt;p&gt;max_load 
I: ) 如果系统过载，停止接受连接。负载数取决于系统，仅当您确实知道自己在做什么时才能调整它。&lt;/p&gt;

&lt;p&gt;rlimit_as, rlmist_cpu, rlimit_data, rlimit_rss, rlimit_stack 
rlimit 参数指定用于服务处理程序的资源限制（内存、CPU 以及特定内存区域）。&lt;/p&gt;

&lt;p&gt;特定于安全性的参数&lt;/p&gt;

&lt;p&gt;only_from, no_access 
对 TCP 封装器的补充，这是阻挡主机建立与我们的连接的方法之一。请注意，缺省值是允许对任何人的访问，除非 TCP 封装器（其规则通常在 /etc/hosts.allow 中）另有规定。&lt;/p&gt;

&lt;p&gt;access_times 
一天中服务可用的时间。例如，“6:00-23:00”意味着服务从上午 6 点到晚上 11:01 可用。&lt;/p&gt;

&lt;p&gt;log_type, log_on_success, log_on_failure 
各种日志记录选项。USERID 标志可能特别麻烦，因为它向连接的主机询问关于与我们连接的用户，这使得处理变慢。尽可能避免使用 USERID。&lt;/p&gt;

&lt;p&gt;bind 
允许服务特定于接口，通常是出于安全性考虑。例如，在网络内部的 FTP 服务只是 FTP，而外部 FTP 连接将生成入侵者警报。“id”参数在这里很有用。&lt;/p&gt;

&lt;p&gt;per_source 
指定来自源 IP 的服务的最大实例数。对于处理“单源拒绝服务（single-source denial-of-service）”攻击或出错程序建立的过多连接非常有用。&lt;/p&gt;

&lt;p&gt;cps 
每秒允许的最大连接数，以及服务再度启用之前的秒数。“30 45”表示“每秒 30 个入站连接，如果超过限制，则等待 45 秒”。主要用于对付拒绝服务攻击。&lt;/p&gt;

&lt;p&gt;deny_time 
对引发 SENSOR 标志的人拒绝服务的时间。&lt;/p&gt;

&lt;p&gt;替换 TCP 封装器
经典的 TCP 封装器软件包是个非常有用的工具。通过一个集中式的文件（通常是 /etc/hosts.allow 和 /etc/hosts.deny），针对每个服务，根据需要来允许或拒绝对任何主机的访问。不幸的是，TCP 封装器库不太了解系统负载、资源限制、多重攻击之类的情况。xinetd 合并了 TCP 封装器功能性（通过 libwrap 库），所以您可以顺利地迁移到 xinted，并继续使用和以前相同的配置文件。&lt;/p&gt;

&lt;p&gt;这差不多就是迁移所要做的全部工作了。保持旧的 hosts.deny 和 hosts.allow 文件，xinetd 将乐意遵循它们。但是，请牢记，xinetd 有许多在 TCP 封装器基础上改进的连接控制选项。例如，限制每秒连接数或过载时的连接数，可以成为对服务器管理极有价值的帮助。&lt;/p&gt;

&lt;p&gt;确保您是使用 libwrap 选项编译 xinetd 的，否则，它将不知道 TCP 封装器。如果 xinetd 来自于 Red Hat Linux 上的 RPM，确保您在开放机器“之前”，测试 TCP 封装器文件是否正常运行。&lt;/p&gt;

&lt;p&gt;高级用途：故障转移
尽管可以有多种方法使用 xinetd，redirect 参数为我们提供了最有趣的使用方法。众所周知，故障转移很难实现，并且硬件故障转移很昂贵。这里所描述的方法（通过简单的软件）既便宜又有效。它具有单故障点 — 重定向点，所以您应该考虑该方式是否可接受。如果不能接受，那么，硬件故障转移就贵得有道理了。&lt;/p&gt;

&lt;p&gt;首先，确定一种方法从两台或者更多的机器中选出一台“活动的”机器。假设您通过一个脚本 set_active.pl 来完成（我们将为 telnet 服务完成该步，但是它对任何其他服务也有效，只要能保持服务切换到其他服务器而不带来不良影响）。脚本将采用我们用来设置新故障转移的机器名，以及给我们适当的用于编辑的 /etc/xinetd.d/SERVICE 文件的服务名。请随意定制脚本以编辑不同文件，或使用不同参数。可以用一行“perl -p -i -e”脚本执行这个作业，但您可以在将来对这种方法作许多扩展，并可以对参数执行错误检查。&lt;/p&gt;

&lt;p&gt;这太简单了。现在只要决定调用这个脚本的过程即可 — 可以是手工、通过一个 cron 作业、或者由另一个程序触发。此时，它成为体系结构决策。别忘了在这时向 xinetd 发送 USR2 信号，如果愿意，也可以重新启动它。在 Red Hat Linux 上可以用“pkill -USR2 xinetd”完成信号的自动化，而重新启动 xinetd 只要使用“/etc/rc.d/init.d/xinetd restart”（在 Linux 上）或者其它类似命令（在大多数 UNIX 系统上）。&lt;/p&gt;

&lt;p&gt;这种故障转移将“不会”对数据库连通性生效，除非在数据库端做许多额外工作。建议您最好将它用于诸如 rsync、ssh、ftp 和 telnet 之类的协议，其中，故障转移机器彼此没有相互依赖性。&lt;/p&gt;

&lt;p&gt;Linux inetd 有多个服务
1， 开启方式   命令行输入 inetd。&lt;/p&gt;

&lt;p&gt;2， 开启服务选择  /etc/inetd.conf       ftp  telnet   etc.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;开启服务对应程序建立软连接  指向 busybox 对应程序     busybox cp .&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;开启对应端口 /etc/server&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 27 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/27/inetd.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/27/inetd.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Django_nginx_uwsgi</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;安装Python包管理
easy_install 包 https://pypi.python.org/pypi/distribute
wget https://pypi.python.org/packages/source/d/distribute/distribute-0.6.49.tar.gz
tar xf distribute-0.6.49.tar.gz
cd distribute-0.6.49
python2.7 setup.py install
easy_install –version
安装 uwsgi
uwsgi:https://pypi.python.org/pypi/uWSGI&lt;/p&gt;

&lt;p&gt;uwsgi 参数详解：http://uwsgi-docs.readthedocs.org/en/latest/Options.html&lt;/p&gt;

&lt;p&gt;pip install uwsgi
uwsgi –version    # 查看 uwsgi 版本
测试 uwsgi 是否正常：&lt;/p&gt;

&lt;p&gt;新建 test.py 文件，内容如下：&lt;/p&gt;

&lt;p&gt;def application(env, start_response):
    start_response(‘200 OK’, [(‘Content-Type’,’text/html’)])
    return “Hello World”
然后在终端运行：&lt;/p&gt;

&lt;p&gt;uwsgi –http :8001 –wsgi-file test.py&lt;/p&gt;

&lt;p&gt;在浏览器内输入：http://127.0.0.1:8001，查看是否有”Hello World”输出，若没有输出，请检查你的安装过程。&lt;/p&gt;

&lt;p&gt;安装 Django
pip install django
测试 django 是否正常，运行：&lt;/p&gt;

&lt;p&gt;django-admin.py startproject demosite
cd demosite
python2.7 manage.py runserver 0.0.0.0:8002
在浏览器内输入：http://127.0.0.1:8002，检查django是否运行正常。&lt;/p&gt;

&lt;p&gt;安装 Nginx
安装命令如下：&lt;/p&gt;

&lt;p&gt;cd ~
wget http://nginx.org/download/nginx-1.5.6.tar.gz
tar xf nginx-1.5.6.tar.gz
cd nginx-1.5.6
./configure –prefix=/usr/local/nginx-1.5.6 \
–with-http_stub_status_module \
–with-http_gzip_static_module
make &amp;amp;&amp;amp; make install
你可以阅读 Nginx 安装配置 了解更多内容。&lt;/p&gt;

&lt;p&gt;uwsgi 配置
uwsgi支持ini、xml等多种配置方式，本文以 ini 为例， 在/ect/目录下新建uwsgi9090.ini，添加如下配置：&lt;/p&gt;

&lt;p&gt;[uwsgi]
socket = 127.0.0.1:9090
master = true         //主进程
vhost = true          //多站模式
no-site = true        //多站模式时不设置入口模块和文件
workers = 2           //子进程数
reload-mercy = 10   &lt;br /&gt;
vacuum = true         //退出、重启时清理文件
max-requests = 1000 &lt;br /&gt;
limit-as = 512
buffer-size = 30000
pidfile = /var/run/uwsgi9090.pid    //pid文件，用于下面的脚本启动、停止该进程
daemonize = /website/uwsgi9090.log
Nginx 配置
找到nginx的安装目录（如：/usr/local/nginx/），打开conf/nginx.conf文件，修改server配置：&lt;/p&gt;

&lt;p&gt;server {
        listen       80;
        server_name  localhost;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    location / {            
        include  uwsgi_params;
        uwsgi_pass  127.0.0.1:9090;              //必须和uwsgi中的设置一致
        uwsgi_param UWSGI_SCRIPT demosite.wsgi;  //入口文件，即wsgi.py相对于项目根目录的位置，“.”相当于一层目录
        uwsgi_param UWSGI_CHDIR /demosite;       //项目根目录
        index  index.html index.htm;
        client_max_body_size 35m;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WSGI是Web Server Gateway Interface的缩写。以层的角度来看，WSGI所在层的位置低于CGI。但与CGI不同的是WSGI具有很强的伸缩性且能运行于多线程或多进程的环境下，这是因为WSGI只是一份标准并没有定义如何去实现。实际上WSGI并非CGI，因为其位于web应用程序与web服务器之间，而web服务器可以是CGI，mod_python（注：现通常使用mod_wsgi代替），FastCGI或者是一个定义了WSGI标准的web服务器就像python标准库提供的独立WSGI服务器称为wsgiref。&lt;/p&gt;

&lt;p&gt;Python Paste - WSGI底层工具集. 包括多线程, SSL和 基于Cookies, sessions等的验证(authentication)库. 可以用Paste方便地搭建自己的Web框架。
WSGI:Python Web Server Gateway Interface v1.0
它是 PEP3333中定义的（PEP3333的目标建立一个简单的普遍适用的服务器与Web框架之间的接口）
WSGI是Python应用程序或框架和Web服务器之间的一种接口&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/12/27/Django_nginx_uwsgi.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/12/27/Django_nginx_uwsgi.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>UNIX下的5种IO模型</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;套接字的IO操作，如recvfrom，分为两个阶段：&lt;/p&gt;

&lt;p&gt;（1）等待内核中的接收缓冲区中有数据可读。&lt;/p&gt;

&lt;p&gt;（2）将接收缓冲区中的数据复制进应用缓冲区。&lt;/p&gt;

&lt;p&gt;1，阻塞式IO&lt;/p&gt;

&lt;p&gt;文件描述符open时，如果没有指定flags为O_NONBLOCK，或者open后，没有使用fcntl设置O_NONBLOCK，默认文件描述符为阻塞模式。&lt;/p&gt;

&lt;p&gt;阻塞式IO在等待接收缓冲区数据到来时，会阻塞；&lt;/p&gt;

&lt;p&gt;数据到来，进行数据复制时，也会阻塞。&lt;/p&gt;

&lt;p&gt;2，非阻塞式IO&lt;/p&gt;

&lt;p&gt;如上所述，可以通过open或者fcntl设置文件描述符为非阻塞模式。&lt;/p&gt;

&lt;p&gt;非阻塞式IO，当内核缓冲区没有数据时，不会阻塞，会立即返回一个错误——EAGAIN或者EWOULDBLOCK。EAGAIN表示需要再次调用recvfrom，以判断数据是否准备好，这也是非阻塞式IO的用法，不断的调用recvfrom，以判断是否可以读。EWOULDBLOCK是虚拟语气，表示“本应该阻塞”，其实没有阻塞。由于并不确定返回EAGAIN还是EWOULDBLOCK，因此需要对这两个值都进行判断。&lt;/p&gt;

&lt;p&gt;从接收缓冲区向应用缓冲区复制数据阶段，调用进程阻塞。&lt;/p&gt;

&lt;p&gt;3，IO复用&lt;/p&gt;

&lt;p&gt;在阻塞式IO中，如果接收缓冲区没有数据，调用进程阻塞于recvfrom操作。使用select或者poll，可以在此情况下使进程阻塞于select或者poll操作（因此要把文件描述符设置为非阻塞式），而且可以同时检测多个文件描述符是否可读，即检测这些描述符对应的内核中的接收缓冲区是否有数据。&lt;/p&gt;

&lt;p&gt;从接收缓冲区向应用缓冲区复制数据阶段，调用进程阻塞。&lt;/p&gt;

&lt;p&gt;4，信号驱动式IO&lt;/p&gt;

&lt;p&gt;信号驱动式IO与上述三个IO模型相比，即不像阻塞式IO那样阻塞于recvfrom操作，也不像非阻塞式IO那样需要多次调用甚至轮询recvfrom才能得知是否有数据，也不像IO复用那样阻塞于select或者poll，而是当内核接收缓冲区有数据时向调用进程发送一个信号。&lt;/p&gt;

&lt;p&gt;从接收缓冲区向应用缓冲区复制数据阶段，调用进程阻塞。&lt;/p&gt;

&lt;p&gt;5，异步IO&lt;/p&gt;

&lt;p&gt;上述4种IO模型，其不同点在于当接收缓冲区没有数据时，如何判断数据已经到来：阻塞式IO中recvfrom会阻塞直到接收缓冲区有数据；非阻塞式IO通过轮询recvfrom以判断接收缓冲区是否有数据；IO复用中使用select或者poll以判断接收缓冲区是否有数据；信号驱动IO通过信号通知接收缓冲区是否有数据。&lt;/p&gt;

&lt;p&gt;其相同点在于，IO操作的第二个阶段，即从内核接收缓冲区向应用缓冲区复制数据时，调用recvfrom的进程会阻塞。&lt;/p&gt;

&lt;p&gt;可见，上述4种IO模型都会使进程阻塞，直到IO操作的两个阶段都完成才能执行其他操作，因此称为同步IO。&lt;/p&gt;

&lt;p&gt;异步IO模型中，IO操作的两个阶段都不阻塞，因此称为异步IO。&lt;/p&gt;

&lt;p&gt;阻塞IO
这是我们熟悉的IO模型，一个进程在作IO操作时，非要等到数据从内核空间拷贝到用户进程空间，才会返回。这个模型的优点就是简单，而且在阻塞的时候，CPU还可以进行调度，去执行别的进程。
非阻塞IO
一开始我看是非阻塞IO，觉得应该要比阻塞IO模型先进，可是当我一看使用方法的时候，就知道这个模型是不会被实际使用的，仅仅只能作为理论上存在的IO模型。这个模型的观点是：进行IO操作的时候，不阻塞，如果没有数据准备好，就直接返回错误码（或者是别的代码）。因此，使用者就只能不断进行轮询来调用IO函数。这样的后果就是，不仅在宏观上形成了与阻塞IO一共的“阻塞”效果，而且在微观上，CPU一直被用来轮询，造成了CPU的浪费。所以，这个模型还不如阻塞IO模型实用。
IO复用
对于IO复用，我的理解有三点：
在一次系统调用中，实现了询问多个描述符的IO准备情况 —— 根据事件通知
为了实现第一点，就需要把阻塞的地方进行转移。把一次系统调用，分为两次系统调用。第一次系统调用可以询问多个描述符的IO准备情况，在这个地方进行阻塞；而第二次系统调用，是针对已经准备好IO的描述符进行调用，此时，理论上（按照我的理解），也是会发生阻塞的，只不过是此时内核已经把数据准备好了，阻塞的时间可以忽略不计罢了。
本质上，还是阻塞的。
信号IO
我们都知道，信号是UNIX提供了进程间进行通信的一种方式。我们常用的 kill -9 命令（kill是向进程传递信号量，9只是众多信号中的一个代号），或者是 Ctrl + C 的时候，就是向某个进程发出终止的信号，这样进程就退出了。
而对于信号IO的模型，我是这么理解的：进程在发起IO操作，系统调用之后，直接访问，内核会在IO数据准备好之后，以某个信号通知发起IO操作的进程，从而使得该进程的信号处理函数可以读取IO数据的操作。
本质上，这也是阻塞的IO模型，因为在信号处理函数中，同样也是要进行阻塞的，只是在在这个时候发起系统系统，内核已经把数据准备好了。
异步IO
这是真正的异步IO了。实现的机制是：用户在发起异步IO的系统调用时，会把相应的数据处理函数作为回调函数，等到IO数据准备好，内核会主动调用此回调函数。可以看出，用户进程在这种模型下，只调用了一次系统调用，而且是立即返回的，因此，就不会出现让进程阻塞的情况，也就符合了POSIX中异步IO的定义。
其实我理解起来，思路是和信号IO差不多的，唯一不同的地方，对于IO数据的操作，异步IO是由内核主动发起的，而信号IO是由用户进程发起的。&lt;/p&gt;

&lt;p&gt;进程切换
为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。&lt;/p&gt;

&lt;p&gt;从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;保存处理机上下文，包括程序计数器和其他寄存器。&lt;/li&gt;
  &lt;li&gt;更新PCB信息。&lt;/li&gt;
  &lt;li&gt;把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。&lt;/li&gt;
  &lt;li&gt;选择另一个进程执行，并更新其PCB。&lt;/li&gt;
  &lt;li&gt;更新内存管理的数据结构。&lt;/li&gt;
  &lt;li&gt;恢复处理机上下文。&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 24 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/24/unix_io5.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/24/unix_io5.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>用户空间实现线程 内核实现线程 线程的调度</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;1、在用户空间中实现线程&lt;/p&gt;

&lt;p&gt;（1）特点：把整个线程包放在用户空间，内核对线程包一无所知。从内核角度考虑，就是按正常的方式管理，即单线程进程（存在运行时系统）&lt;/p&gt;

&lt;p&gt;（2）优点：&lt;/p&gt;

&lt;p&gt;1、用户级线程包可以在不支持线程的操作系统上实现。&lt;/p&gt;

&lt;p&gt;2、线程切换至少要比陷入内核要快一个数量级。在线程完成运行时，它调用thread_yield可以把该线程的信息保存在线程表中；进而，它可以调用线程调度程序来选择另一个要运行的线程。保存该线程状态的过程和调度程序都只是本地过程，所以启动它们比进行内核调用效率更高。另一方面，不需要陷阱，不需要上下文切换，也不需要对内存高速缓存进行刷新，这使得线程调度非常快捷。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 在用户空间管理线程时，每个进程都需要有其专用的线程表，用来跟踪该进程中的线程。与进程表类似，线程表记录各个线程的属性，如每个线程的程序计数器，堆栈指针，寄存器和状态等，线程表由运行时系统管理，当一个线程转换到就绪状态或阻塞状态是，在该线程表中存放重新启动该线程所需的信息，与内核在进程表中存放进程的信息完全一样。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当某个线程做了一些会引起在本地阻塞的事情之后，例如等待进程中另一个线程完成某些工作，它调用一个运行时系统的过程，这个过程检查该线程是否必须进入阻塞状态。如果是，它在线程表中保持该线程的寄存器，并查看表中可运行的就绪线程，并把新线程的保存值重新装入机器的寄存器中。只要堆栈指针和程序计数器一被切换，新线程就又自动投入运行。&lt;/p&gt;

&lt;p&gt;(3) 允许每个进程有自己定制的调度算法。&lt;/p&gt;

&lt;p&gt;(4) 具有较好的可扩展性，这是因为在内核空间中内核线程需要一些固定表格空间和堆栈空间，当内核线程的数量非常大，就会出现问题。&lt;/p&gt;

&lt;p&gt;（3）缺点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   线程发生I/O或页面故障引起的阻塞时，如果调用阻塞系统调用则内核由于不知道有多线程的存在，而会阻塞整个进程从而阻塞所有线程。注（阻塞调用是指调用结果返回之前，当前线程会被挂起。函数只有在得到结果之后才会返回。）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在一个单独的进程内部，没有时钟中断，所以不能用轮转调度的方式调度线程。如果一个线程开始运行，那么在该进程中的其他线程就不能运行，除非第一个线程自动放弃CPU。&lt;/p&gt;

&lt;p&gt;下面是线程包实现图&lt;/p&gt;

&lt;p&gt;2、在内核中实现线程&lt;/p&gt;

&lt;p&gt;（1）特点：&lt;/p&gt;

&lt;p&gt;在内核中实现线程，此时不再需要运行时系统。另外，每个进程中也没有线程表，相反，在内核中用来记录系统中所有线程的线程表。当一个线程阻塞时，内核可以根据其选择，可以运行同一个进程中的另一个线程，或者运行另一个进程中的线程。而在用户级线程中，运行时系统始终运行自己进程中的线程，直到内核剥夺它的CPU为止。当某个线程希望创建一个新线程或撤销一个已有线程时，它进行一个系统调用。在内核中实现线程时，内核必须维护两个表，传统的进程表以便跟踪进程的状态和线程表。&lt;/p&gt;

&lt;p&gt;（2）优点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    内核线程不需要任何新的，非阻塞系统调用。另外，如果某个进程中的线程引起了页面故障，内核可以很方便地检查该进程是否有任何其他可运行的线程，如果有，在等待所需要的页面从磁盘读入时，就选择一个可运行的线程运行。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;（3）内核级线程的缺点是：应用程序线程在用户态运行，而线程调度和管理在内核实现。在同一进程中，控制权从一个线程转移到另一个线程，需要用户态-内核态-用户态的模式切换，系统开销较大。（应用程序线程和线程调度管理，都在同一进程内）&lt;/p&gt;

&lt;p&gt;综上：用户级线程和内核级线程之间的差别在于性能。用户级线程的切换需要少量的机器指令，而内核级线程需要完整的上下文切换，修改内存映像，使高速缓存失效，这导致了若干数量级的延迟。另一方面，在使用内核级线程时，一旦线程阻塞在I/O就不需要像在用户级线程中那样将整个进程挂起。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   所有能够阻塞线程的调用都以系统调用的形式实现，代价可观。当一个线程阻塞时，内核根据选择可以运行另一个进程的线程，而用户空间实现的线程中，运行时系统始终运行自己进程中的线程。说明：由于内核创建线程代价大，故有线程回收。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;线程调度&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  当若干进程都有多个线程时，存在两个层次的并行，进程和线程。这样的系统中调度处理有本质的区别，是用户级线程还是内核级线程。

  对于用户级线程，内核并不知道有线程存在，所以内核还是和以前一样地操作，选取一个进程，假设为A，并给予A时间片控制，A的线程调度程序决定哪个线程运行，假设为A1。由于多道线程不存在时钟中断，所以，这个线程可以按其意愿任意运行多长时间。如果该线程用完了进程的全部时间片，内核就会选择另一个进程运行。

  在线程A终于又一次运行时，线程A1会接着运行。该线程会继续耗费A进程的所有时间，直到它完成工作。不过，该线程的这种不合群的行为不会影响到其他的进程。其他进程会得到调度程序所分配的合适份额，不会考虑进程A内部所发生的事。但是，用户级线程缺乏一个时钟将运行过长的线程加以中断。

   对于内核级线程而言，内核选择一个特定的线程运行，它不用考虑该线程属于哪个进程，不过，如果有必要的话，它可以这样做。对被选择的线程赋予一个时间片，而且，如果超过了时间片，就会强制挂起该线程。
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sun, 24 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/24/thread_namespace.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/24/thread_namespace.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>进程切换</title>
        <description>&lt;p&gt;为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换（process switch）、任务切换（task switch）或上下文切换（content switch）。&lt;/p&gt;

&lt;p&gt;硬件上下文
尽管每个进程都有自己的地址空间，但所有进程必须共享CPU寄存器。因此，在恢复一个进程的执行之前，内核必须确保每个寄存器装载了挂起进程时所需要的值。&lt;/p&gt;

&lt;p&gt;进程恢复执行前必须装入寄存器的一组数据成为硬件上下文（hardware context）。硬件上下文是进程可执行上下文的一个自己，因为可执行上下文包含进程执行时所需要的所有信息。在Linux中，进程硬件上下午的一部分存放在TSS段，而剩余部分存放在内核态堆栈中。&lt;/p&gt;

&lt;p&gt;在下面描述中，假定用prev局部变量表示切换出的进程描述符，next表示切换进的进程描述符。因此，我们把进程切换定义为这样的行为：保存prev硬件上下文，用next硬件上下文代替prev。因为进程切换经常发生，因此减少保存和装入硬件上下文所话费的时间是非常重要的。&lt;/p&gt;

&lt;p&gt;早期Linux版本利用80x86体系结构所需提供的硬件支持，并通过far jmp1指令跳到next进程TSS描述符的选择符来执行进程切换。当执行这条指令时，CPU通过自动保存原来的硬件上下文，装入新的硬件上下文来执行硬件上下文切换。但Linux2.6使用软件执行进程切换，原因有：&lt;/p&gt;

&lt;p&gt;通过一组mov指令逐步执行切换，这样能较好地控制所装入的数据的合法性，一面被恶意用户伪造。far jmp指令不会有这样的检查。
旧方法和新方法所需时间大致相同。
进程切换值发生在内核态，在执行进程切换之前，用户态进程使用的所有寄存器内容已保存在内核堆栈上，这也包括ss和esp这对寄存器的内容。&lt;/p&gt;

&lt;p&gt;任务状态段
80x86体系结构包含了一个特殊的段类型，叫任务状态段（Task State Segment，TSS）来存放硬件上下文，尽管Linux并不使用硬件上下文切换，但是强制它为系统中每个不同的CPU创建一个TSS，这样做主要有两个理由：&lt;/p&gt;

&lt;p&gt;当80x86的一个CPU从用户态切换到内核态时，它就从TSS中后去内核态堆栈的地址。
当用户态进程试图通过in或out指令访问一个I/O端口时，CPU需要访问存放在TSS中的I/O许可位图以检查该进程是否有访问端口的权利。
更确切的说，当进程在用户态执行in或out指令时，控制单元执行下列操作：&lt;/p&gt;

&lt;p&gt;检查eflags寄存器中的2位IOPL字段，如果字段的值为3，控制单元就执行I/O指令。否则，执行下一个检查。
访问tr寄存器以确定当前的TSS和相应的I/O许可权位图。
检查I/O指令中指定的I/O端口在I/O许可权位图中对应的位，如果该位清，这条指令就执行，否则控制单元产生一个异常。
tss_struct结构描述TSS的格式，init_tss数组为系统上每个不同的CPU存放一个TSS。在每次进程切换时，内核都更新TSS的某些字段以便相应的CPU控制单元可以安全地检索到它需要的信息。因此，TSS反映了CPU上当前进程的特权级，但不必为没有在运行的进程保留TSS。&lt;/p&gt;

&lt;p&gt;每个TSS有它自己8字节的任务状态段描述符（Task State Segment Descriptor，TSSD）。这个描述符包括指向TSS起始地址的32位Base字段，20位Limit字段。TSSD的S标志位被清0，以表示相应的TSS时系统段的事实。&lt;/p&gt;

&lt;p&gt;Type字段被置位11或9以表示这个段实际上是一个TSS。在Intel的原始设计中，系统中的每个进程都应当指向自己的TSS；Type字段的第二个有效位叫Busy位；如果进程正由CPU执行，则该位置1，否则为0。在Linux的设计中，每个CPU只有一个TSS，因此Busy位总是为1.&lt;/p&gt;

&lt;p&gt;由Linux创建的TSSD存放在全局描述符表（GDT）中，GDT的基地址存放在每个CPU的gdtr寄存器中。每个CPU的tr寄存器包含相应TSS的TSSD选择符，也包含了两个隐藏的非编程字段：TSSD的Base字段和Limit字段。这样，处理器就能够直接TSS寻址而不需要从GDT中检索TSS地址。&lt;/p&gt;

&lt;p&gt;thread字段
在每次进程切换时，被替换的进程的硬件上下文必须保存在别处。不能像Intel原始设计那样保存在TSS中，因为Linux为每个处理器而不是为每个进程使用TSS。&lt;/p&gt;

&lt;p&gt;因此，每个进程描述符包含一个类型为thread_struct的thread字段，只要进程被切换出去，内核就把其硬件上下文保存在这个结构中。随后可以看到，这个数据结构包含的字段涉及大部分CPU寄存器，但不包括eax、ebx等等这些通用寄存器。它们的值保留在内核堆栈中。&lt;/p&gt;

&lt;p&gt;执行进程切换
进程切换可能只发生在精心定义的点：schedule()函数，这个函数很长，会在以后更长的篇幅里讲解。。这里，只关注内核如何执行一个进程切换。&lt;/p&gt;

&lt;p&gt;进程切换由两步组成：&lt;/p&gt;

&lt;p&gt;切换页全局目录以安装一个新的地址空间。
切换内核态堆栈和硬件上下文，因为硬件上下文提供了内核执行新进程所需要的所有信息，包含CPU寄存器。
switch_to宏
进程切换的第二步由switch_to宏执行。它是内核中与硬件关系最为密切的例程之一，必须下很多功夫了解。&lt;/p&gt;

&lt;p&gt;&amp;lt;include/asm-generic/system.h&amp;gt;
/* context switching is now performed out-of-line in switch_to.S */
extern struct task_struct *__switch_to(struct task_struct *,
        struct task_struct *);
#define switch_to(prev, next, last)\
    do {\
        ((last) = __switch_to((prev), (next)));\
    } while (0)
首先，该宏有三个参数，prev、next和last，prev和next的作用仅是局部变量prev和next的占位符，即它们是输入参数，分别表示被替换进程和新进程描述符的地址在内存中的位置。&lt;/p&gt;

&lt;p&gt;在任何进程切换中，涉及到的是三个进程而不是两个。假设内核决定暂停进程A而激活进程B，在schedule()函数中，prev指向A的描述符，而next指向B的进程描述符。switch_to宏一旦使A暂停，A的执行流就被冻结。&lt;/p&gt;

&lt;p&gt;随后，当内核想再次激活A，就必须暂停另一个进程C，因为这通常不是B，因为B有可能被其他进程比如C切换。于是就要用prev指向C而next指向A来执行另一个switch_to宏。当A恢复它执行的流时，就会找到它原来的内核栈，于是prev局部变量还是指向A的描述符而next指向B的描述符。此时，代表进程A执行的内核就失去了对C的任何引用。但引用对于完成进程切换是有用的，所以需要保留。&lt;/p&gt;

&lt;p&gt;switch_to宏的最后一个参数是输出参数，它表示宏把进程C的描述符地址写在内存的什么位置了，不过，这个是在恢复A执行之后完成的。在进程切换之前，宏把第一个输入参数prev表示的变量存入CPU的eax寄存器。在完成进程切换，A已经恢复执行时，宏把CPU的eax寄存器的内容写入由第三个参数last所指示的A在内存中的位置。因为CPU寄存器不会在切换点发生变化，所以C的描述符地址也存在内存的这个位置。在schedule()执行过程中，last参数指向A的局部变量prev，所以prev被C的地址覆盖。&lt;/p&gt;

&lt;p&gt;__switch_to()函数
__switch_to()函数执行大多数开始于switch_to()宏的进程切换。这个函数作用于prev_p和next_p参数，这两个参数表示前一个进程和新进程。这个函数的调用不同于一般的函数调用。因为__switch_to()从eax和edx取参数prev_p和next_p，而不像大多数函数一样从栈中取参数。&lt;/p&gt;

&lt;p&gt;&amp;lt;arch/x86/kernel/process_32.c&amp;gt;
__switch_to(
    struct task_struct *prev_p,
    struct task_struct *next_p)
{
    struct thread_struct *prev = &amp;amp;prev_p-&amp;gt;thread,
                 *next = &amp;amp;next_p-&amp;gt;thread;
    int cpu = smp_processor_id();
    struct tss_struct *tss = &amp;amp;per_cpu(init_tss, cpu);
    bool preload_fpu;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;preload_fpu = tsk_used_math(next_p) &amp;amp;&amp;amp; next_p-&amp;gt;fpu_counter &amp;gt; 5;

__unlazy_fpu(prev_p);

if (preload_fpu)
    prefetch(next-&amp;gt;xstate);

load_sp0(tss, next);

lazy_save_gs(prev-&amp;gt;gs);

load_TLS(next, cpu);

if (get_kernel_rpl() &amp;amp;&amp;amp; unlikely(prev-&amp;gt;iopl != next-&amp;gt;iopl))
    set_iopl_mask(next-&amp;gt;iopl);

if (unlikely(task_thread_info(prev_p)-&amp;gt;flags 
    &amp;amp; _TIF_WORK_CTXSW_PREV
    || task_thread_info(next_p)-&amp;gt;flags
    &amp;amp; _TIF_WORK_CTXSW_NEXT))
    __switch_to_xtra(prev_p, next_p, tss);

if (preload_fpu)
    clts();

arch_end_context_switch(next_p);

if (preload_fpu)
    __math_state_restore();
if (prev-&amp;gt;gs | next-&amp;gt;gs)
    lazy_load_gs(next-&amp;gt;gs);

percpu_write(current_task, next_p);

return prev_p; } 这个函数执行步骤如下：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行由__unlay_fpu()宏代码产生的代码，以有选择地保存prev_p进程的FPU、MMX以及XMM寄存器的内容。&lt;/p&gt;

&lt;p&gt;执行smp_processor_id()宏获得本地CPU的下表，即执行代码的CPU。该宏从当前进程的thread_info结构的cpu字段获得下标并保存到cpu局部变量。&lt;/p&gt;

&lt;p&gt;把next_p-&amp;gt;thread.esp0装入对应于本地CPU的TSS的esp0字段。其实，任何由sysenter汇编指令产生的从用户态到内核态的特权级转换将把这个地址拷贝到esp寄存器中。&lt;/p&gt;

&lt;p&gt;把next_p进程使用的线程局部存储（TLS）段装载入本地CPU的全局描述符表。&lt;/p&gt;

&lt;p&gt;把fs和gs段寄存器的内容分别存放在prev_p-&amp;gt;thread.fs和prev_p-&amp;gt;thread.gs中。esi寄存器指向prev_p-&amp;gt;thread结构。&lt;/p&gt;

&lt;p&gt;如果fs或gs段寄存器已经被prev_p或next_p进程中的任意一个使用，则将next_p进程的thread_struct描述符中保存的值装入这些寄存器。&lt;/p&gt;

&lt;p&gt;用next_p-&amp;gt;thread.debugreg数组内容装载dr0…dr7中的6个调试寄存器。只有在next_p被挂起时正在使用调试寄存器，这种操作才能进行。&lt;/p&gt;

&lt;p&gt;如果必要，则更新TSS中的I/O位图。然后终止，prev_p参数被拷贝到eax，因为缺省情况下任何C函数的返回值被传给eax寄存器。所以eax的值在调用__switch_to()的过程中被保护起来；这很重要，因为调用该函数时会假定eax总是用来存放将被替换的进程描述符地址。&lt;/p&gt;

&lt;p&gt;汇编语言指令ret把栈定保存的返回地址装入eip程序计数器。不过，__swtich_to()函数时通过简单的跳转被调用的。因此，ret汇编指令在栈中找到标号为1的指令地址，其中标号为1的地址是由switch_to()宏推入堆栈的。&lt;/p&gt;
</description>
        <pubDate>Sun, 24 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/24/thread.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/24/thread.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>IO多路复用之select、poll、epoll</title>
        <description>&lt;!-- more --&gt;
&lt;pre&gt;&lt;code&gt; 目前支持I/O多路复用的系统调用有 select，pselect，poll，epoll，I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，pselect，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。
 与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一、使用场景
IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。IO多路复用适用如下场合：
　　1）当客户处理多个描述符时（一般是交互式输入和网络套接口），必须使用I/O复用。
　　2）当一个客户同时处理多个套接口时，这种情况是可能的，但很少出现。
　　3）如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用。
　　4）如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用。
　　5）如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用。&lt;/p&gt;

&lt;p&gt;二、select、poll、epoll简介
　　epoll跟select都能提供多路I/O复用的解决方案。在现在的Linux内核里有都能够支持，其中epoll是Linux所特有，而select则应该是POSIX所规定，一般操作系统均有实现。
1、select
基本原理：select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述符就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。&lt;/p&gt;

&lt;p&gt;基本流程，如图所示：
	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/ioMutex.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但是这样也会造成效率的降低。&lt;/p&gt;

&lt;p&gt;select本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理。这样所带来的缺点是：
1、select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认值是1024。
　　一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048.
2、对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低。
　　当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度，不管哪个Socket是活跃的，都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。
3、需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。&lt;/p&gt;

&lt;p&gt;2、poll
基本原理：poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。&lt;/p&gt;

&lt;p&gt;它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样有一个缺点：
1）大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。
2）poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。&lt;/p&gt;

&lt;p&gt;注意：从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。&lt;/p&gt;

&lt;p&gt;3、epoll
　　epoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。&lt;/p&gt;

&lt;p&gt;基本原理：epoll支持水平触发和边缘触发，最大的特点在于边缘触发，它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。&lt;/p&gt;

&lt;p&gt;epoll的优点：
1、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）。
2、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。
　　只有活跃可用的FD才会调用callback函数；即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。
3、内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。&lt;/p&gt;

&lt;p&gt;epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：
LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。
ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。
1、LT模式
　　LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。
2、ET模式
　　ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)。
　　ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。
3、在select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait()时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。)
注意：如果没有大量的idle-connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle-connection，就会发现epoll的效率大大高于select/poll。&lt;/p&gt;

&lt;p&gt;三、select、poll、epoll区别
1、支持一个进程所能打开的最大连接数
 	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/selectPoolConnect.png&quot; /&gt;
2、FD剧增后带来的IO效率问题
 	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/selectPollFd.png&quot; /&gt;
3、消息传递方式
 	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/selectPollMmap.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;综上，在选择select，poll，epoll时要根据具体的使用场合以及这三种方式的自身特点：
1、表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。
2、select低效是因为每次它都需要轮询。但低效也是相对的，视情况而定，也可通过良好的设计改善。&lt;/p&gt;

&lt;p&gt;select() 在BSD中被引入，而poll()是SysV STREAM流控制的产物。因此，这里就有了平台移植上的考虑：纯粹的BSD系统可 能仍然缺少poll()，而早一些的SVR3系统中可能没有select()，尽管在SVR4中将其加入。目前两者都是POSIX. 1g标准，（译者 注：因此在Linux上两者都存在）&lt;/p&gt;

&lt;p&gt;select()和poll()本质上来讲做的是同一件事，只是完成的方法不一样。两者都通过检验一组文件描述符来检测是否有特定的时间将在上面发生并在一定的时间内等待其发生。&lt;/p&gt;

&lt;p&gt;[重要事项：无论select()还是poll()都不对普通文件起很大效用，它们着重用于套接口(socket)、管道(pipe)、伪终端(pty)、终端设备(tty)和其他一些字符设备，但是这些操作都是系统相关(system-dependent)的。]&lt;/p&gt;

&lt;p&gt;select()函数的接口主要是建立在一种叫’fd_set’类型的基础上。它(‘fd_set’) 是一组文件描述符(fd)的集合。由于fd_set类型的长度在不同平台上不同，因此应该用一组标准的宏定义来处理此类变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fd_set set;
FD_ZERO(&amp;amp;set);       /* 将set清零 */
FD_SET(fd, &amp;amp;set);    /* 将fd加入set */
FD_CLR(fd, &amp;amp;set);    /* 将fd从set中清除 */
FD_ISSET(fd, &amp;amp;set);  /* 如果fd在set中则真　*/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 过去，一个fd_set通常只能包含少于等于32个文件描述符，因为fd_set其实只用了一个int的比特矢量来实现，在大多数情况下，检查 fd_set能包括任意值的文件描述符是系统的责任，但确定你的fd_set到底能放多少有时你应该检查/修改宏FD_SETSIZE的值。&lt;em&gt;这个值是系 统相关的&lt;/em&gt;，同时检查你的系统中的select() 的man手册。有一些系统对多于1024个文件描述符的支持有问题。[译者注： Linux就是这样 的系统！你会发现sizeof(fd_set)的结果是128(*8 = FD_SETSIZE=1024)　尽管很少你会遇到这种情况。]&lt;/p&gt;

&lt;p&gt;select的基本接口十分简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int select(int nfds, fd_set *readset, fd_set *writeset,
           fd_set *exceptset, struct timeval *timeout);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中：&lt;/p&gt;

&lt;p&gt;nfds   &lt;br /&gt;
     需要检查的文件描述符个数，数值应该比是三组fd_set中最大数
     更大，而不是实际文件描述符的总数。
readset  &lt;br /&gt;
     用来检查可读性的一组文件描述符。
writeset
     用来检查可写性的一组文件描述符。
exceptset
     用来检查意外状态的文件描述符。(注：错误并不是意外状态)
timeout
     NULL指针代表无限等待，否则是指向timeval结构的指针，代表最
     长等待时间。(如果其中tv_sec和tv_usec都等于0, 则文件描述符
     的状态不被影响，但函数并不挂起)&lt;/p&gt;

&lt;p&gt;函数将返回响应操作的对应操作文件描述符的总数，且三组数据均在恰当位置被修改，只有响应操作的那一些没有修改。接着应该用FD_ISSET宏来查找返回的文件描述符组。&lt;/p&gt;

&lt;p&gt;poll ()接受一个指向结构’struct pollfd’列表的指针，其中包括了你想测试的文件描述符和事件。事件由一个在结构中事件域的比特掩码确定。当前 的结构在调用后将被填写并在事件发生后返回。在SVR4(可能更早的一些版本)中的 “poll.h”文件中包含了用于确定事件的一些宏定义。事件的等待 时间精确到毫秒 (但令人困惑的是等待时间的类型却是int)，当等待时间为0时，poll()函数立即返回，-1则使poll()一直挂起直到一个指定 事件发生。下面是pollfd的结构。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; struct pollfd {
     int fd;        /* 文件描述符 */
     short events;  /* 等待的事件 */
     short revents; /* 实际发生了的事件 */
 };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;于select()十分相似，当返回正值时，代表满足响应事件的文件描述符的个数，如果返回0则代表在规定事件内没有事件发生。如发现返回为负则应该立即查看 errno，因为这代表有错误发生。&lt;/p&gt;

&lt;p&gt;如果没有事件发生，revents会被清空，所以你不必多此一举。&lt;/p&gt;
</description>
        <pubDate>Sun, 24 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/24/select_poll.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/24/select_poll.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>goroutine</title>
        <description>&lt;p&gt;Go runtime的调度器：在了解Go的运行时的scheduler之前，需要先了解为什么需要它，因为我们可能会想，OS内核不是已经有一个线程scheduler了嘛？熟悉POSIX API的人都知道，POSIX的方案在很大程度上是对Unix process进场模型的一个逻辑描述和扩展，两者有很多相似的地方。 Thread有自己的信号掩码，CPU affinity等。但是很多特征对于Go程序来说都是累赘。 尤其是context上下文切换的耗时。另一个原因是Go的垃圾回收需要所有的goroutine停止，使得内存在一个一致的状态。垃圾回收的时间点是不确定的，如果依靠OS自身的scheduler来调度，那么会有大量的线程需要停止工作。 单独的开发一个GO得调度器，可以是其知道在什么时候内存状态是一致的，也就是说，当开始垃圾回收时，运行时只需要为当时正在CPU核上运行的那个线程等待即可，而不是等待所有的线程。用户空间线程和内核空间线程之间的映射关系有：N:1,1:1和M:NN:1是说，多个（N）用户线程始终在一个内核线程上跑，context上下文切换确实很快，但是无法真正的利用多核。1：1是说，一个用户线程就只在一个内核线程上跑，这时可以利用多核，但是上下文switch很慢。M:N是说， 多个goroutine在多个内核线程上跑，这个看似可以集齐上面两者的优势，但是无疑增加了调度的难度。
Go的调度器内部有三个重要的结构：M，P，G
M:代表真正的内核OS线程，和POSIX里的thread差不多，真正干活的人G:代表一个goroutine，它有自己的栈，instruction pointer和其他信息（正在等待的channel等等），用于调度。P:代表调度的上下文，可以把它看做一个局部的调度器，使go代码在一个线程上跑，它是实现从N:1到N:M映射的关键。
有2个物理线程M，每一个M都拥有一个context（P），每一个也都有一个正在运行的goroutine。P的数量可以通过GOMAXPROCS()来设置，它其实也就代表了真正的并发度，即有多少个goroutine可以同时运行。图中灰色的那些goroutine并没有运行，而是出于ready的就绪态，正在等待被调度。P维护着这个队列（称之为runqueue），Go语言里，启动一个goroutine很容易：go function 就行，所以每有一个go语句被执行，runqueue队列就在其末尾加入一个goroutine，在下一个调度点，就从runqueue中取出（如何决定取哪个goroutine？）一个goroutine执行。为何要维护多个上下文P？因为当一个OS线程被阻塞时，P可以转而投奔另一个OS线程！图中看到，当一个OS线程M0陷入阻塞时，P转而在OS线程M1上运行。调度器保证有足够的线程来运行所以的context P。&lt;/p&gt;

&lt;p&gt;M1可能是被创建，或者从线程缓存中取出。当MO返回时，它必须尝试取得一个context P来运行goroutine，一般情况下，它会从其他的OS线程那里steal偷一个context过来，如果没有偷到的话，它就把goroutine放在一个global runqueue里，然后自己就去睡大觉了（放入线程缓存里）。Contexts们也会周期性的检查global runqueue，否则global runqueue上的goroutine永远无法执行。
另一种情况是P所分配的任务G很快就执行完了（分配不均），这就导致了一个上下文P闲着没事儿干而系统却任然忙碌。但是如果global runqueue没有任务G了，那么P就不得不从其他的上下文P那里拿一些G来执行。一般来说，如果上下文P从其他的上下文P那里要偷一个任务的话，一般就‘偷’run queue的一半，这就确保了每个OS线程都能充分的使用。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;操作系统与运行库&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于普通的电脑用户来说，能理解应用程序是运行在操作系统之上就足够了，可对于开发者，我们还需要了解我们写的程序是如何在操作系统之上运行起来的，操作系统如何为应用程序提供服务，这样我们才能分清楚哪些服务是操作系统提供的，而哪些服务是由我们所使用的语言的运行库提供的。&lt;/p&gt;

&lt;p&gt;除了内存管理、文件管理、进程管理、外设管理等等内部模块以外，操作系统还提供了许多外部接口供应用程序使用，这些接口就是所谓的“系统调用”。从DOS时代开始，系统调用就是通过软中断的形式来提供，也就是著名的INT 21，程序把需要调用的功能编号放入AH寄存器，把参数放入其他指定的寄存器，然后调用INT 21，中断返回后，程序从指定的寄存器(通常是AL)里取得返回值。这样的做法一直到奔腾2也就是P6出来之前都没有变，譬如windows通过INT 2E提供系统调用，Linux则是INT 80，只不过后来的寄存器比以前大一些，而且可能再多一层跳转表查询。后来，Intel和AMD分别提供了效率更高的SYSENTER/SYSEXIT和SYSCALL/SYSRET指令来代替之前的中断方式，略过了耗时的特权级别检查以及寄存器压栈出栈的操作，直接完成从RING 3代码段到RING 0代码段的转换。&lt;/p&gt;

&lt;p&gt;系统调用都提供什么功能呢？用操作系统的名字加上对应的中断编号到谷歌上一查就可以得到完整的列表 (Windows, Linux)，这个列表就是操作系统和应用程序之间沟通的协议，如果需要超出此协议的功能，我们就只能在自己的代码里去实现，譬如，对于内存管理，操作系统只提供进程级别的内存段的管理，譬如Windows的virtualmemory系列，或是Linux的brk，操作系统不会去在乎应用程序如何为新建对象分配内存，或是如何做垃圾回收，这些都需要应用程序自己去实现。如果超出此协议的功能无法自己实现，那我们就说该操作系统不支持该功能，举个例子，Linux在2.6之前是不支持多线程的，无论如何在程序里模拟，我们都无法做出多个可以同时运行的并符合POSIX 1003.1c语义标准的调度单元。&lt;/p&gt;

&lt;p&gt;可是，我们写程序并不需要去调用中断或是SYSCALL指令，这是因为操作系统提供了一层封装，在Windows上，它是NTDLL.DLL，也就是常说的Native API，我们不但不需要去直接调用INT 2E或SYSCALL，准确的说，我们不能直接去调用INT 2E或SYSCALL，因为Windows并没有公开其调用规范，直接使用INT 2E或SYSCALL无法保证未来的兼容性。在Linux上则没有这个问题，系统调用的列表都是公开的，而且Linus非常看重兼容性，不会去做任何更改，glibc里甚至专门提供了syscall(2)来方便用户直接用编号调用，不过，为了解决glibc和内核之间不同版本兼容性带来的麻烦，以及为了提高某些调用的效率(譬如__NR_ gettimeofday)，Linux上还是对部分系统调用做了一层封装，就是VDSO (早期叫linux-gate.so)。&lt;/p&gt;

&lt;p&gt;可是，我们写程序也很少直接调用NTDLL或者VDSO，而是通过更上一层的封装，这一层处理了参数准备和返回值格式转换、以及出错处理和错误代码转换，这就是我们所使用语言的运行库，对于C语言，Linux上是glibc，Windows上是kernel32(或调用msvcrt)，对于其他语言，譬如Java，则是JRE，这些“其他语言”的运行库通常最终还是调用glibc或kernel32。&lt;/p&gt;

&lt;p&gt;“运行库”这个词其实不止包括用于和编译后的目标执行程序进行链接的库文件，也包括了脚本语言或字节码解释型语言的运行环境，譬如Python，C#的CLR，Java的JRE。&lt;/p&gt;

&lt;p&gt;对系统调用的封装只是运行库的很小一部分功能，运行库通常还提供了诸如字符串处理、数学计算、常用数据结构容器等等不需要操作系统支持的功能，同时，运行库也会对操作系统支持的功能提供更易用更高级的封装，譬如带缓存和格式的IO、线程池。&lt;/p&gt;

&lt;p&gt;所以，在我们说“某某语言新增了某某功能”的时候，通常是这么几种可能：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;支持新的语义或语法，从而便于我们描述和解决问题。譬如Java的泛型、Annotation、lambda表达式。&lt;/li&gt;
  &lt;li&gt;提供了新的工具或类库，减少了我们开发的代码量。譬如Python 2.7的argparse&lt;/li&gt;
  &lt;li&gt;对系统调用有了更良好更全面的封装，使我们可以做到以前在这个语言环境里做不到或很难做到的事情。譬如Java NIO&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;但任何一门语言，包括其运行库和运行环境，都不可能创造出操作系统不支持的功能，Go语言也是这样，不管它的特性描述看起来多么炫丽，那必然都是其他语言也可以做到的，只不过Go提供了更方便更清晰的语义和支持，提高了开发的效率。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;并发与并行 (Concurrency and Parallelism)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;并发是指程序的逻辑结构。非并发的程序就是一根竹竿捅到底，只有一个逻辑控制流，也就是顺序执行的(Sequential)程序，在任何时刻，程序只会处在这个逻辑控制流的某个位置。而如果某个程序有多个独立的逻辑控制流，也就是可以同时处理(deal)多件事情，我们就说这个程序是并发的。这里的“同时”，并不一定要是真正在时钟的某一时刻(那是运行状态而不是逻辑结构)，而是指：如果把这些逻辑控制流画成时序流程图，它们在时间线上是可以重叠的。&lt;/p&gt;

&lt;p&gt;并行是指程序的运行状态。如果一个程序在某一时刻被多个CPU流水线同时进行处理，那么我们就说这个程序是以并行的形式在运行。（严格意义上讲，我们不能说某程序是“并行”的，因为“并行”不是描述程序本身，而是描述程序的运行状态，但这篇小文里就不那么咬文嚼字，以下说到“并行”的时候，就是指代“以并行的形式运行”）显然，并行一定是需要硬件支持的。&lt;/p&gt;

&lt;p&gt;而且不难理解：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;并发是并行的必要条件，如果一个程序本身就不是并发的，也就是只有一个逻辑控制流，那么我们不可能让其被并行处理。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;并发不是并行的充分条件，一个并发的程序，如果只被一个CPU流水线进行处理(通过分时)，那么它就不是并行的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;并发只是更符合现实问题本质的表达方式，并发的最初目的是简化代码逻辑，而不是使程序运行的更快；&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这几段略微抽象，我们可以用一个最简单的例子来把这些概念实例化：用C语言写一个最简单的HelloWorld，它就是非并发的，如果我们建立多个线程，每个线程里打印一个HelloWorld，那么这个程序就是并发的，如果这个程序运行在老式的单核CPU上，那么这个并发程序还不是并行的，如果我们用多核多CPU且支持多任务的操作系统来运行它，那么这个并发程序就是并行的。&lt;/p&gt;

&lt;p&gt;还有一个略微复杂的例子，更能说明并发不一定可以并行，而且并发不是为了效率，就是Go语言例子里计算素数的sieve.go。我们从小到大针对每一个因子启动一个代码片段，如果当前验证的数能被当前因子除尽，则该数不是素数，如果不能，则把该数发送给下一个因子的代码片段，直到最后一个因子也无法除尽，则该数为素数，我们再启动一个它的代码片段，用于验证更大的数字。这是符合我们计算素数的逻辑的，而且每个因子的代码处理片段都是相同的，所以程序非常的简洁，但它无法被并行，因为每个片段都依赖于前一个片段的处理结果和输出。&lt;/p&gt;

&lt;p&gt;并发可以通过以下方式做到：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;显式地定义并触发多个代码片段，也就是逻辑控制流，由应用程序或操作系统对它们进行调度。它们可以是独立无关的，也可以是相互依赖需要交互的，譬如上面提到的素数计算，其实它也是个经典的生产者和消费者的问题：两个逻辑控制流A和B，A产生输出，当有了输出后，B取得A的输出进行处理。线程只是实现并发的其中一个手段，除此之外，运行库或是应用程序本身也有多种手段来实现并发，这是下节的主要内容。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;隐式地放置多个代码片段，在系统事件发生时触发执行相应的代码片段，也就是事件驱动的方式，譬如某个端口或管道接收到了数据(多路IO的情况下)，再譬如进程接收到了某个信号(signal)。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;并行可以在四个层面上做到：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;多台机器。自然我们就有了多个CPU流水线，譬如Hadoop集群里的MapReduce任务。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;多CPU。不管是真的多颗CPU还是多核还是超线程，总之我们有了多个CPU流水线。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;单CPU核里的ILP(Instruction-level parallelism)，指令级并行。通过复杂的制造工艺和对指令的解析以及分支预测和乱序执行，现在的CPU可以在单个时钟周期内执行多条指令，从而，即使是非并发的程序，也可能是以并行的形式执行。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;单指令多数据(Single instruction, multiple data. SIMD)，为了多媒体数据的处理，现在的CPU的指令集支持单条指令对多条数据进行操作。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其中，1牵涉到分布式处理，包括数据的分布和任务的同步等等，而且是基于网络的。3和4通常是编译器和CPU的开发人员需要考虑的。这里我们说的并行主要针对第2种：单台机器内的多核CPU并行。&lt;/p&gt;

&lt;p&gt;关于并发与并行的问题，Go语言的作者Rob Pike专门就此写过一个幻灯片：http://talks.golang.org/2012/waza.slide&lt;/p&gt;

&lt;p&gt;在CMU那本著名的《Computer Systems: A Programmer’s Perspective》里的这张图也非常直观清晰：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;线程的调度&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上一节主要说的是并发和并行的概念，而线程是最直观的并发的实现，这一节我们主要说操作系统如何让多个线程并发的执行，当然在多CPU的时候，也就是并行的执行。我们不讨论进程，进程的意义是“隔离的执行环境”，而不是“单独的执行序列”。&lt;/p&gt;

&lt;p&gt;我们首先需要理解IA-32 CPU的指令控制方式，这样才能理解如何在多个指令序列(也就是逻辑控制流)之间进行切换。CPU通过CS:EIP寄存器的值确定下一条指令的位置，但是CPU并不允许直接使用MOV指令来更改EIP的值，必须通过JMP系列指令、CALL/RET指令、或INT中断指令来实现代码的跳转；在指令序列间切换的时候，除了更改EIP之外，我们还要保证代码可能会使用到的各个寄存器的值，尤其是栈指针SS:ESP，以及EFLAGS标志位等，都能够恢复到目标指令序列上次执行到这个位置时候的状态。&lt;/p&gt;

&lt;p&gt;线程是操作系统对外提供的服务，应用程序可以通过系统调用让操作系统启动线程，并负责随后的线程调度和切换。我们先考虑单颗单核CPU，操作系统内核与应用程序其实是也是在共享同一个CPU，当EIP在应用程序代码段的时候，内核并没有控制权，内核并不是一个进程或线程，内核只是以实模式运行的，代码段权限为RING 0的内存中的程序，只有当产生中断或是应用程序呼叫系统调用的时候，控制权才转移到内核，在内核里，所有代码都在同一个地址空间，为了给不同的线程提供服务，内核会为每一个线程建立一个内核堆栈，这是线程切换的关键。通常，内核会在时钟中断里或系统调用返回前(考虑到性能，通常是在不频繁发生的系统调用返回前)，对整个系统的线程进行调度，计算当前线程的剩余时间片，如果需要切换，就在“可运行”的线程队列里计算优先级，选出目标线程后，则保存当前线程的运行环境，并恢复目标线程的运行环境，其中最重要的，就是切换堆栈指针ESP，然后再把EIP指向目标线程上次被移出CPU时的指令。Linux内核在实现线程切换时，耍了个花枪，它并不是直接JMP，而是先把ESP切换为目标线程的内核栈，把目标线程的代码地址压栈，然后JMP到__switch_to()，相当于伪造了一个CALL __switch_to()指令，然后，在__switch_to()的最后使用RET指令返回，这样就把栈里的目标线程的代码地址放入了EIP，接下来CPU就开始执行目标线程的代码了，其实也就是上次停在switch_to这个宏展开的地方。&lt;/p&gt;

&lt;p&gt;这里需要补充几点：(1) 虽然IA-32提供了TSS (Task State Segment)，试图简化操作系统进行线程调度的流程，但由于其效率低下，而且并不是通用标准，不利于移植，所以主流操作系统都没有去利用TSS。更严格的说，其实还是用了TSS，因为只有通过TSS才能把堆栈切换到内核堆栈指针SS0:ESP0，但除此之外的TSS的功能就完全没有被使用了。(2) 线程从用户态进入内核的时候，相关的寄存器以及用户态代码段的EIP已经保存了一次，所以，在上面所说的内核态线程切换时，需要保存和恢复的内容并不多。(3) 以上描述的都是抢占式(preemptively)的调度方式，内核以及其中的硬件驱动也会在等待外部资源可用的时候主动调用schedule()，用户态的代码也可以通过sched_yield()系统调用主动发起调度，让出CPU。&lt;/p&gt;

&lt;p&gt;现在我们一台普通的PC或服务里通常都有多颗CPU (physical package)，每颗CPU又有多个核 (processor core)，每个核又可以支持超线程 (two logical processors for each core)，也就是逻辑处理器。每个逻辑处理器都有自己的一套完整的寄存器，其中包括了CS:EIP和SS:ESP，从而，以操作系统和应用的角度来看，每个逻辑处理器都是一个单独的流水线。在多处理器的情况下，线程切换的原理和流程其实和单处理器时是基本一致的，内核代码只有一份，当某个CPU上发生时钟中断或是系统调用时，该CPU的CS:EIP和控制权又回到了内核，内核根据调度策略的结果进行线程切换。但在这个时候，如果我们的程序用线程实现了并发，那么操作系统可以使我们的程序在多个CPU上实现并行。&lt;/p&gt;

&lt;p&gt;这里也需要补充两点：(1) 多核的场景里，各个核之间并不是完全对等的，譬如在同一个核上的两个超线程是共享L1/L2缓存的；在有NUMA支持的场景里，每个核访问内存不同区域的延迟是不一样的；所以，多核场景里的线程调度又引入了“调度域”(scheduling domains)的概念，但这不影响我们理解线程切换机制。(2) 多核的场景下，中断发给哪个CPU？软中断(包括除以0，缺页异常，INT指令)自然是在触发该中断的CPU上产生，而硬中断则又分两种情况，一种是每个CPU自己产生的中断，譬如时钟，这是每个CPU处理自己的，还有一种是外部中断，譬如IO，可以通过APIC来指定其送给哪个CPU；因为调度程序只能控制当前的CPU，所以，如果IO中断没有进行均匀的分配的话，那么和IO相关的线程就只能在某些CPU上运行，导致CPU负载不均，进而影响整个系统的效率。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;并发编程框架&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上大概介绍了一个用多线程来实现并发的程序是如何被操作系统调度以及并行执行(在有多个逻辑处理器时)，同时大家也可以看到，代码片段或者说逻辑控制流的调度和切换其实并不神秘，理论上，我们也可以不依赖操作系统和其提供的线程，在自己程序的代码段里定义多个片段，然后在我们自己程序里对其进行调度和切换。&lt;/p&gt;

&lt;p&gt;为了描述方便，我们接下来把“代码片段”称为“任务”。&lt;/p&gt;

&lt;p&gt;和内核的实现类似，只是我们不需要考虑中断和系统调用，那么，我们的程序本质上就是一个循环，这个循环本身就是调度程序schedule()，我们需要维护一个任务的列表，根据我们定义的策略，先进先出或是有优先级等等，每次从列表里挑选出一个任务，然后恢复各个寄存器的值，并且JMP到该任务上次被暂停的地方，所有这些需要保存的信息都可以作为该任务的属性，存放在任务列表里。&lt;/p&gt;

&lt;p&gt;看起来很简单啊，可是我们还需要解决几个问题：&lt;/p&gt;

&lt;p&gt;(1) 我们运行在用户态，是没有中断或系统调用这样的机制来打断代码执行的，那么，一旦我们的schedule()代码把控制权交给了任务的代码，我们下次的调度在什么时候发生？答案是，不会发生，只有靠任务主动调用schedule()，我们才有机会进行调度，所以，这里的任务不能像线程一样依赖内核调度从而毫无顾忌的执行，我们的任务里一定要显式的调用schedule()，这就是所谓的协作式(cooperative)调度。(虽然我们可以通过注册信号处理函数来模拟内核里的时钟中断并取得控制权，可问题在于，信号处理函数是由内核调用的，在其结束的时候，内核重新获得控制权，随后返回用户态并继续沿着信号发生时被中断的代码路径执行，从而我们无法在信号处理函数内进行任务切换)&lt;/p&gt;

&lt;p&gt;(2) 堆栈。和内核调度线程的原理一样，我们也需要为每个任务单独分配堆栈，并且把其堆栈信息保存在任务属性里，在任务切换时也保存或恢复当前的SS:ESP。任务堆栈的空间可以是在当前线程的堆栈上分配，也可以是在堆上分配，但通常是在堆上分配比较好：几乎没有大小或任务总数的限制、堆栈大小可以动态扩展(gcc有split stack，但太复杂了)、便于把任务切换到其他线程。&lt;/p&gt;

&lt;p&gt;到这里，我们大概知道了如何构造一个并发的编程框架，可如何让任务可以并行的在多个逻辑处理器上执行呢？只有内核才有调度CPU的权限，所以，我们还是必须通过系统调用创建线程，才可以实现并行。在多线程处理多任务的时候，我们还需要考虑几个问题：&lt;/p&gt;

&lt;p&gt;(1) 如果某个任务发起了一个系统调用，譬如长时间等待IO，那当前线程就被内核放入了等待调度的队列，岂不是让其他任务都没有机会执行？&lt;/p&gt;

&lt;p&gt;在单线程的情况下，我们只有一个解决办法，就是使用非阻塞的IO系统调用，并让出CPU，然后在schedule()里统一进行轮询，有数据时切换回该fd对应的任务；效率略低的做法是不进行统一轮询，让各个任务在轮到自己执行时再次用非阻塞方式进行IO，直到有数据可用。&lt;/p&gt;

&lt;p&gt;如果我们采用多线程来构造我们整个的程序，那么我们可以封装系统调用的接口，当某个任务进入系统调用时，我们就把当前线程留给它(暂时)独享，并开启新的线程来处理其他任务。&lt;/p&gt;

&lt;p&gt;(2) 任务同步。譬如我们上节提到的生产者和消费者的例子，如何让消费者在数据还没有被生产出来的时候进入等待，并且在数据可用时触发消费者继续执行呢？&lt;/p&gt;

&lt;p&gt;在单线程的情况下，我们可以定义一个结构，其中有变量用于存放交互数据本身，以及数据的当前可用状态，以及负责读写此数据的两个任务的编号。然后我们的并发编程框架再提供read和write方法供任务调用，在read方法里，我们循环检查数据是否可用，如果数据还不可用，我们就调用schedule()让出CPU进入等待；在write方法里，我们往结构里写入数据，更改数据可用状态，然后返回；在schedule()里，我们检查数据可用状态，如果可用，则激活需要读取此数据的任务，该任务继续循环检测数据是否可用，发现可用，读取，更改状态为不可用，返回。代码的简单逻辑如下：&lt;/p&gt;

&lt;p&gt;struct chan {
    bool ready,
    int data
};&lt;/p&gt;

&lt;p&gt;int read (struct chan *c) {
    while (1) {
        if (c-&amp;gt;ready) {
            c-&amp;gt;ready = false;
            return c-&amp;gt;data;
        } else {
            schedule();
        }
    }
}&lt;/p&gt;

&lt;p&gt;void write (struct chan *c, int i) {
    while (1) {
        if (c-&amp;gt;ready) {
            schedule(); 
        } else {
            c-&amp;gt;data = i;
            c-&amp;gt;ready = true;
            schedule(); // optional
            return;
        }
    }
}
很显然，如果是多线程的话，我们需要通过线程库或系统调用提供的同步机制来保护对这个结构体内数据的访问。&lt;/p&gt;

&lt;p&gt;以上就是最简化的一个并发框架的设计考虑，在我们实际开发工作中遇到的并发框架可能由于语言和运行库的不同而有所不同，在功能和易用性上也可能各有取舍，但底层的原理都是殊途同归。&lt;/p&gt;

&lt;p&gt;譬如，glic里的getcontext/setcontext/swapcontext系列库函数可以方便的用来保存和恢复任务执行状态；Windows提供了Fiber系列的SDK API；这二者都不是系统调用，getcontext和setcontext的man page虽然是在section 2，但那只是SVR4时的历史遗留问题，其实现代码是在glibc而不是kernel；CreateFiber是在kernel32里提供的，NTDLL里并没有对应的NtCreateFiber。&lt;/p&gt;

&lt;p&gt;在其他语言里，我们所谓的“任务”更多时候被称为“协程”，也就是Coroutine。譬如C++里最常用的是Boost.Coroutine；Java因为有一层字节码解释，比较麻烦，但也有支持协程的JVM补丁，或是动态修改字节码以支持协程的项目；PHP和Python的generator和yield其实已经是协程的支持，在此之上可以封装出更通用的协程接口和调度；另外还有原生支持协程的Erlang等，笔者不懂，就不说了，具体可参见Wikipedia的页面：http://en.wikipedia.org/wiki/Coroutine&lt;/p&gt;

&lt;p&gt;由于保存和恢复任务执行状态需要访问CPU寄存器，所以相关的运行库也都会列出所支持的CPU列表。&lt;/p&gt;

&lt;p&gt;从操作系统层面提供协程以及其并行调度的，好像只有OS X和iOS的Grand Central Dispatch，其大部分功能也是在运行库里实现的。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;goroutine&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Go语言通过goroutine提供了目前为止所有(我所了解的)语言里对于并发编程的最清晰最直接的支持，Go语言的文档里对其特性也描述的非常全面甚至超过了，在这里，基于我们上面的系统知识介绍，列举一下goroutine的特性，算是小结：&lt;/p&gt;

&lt;p&gt;(1) goroutine是Go语言运行库的功能，不是操作系统提供的功能，goroutine不是用线程实现的。具体可参见Go语言源码里的pkg/runtime/proc.c&lt;/p&gt;

&lt;p&gt;(2) goroutine就是一段代码，一个函数入口，以及在堆上为其分配的一个堆栈。所以它非常廉价，我们可以很轻松的创建上万个goroutine，但它们并不是被操作系统所调度执行&lt;/p&gt;

&lt;p&gt;(3) 除了被系统调用阻塞的线程外，Go运行库最多会启动$GOMAXPROCS个线程来运行goroutine&lt;/p&gt;

&lt;p&gt;(4) goroutine是协作式调度的，如果goroutine会执行很长时间，而且不是通过等待读取或写入channel的数据来同步的话，就需要主动调用Gosched()来让出CPU&lt;/p&gt;

&lt;p&gt;(5) 和所有其他并发框架里的协程一样，goroutine里所谓“无锁”的优点只在单线程下有效，如果$GOMAXPROCS &amp;gt; 1并且协程间需要通信，Go运行库会负责加锁保护数据，这也是为什么sieve.go这样的例子在多CPU多线程时反而更慢的原因&lt;/p&gt;

&lt;p&gt;(6) Web等服务端程序要处理的请求从本质上来讲是并行处理的问题，每个请求基本独立，互不依赖，几乎没有数据交互，这不是一个并发编程的模型，而并发编程框架只是解决了其语义表述的复杂性，并不是从根本上提高处理的效率，也许是并发连接和并发编程的英文都是concurrent吧，很容易产生“并发编程框架和coroutine可以高效处理大量并发连接”的误解。&lt;/p&gt;

&lt;p&gt;(7) Go语言运行库封装了异步IO，所以可以写出貌似并发数很多的服务端，可即使我们通过调整$GOMAXPROCS来充分利用多核CPU并行处理，其效率也不如我们利用IO事件驱动设计的、按照事务类型划分好合适比例的线程池。在响应时间上，协作式调度是硬伤。&lt;/p&gt;

&lt;p&gt;(8) goroutine最大的价值是其实现了并发协程和实际并行执行的线程的映射以及动态扩展，随着其运行库的不断发展和完善，其性能一定会越来越好，尤其是在CPU核数越来越多的未来，终有一天我们会为了代码的简洁和可维护性而放弃那一点点性能的差别。&lt;/p&gt;

</description>
        <pubDate>Sun, 24 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/24/goroutine.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/24/goroutine.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>goclipse eclipse go 开发环境搭建＋跳转支持</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Eclipse Mars (4.5)  下载地址：https://www.eclipse.org/mars/
golang 1.6.0rc2 下载地址：http://www.golangtc.com/download
goclipse 0.14.1 下载地址: https://github.com/GoClipse/goclipse
eclipse在线安装地址：http://goclipse.github.io/releases/
eclipse+goclipse安装后不能联想和跳转问题解决
Window-&amp;gt;Preferences-&amp;gt;Go
指定$GOPATH 和$GOROOT
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/goclipse.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;go get -u -v github.com/nsf/gocode&lt;/p&gt;

&lt;p&gt;go install -u -v github.com/nsf/gocode&lt;/p&gt;

&lt;p&gt;go get -v github.com/rogpeppe/godef&lt;/p&gt;

&lt;p&gt;go install -v github.com/rogpeppe/godef&lt;/p&gt;

&lt;p&gt;go get -u -v golang.org/x/tools/cmd/guru&lt;/p&gt;

&lt;p&gt;go install  -u -v golang.org/x/tools/cmd/guru&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/goClipseTool.png&quot; /&gt;
至此可以完美支持跳转&lt;/p&gt;
</description>
        <pubDate>Sun, 24 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/12/24/goclipse.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/12/24/goclipse.html</guid>
        
        
        <category>web</category>
        
      </item>
    
  </channel>
</rss>
