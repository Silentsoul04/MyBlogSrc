<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 15 Nov 2017 10:58:12 +0800</pubDate>
    <lastBuildDate>Wed, 15 Nov 2017 10:58:12 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>spark_memory</title>
        <description>&lt;p&gt;在 spark-env.sh 中添加：&lt;/p&gt;

&lt;p&gt;export JAVA_HOME=/home/tom/jdk1.8.0_73/
export SCALA_HOME=/home/tom//scala-2.10.6
export SPARK_MASTER_IP=localhost
export SPARK_WORKER_MEMORY=4G&lt;/p&gt;

&lt;p&gt;内存设置太少会导致运行失败
可以代码中设置内存
//set new runtime options
spark.conf.set(“spark.sql.shuffle.partitions”, 1)
spark.conf.set(“spark.executor.memory”, “512m”)&lt;/p&gt;

&lt;p&gt;Spark虽然是in memory的运算平台，但从官方资料看，似乎本身对内存的要求并不是特别苛刻。官方网站只是要求内存在8GB之上即可（Impala要求机器配置在128GB）Spark建议需要提供至少75%的内存空间分配给Spark，至于其余的内存空间，则分配给操作系统与buffer cache。Spark对内存的消耗主要分为三部分（即取决于你的应用程序的需求）：
      数据集中对象的大小；
      访问这些对象的内存消耗；
      垃圾回收GC的消耗&lt;/p&gt;

&lt;p&gt;spark executor都是装载在container里运行，container默认的内存是1G（参数yarn.scheduler.minimum-allocation-mb定义），executor分配的内存是executor-memory，所以向YARN申请的内存是（executor-memory + 1）* num-executors。 Executor 内存的大小，和性能本身当然并没有直接的关系，但是几乎所有运行时性能相关的内容都或多或少间接和内存大小相关。这个参数最终会被设置到Executor的JVM的heap尺寸上。如果Executor的数量和内存大小受机器物理配置影响相对固定，那么你就需要合理规划每个分区任务的数据规模，例如采用更多的分区，用增加任务数量（进而需要更多的批次来运算所有的任务）的方式来减小每个任务所需处理的数据大小。&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/15/spark_memory.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/15/spark_memory.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark toDF 失败原因总结</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;错误提示
value toDF is not a member of org.apache.spark.rdd.RDD[(org.apache.spark.ml.&lt;/p&gt;

&lt;p&gt;解决办法
 val conf = new SparkConf().setAppName(“SimpleParamsExample1”)
    val sc = new SparkContext(conf)&lt;/p&gt;

&lt;p&gt;val sqlContext= new org.apache.spark.sql.SQLContext(sc)
  import sqlContext.implicits._&lt;/p&gt;

&lt;p&gt;错误: 找不到或无法加载主类 example.Statistics
译器顺序：右键项目-properties-scala Compiler -Build manager ：
 set the compile order to JavaThenScala instead of Mixed&lt;/p&gt;

&lt;p&gt;右键项目-properties-scala Compiler -Standard 
 选择安装的scala 版本&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_toDF.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_toDF.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark_start问题原因及解决办法</title>
        <description>&lt;console&gt;:14: error: not found: value spark
       import spark.implicits._
              ^
&lt;console&gt;:14: error: not found: value spark
       import spark.sql
              ^
              
scala&amp;gt;  var rdd = sc.parallelize(1 to 10)
&lt;console&gt;:39: error: not found: value sc
        var rdd = sc.parallelize(1 to 10)
        
        
日志：
Caused by: java.lang.RuntimeException: java.net.ConnectException: Call From localhost/127.0.0.1 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

hadoop没有启动
启动hadoop

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)
Type in expressions to have them evaluated.
Type :help for more information.

scala&amp;gt; var rdd = sc.parallelize(1 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24
&lt;/console&gt;&lt;/console&gt;&lt;/console&gt;&lt;/console&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_start.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_start.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark_rdd创建转换</title>
        <description>&lt;p&gt;RDD创建方式
1）从Hadoop文件系统（如HDFS、Hive、HBase）输入创建。
2）从父RDD转换得到新RDD。
3）通过parallelize或makeRDD将单机数据创建为分布式RDD。
4）基于DB(Mysql)、NoSQL(HBase)、S3(SC3)、数据流创建。&lt;/p&gt;

&lt;p&gt;从集合创建RDD
parallelize
def parallelize&lt;a href=&quot;seq: Seq[T], numSlices: Int = defaultParallelism&quot;&gt;T&lt;/a&gt;(implicit arg0: ClassTag[T]): RDD[T]
从一个Seq集合创建RDD。
参数1：Seq集合，必须。
参数2：分区数，默认为该Application分配到的资源的CPU核数
scala&amp;gt; var rdd = sc.parallelize(1 to 10)&lt;/p&gt;

&lt;p&gt;makeRDD
def makeRDD&lt;a href=&quot;seq: Seq[T], numSlices: Int = defaultParallelism&quot;&gt;T&lt;/a&gt;(implicit arg0: ClassTag[T]): RDD[T]
这种用法和parallelize完全相同
def makeRDD&lt;a href=&quot;seq: Seq[(T, Seq[String])]&quot;&gt;T&lt;/a&gt;(implicit arg0: ClassTag[T]): RDD[T]
该用法可以指定每一个分区的preferredLocations。
scala&amp;gt; var rdd=sc.makeRDD(Seq((1 to 10)))&lt;/p&gt;

&lt;p&gt;从外部存储创建RDD&lt;/p&gt;

&lt;p&gt;textFile
//从hdfs文件创建.
//从hdfs文件创建&lt;br /&gt;
scala&amp;gt; var rdd = sc.textFile(“hdfs:///tmp/lxw1234/1.txt”)&lt;br /&gt;
//从本地文件创建&lt;br /&gt;
scala&amp;gt; var rdd = sc.textFile(“file:///etc/hadoop/conf/core-site.xml”)&lt;/p&gt;

&lt;p&gt;注意这里的本地文件路径需要在Driver和Executor端存在。
从其他HDFS文件格式创建
hadoopFile
sequenceFile
objectFile
newAPIHadoopFile
从Hadoop接口API创建
hadoopRDD
newAPIHadoopRDD
比如：从HBase创建RDD&lt;/p&gt;

&lt;p&gt;scala&amp;gt;  var rdd = sc.parallelize(1 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:26&lt;/console&gt;&lt;/p&gt;

&lt;p&gt;scala&amp;gt; rdd.foreach(println(_))
3
8
1
9
10
4
5
2
6
7&lt;/p&gt;

&lt;p&gt;1.RDD -&amp;gt; Dataset 
scala&amp;gt; val ds = rdd.toDS()
ds: org.apache.spark.sql.Dataset[Int] = [value: int]
scala&amp;gt; ds.foreach(println(_))
1
2
6
7
3
4
5
8
9
10&lt;/p&gt;

&lt;p&gt;2.RDD -&amp;gt; DataFrame 
val df = spark.read.json(rdd)
scala&amp;gt; val df=rdd.toDF()
df: org.apache.spark.sql.DataFrame = [value: int]&lt;/p&gt;

&lt;p&gt;scala&amp;gt; df.foreach(println(_))
[1]
[3]
[2]
[6]
[7]
[4]
[8]
[9]
[10]
[5]&lt;/p&gt;

&lt;p&gt;3.Dataset -&amp;gt; RDD
val rdd = ds.rdd&lt;/p&gt;

&lt;p&gt;4.Dataset -&amp;gt; DataFrame
val df = ds.toDF()&lt;/p&gt;

&lt;p&gt;5.DataFrame -&amp;gt; RDD
val rdd = df.toJSON.rdd&lt;/p&gt;

&lt;p&gt;6.DataFrame -&amp;gt; Dataset
val ds = df.toJSON&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_rdd.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_rdd.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark的ML和MLLib两个包区别和联系</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;ML的API是面向Dataset的（Dataframe是Dataset的子集，也就是Dataset[Row]）， mllib是面对RDD的。Dataset和RDD有啥不一样呢？Dataset的底端是RDD。Dataset对RDD进行了更深一层的优化，比如说有sql语言类似的黑魔法，Dataset支持静态类型分析所以在compile time就能报错，各种combinators（map，foreach等）性能会更好&lt;/p&gt;

&lt;p&gt;spark.mllib中的算法接口是基于RDDs的；spark.ml中的算法接口是基于DataFrames的。实际使用中推荐ml，建立在DataFrames基础上的ml中一系列算法更适合创建包含从数据清洗到特征工程再到模型训练等一系列工作的ML pipeline；而且未来mllib也会被弃用。&lt;/p&gt;

&lt;p&gt;Spark机器学习库现支持两种接口的API:RDD-based和DataFrame-based，Spark官方网站上说，RDD-based APIs在2.0后进入维护模式，主要的机器学习API是spark-ml包中的DataFrame-based API，并将在3.0后完全移除RDD-based API。&lt;/p&gt;

&lt;p&gt;DataFrame-based API 包含在org.apache.spark.ml包中，其中主要的类结构如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/sparkml.png&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_ml_mllib.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_ml_mllib.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>RDD/Dataset/DataFrame互转</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;1.RDD -&amp;gt; Dataset 
val ds = rdd.toDS()&lt;/p&gt;

&lt;p&gt;2.RDD -&amp;gt; DataFrame 
val df = spark.read.json(rdd)&lt;/p&gt;

&lt;p&gt;3.Dataset -&amp;gt; RDD
val rdd = ds.rdd&lt;/p&gt;

&lt;p&gt;4.Dataset -&amp;gt; DataFrame
val df = ds.toDF()&lt;/p&gt;

&lt;p&gt;5.DataFrame -&amp;gt; RDD
val rdd = df.toJSON.rdd&lt;/p&gt;

&lt;p&gt;6.DataFrame -&amp;gt; Dataset
val ds = df.toJSON&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/rdd_dataset_dataframe.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/rdd_dataset_dataframe.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>dataSet和dataFrame的创建方法</title>
        <description>&lt;p&gt;Spark创建DataFrame的三种方法
跟关系数据库的表(Table)一样，DataFrame是Spark中对带模式(schema)行列数据的抽象。DateFrame广泛应用于使用SQL处理大数据的各种场景。&lt;/p&gt;

&lt;p&gt;通过导入(importing)Spark sql implicits, 就可以将本地序列(seq), 数组或者RDD转为DataFrame。只要这些数据的内容能指定数据类型即可。
本地seq + toDF创建DataFrame示例：&lt;/p&gt;

&lt;p&gt;import sqlContext.implicits._
val df = Seq(
  (1, “First Value”, java.sql.Date.valueOf(“2010-01-01”)),
  (2, “Second Value”, java.sql.Date.valueOf(“2010-02-01”))
).toDF(“int_column”, “string_column”, “date_column”)
注意：如果直接用toDF()而不指定列名字，那么默认列名为”_1”, “_2”, …&lt;/p&gt;

&lt;p&gt;通过case class + toDF创建DataFrame的示例&lt;/p&gt;

&lt;p&gt;// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._&lt;/p&gt;

&lt;p&gt;// Define the schema using a case class.
// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
// you can use custom classes that implement the Product interface.
case class Person(name: String, age: Int)&lt;/p&gt;

&lt;p&gt;// Create an RDD of Person objects and register it as a table.
val people = sc.textFile(“examples/src/main/resources/people.txt”).map(_.split(“,”)).map(p =&amp;gt; Person(p(0), p(1).trim.toInt)).toDF()
people.registerTempTable(“people”)&lt;/p&gt;

&lt;p&gt;// 使用 sqlContext 执行 sql 语句.
val teenagers = sqlContext.sql(“SELECT name FROM people WHERE age &amp;gt;= 13 AND age &amp;lt;= 19”)&lt;/p&gt;

&lt;p&gt;// 注：sql()函数的执行结果也是DataFrame，支持各种常用的RDD操作.
// The columns of a row in the result can be accessed by ordinal.
teenagers.map(t =&amp;gt; “Name: “ + t(0)).collect().foreach(println)&lt;/p&gt;

&lt;p&gt;方法二，Spark中使用createDataFrame函数创建DataFrame&lt;/p&gt;

&lt;p&gt;在SqlContext中使用createDataFrame也可以创建DataFrame。跟toDF一样，这里创建DataFrame的数据形态也可以是本地数组或者RDD。
通过row+schema创建示例&lt;/p&gt;

&lt;p&gt;import org.apache.spark.sql.types._
val schema = StructType(List(
    StructField(“integer_column”, IntegerType, nullable = false),
    StructField(“string_column”, StringType, nullable = true),
    StructField(“date_column”, DateType, nullable = true)
))&lt;/p&gt;

&lt;p&gt;val rdd = sc.parallelize(Seq(
  Row(1, “First Value”, java.sql.Date.valueOf(“2010-01-01”)),
  Row(2, “Second Value”, java.sql.Date.valueOf(“2010-02-01”))
))
val df = sqlContext.createDataFrame(rdd, schema)
方法三，通过文件直接创建DataFrame&lt;/p&gt;

&lt;p&gt;使用parquet文件创建&lt;/p&gt;

&lt;p&gt;val df = sqlContext.read.parquet(“hdfs:/path/to/file”)
使用json文件创建&lt;/p&gt;

&lt;p&gt;val df = spark.read.json(“examples/src/main/resources/people.json”)&lt;/p&gt;

&lt;p&gt;// Displays the content of the DataFrame to stdout
df.show()
// +—-+——-+
// | age|   name|
// +—-+——-+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +—-+——-+
使用csv文件,spark2.0+之后的版本可用&lt;/p&gt;

&lt;p&gt;//首先初始化一个SparkSession对象
val spark = org.apache.spark.sql.SparkSession.builder
        .master(“local”)
        .appName(“Spark CSV Reader”)
        .getOrCreate;&lt;/p&gt;

&lt;p&gt;//然后使用SparkSessions对象加载CSV成为DataFrame
val df = spark.read
        .format(“com.databricks.spark.csv”)
        .option(“header”, “true”) //reading the headers
        .option(“mode”, “DROPMALFORMED”)
        .load(“csv/file/path”); //.csv(“csv/file/path”) //spark 2.0 api&lt;/p&gt;

&lt;p&gt;df.show()&lt;/p&gt;

&lt;p&gt;DataSet数据集是一个强类型的域特定对象的集合，可以使用功能或关系操作并行转换.。每个数据集还有一个无类型的视图称为Dataframe，这是一个行(Row)的数据集。
在DataSet上的操作，分为transformations和actions。transformations会产生新的数据集（DataSet），而actions则是触发计算并产生结果。transformations包括：map, filter, select, and aggregate (&lt;code&gt;groupBy&lt;/code&gt;). 等操作。而actions 包括： count, show 或把数据写入文件系统中。
DataSet是懒惰(‘lazy’)的，也就是说当一个action被调用时才会触发一个计算。在内部实现，数据集表示的是一个逻辑计划，它描述了生成数据所需的计算。当action被调用时，spark的查询优化器会优化这个逻辑计划，并生成一个物理计划，该物理计划可以通过并行和分布式的方式来执行。使用&lt;code&gt;explain&lt;/code&gt;解释函数，来进行逻辑计划的探索和物理计划的优化。
为了有效地支持特定领域的对象，Encoder（编码器）是必需的。例如，给出一个Person的类，有两个字段：name(string)和age(int)，通过一个encoder来告诉spark在运行的时候产生代码把Person对象转换成一个二进制结构。这种二进制结构通常有更低的内存占用，以及优化的数据处理效率（例如在一个柱状格式）。若要了解数据的内部二进制表示，请使用schema(表结构)函数。&lt;/p&gt;

&lt;p&gt;创建一个数据集有两种方式。
1, 第一种也是最常用的一种，就是利用SparkSession的read函数在存储系统中读取一个文件。代码如下：
val people = spark.read.parquet(“…”).as[Person]&lt;/p&gt;

&lt;p&gt;2, 利用Datasets的转换(transformations)函数，例如下面的代码：利用filter来创建一个新的Dataset。&lt;/p&gt;

&lt;p&gt;数据集(Dataset)的操作是无类型的，通过各种DSL(domain-specific-language)函数，这些函数是基于数据集Dataset , 类[[Column]],和 函数[[functions]]来定义的。这些操作非常类似基于R和Python抽象出来的data frame的操作。
从Dateset中选择一列
val ageCol = people(“age”) // in Scala&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/dataSet_dataFrame.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/dataSet_dataFrame.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>php_session</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;void session_write_close ( void )&lt;/p&gt;

&lt;p&gt;End the current session and store session data.&lt;/p&gt;

&lt;p&gt;Session data is usually stored after your script terminated without the need to call session_write_close(), but as session data is locked to prevent concurrent writes only one script may operate on a session at any time. When using framesets together with sessions you will experience the frames loading one by one due to this locking. You can reduce the time needed to load all the frames by ending the session as soon as all changes to session variables are done.
也就是说session是有锁的，为防止并发的写会话数据,php自带的的文件保存会话数据是加了一个互斥锁（在session_start()的时候）。 
程序执行session_start()，此时当前程序就开始持有锁。 
程序结束，此时程序自动释放Session的锁。&lt;/p&gt;

&lt;p&gt;如果同一个客户端同时并发发送多个请求（如ajax在页面同时发送多个请求），且脚本执行时间较长，就会导致session文件阻塞，影响性能。因为对于每个请求，PHP执行session_start()，就会取得文件独占锁，只有在该请求处理结束后，才会释放独占锁。这样，同时多个请求就会引起阻塞。解决方案如下： 
修改会话变量后，立即使用session_write_close()来保存会话数据并释放文件锁。
session_start(); &lt;br /&gt;
$_SESSION[‘test’] = ‘test’;
session_write_close();&lt;/p&gt;

&lt;p&gt;(PHP 5 &amp;gt;= 5.3.3, PHP 7)
fastcgi_finish_request — 冲刷(flush)所有响应的数据给客户端
如果有锁的话会使异步作用失效&lt;/p&gt;

&lt;p&gt;There are some pitfalls  you should be aware of when using this function.&lt;/p&gt;

&lt;p&gt;The script will still occupy a FPM process after fastcgi_finish_request(). So using it excessively for long running tasks may occupy all your FPM threads up to pm.max_children. This will lead to gateway errors on the webserver.&lt;/p&gt;

&lt;p&gt;Another important thing is session handling. Sessions are locked as long as they’re active (see the documentation for session_write_close()). This means subsequent requests will block until the session is closed.&lt;/p&gt;

&lt;p&gt;You should therefore call session_write_close() as soon as possible (even before fastcgi_finish_request()) to allow subsequent requests and a good user experience.&lt;/p&gt;

&lt;p&gt;This also applies for all other locking techniques as flock or database locks for example. As long as a lock is active subsequent requests might bock.&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/11/09/php_session.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/11/09/php_session.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>jupyter_matplotlib</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;安装：
$pip install matplotlib
$jupyter notebook&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;module://ipykernel.pylab.backend_inline&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5 &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterplot.png&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# We&amp;#39;ll explain the &amp;quot;111&amp;quot; later. Basically, 1 row and 1 column.&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;An Example Axes&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Y-Axis&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;X-Axis&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterplotAxes.png&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;lightblue&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#绘制线&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;26&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;darkgreen&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;^&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#绘制散点图&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterplotLine.png&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterplotMulti.png&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 09 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/11/09/jupyter_matplotlib.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/11/09/jupyter_matplotlib.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>Jupyter_slides</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;使用jupyter完成后，需要将后缀为.ipynb文件转换成.html文件才能展示出效果&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;打开命令行终端，运行jupyter notebook&lt;/li&gt;
  &lt;li&gt;在制作slides时，首先在view中，将视图切换到Slidesshow
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterSlider.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;在要编辑的文本行中，slice type中选择slice或sub-slice。若选择slice，slice之间是左右切换，每个slice和sub-slice相当于一张幻灯片。同一个slice和它的菜单sub-slice之间是上下切换。
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterSunSlider.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;完成slice的制作后，我们就可以将.ipynb文件转换生成.html文件，以网页的形式展示幻灯片。&lt;/li&gt;
  &lt;li&gt;再打开一个命令行终端，进入所要转换的文件目录下，运行一下命令，生成html文件。
jupyter-nbconvert –to slides test.ipynb –reveal-prefix  ‘https://cdn.bootcss.com/reveal.js/3.5.0’ –output test
6.至此，完成了slice的制作，效果图如下
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterResult.png&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;labels
菜单栏选择View—&amp;gt;Toggle Toolbar—&amp;gt;打开&lt;/p&gt;

&lt;p&gt;菜单栏选择View—&amp;gt;Cell Toolbar—&amp;gt;Slidesshow—&amp;gt;选择&lt;/p&gt;

&lt;p&gt;Slide
单个view，左右滑动切换&lt;/p&gt;

&lt;p&gt;Sub-Slide
Cell的sub-cell，上下滑动切换&lt;/p&gt;

&lt;p&gt;Fragment
这个是Slide或Sub-Slide的属性，可以按次序展示，单击一次出现一条&lt;/p&gt;

&lt;p&gt;Skip
跳过，注释非演示代码用的&lt;/p&gt;

&lt;p&gt;Notes
在页面按s就可以跳出来的注释&lt;/p&gt;

&lt;p&gt;Reveal
themes
Sky, Beige, Serif, etc.&lt;/p&gt;

&lt;p&gt;transitions
Cube, Zoom, None, etc.&lt;/p&gt;

&lt;p&gt;gen
jupyter-nbconvert –to slides Python_Share.ipynb –reveal-prefix ‘//cdn.bootcss.com/reveal.js/3.2.0’ –output Python_Share
server
python -m SimpleHTTPServer 8000&lt;/p&gt;
</description>
        <pubDate>Thu, 09 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/11/09/Jupyter_slides.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/11/09/Jupyter_slides.html</guid>
        
        
        <category>web</category>
        
      </item>
    
  </channel>
</rss>
