<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 10 Dec 2017 16:43:34 +0800</pubDate>
    <lastBuildDate>Sun, 10 Dec 2017 16:43:34 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>truss、strace或ltrace</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;truss和strace用来 跟踪一个进程的系统调用或信号产生的情况，而 ltrace用来 跟踪进程调用库函数的情况。truss是早期为System V R4开发的调试程序，包括Aix、FreeBSD在内的大部分Unix系统都自带了这个工具；而strace最初是为SunOS系统编写的，ltrace最早出现在GNU/Debian Linux中。这两个工具现在也已被移植到了大部分Unix系统中，大多数Linux发行版都自带了strace和ltrace，而FreeBSD也可通过Ports安装它们。
你不仅可以从命令行调试一个新开始的程序，也可以把truss、strace或ltrace绑定到一个已有的PID上来调试一个正在运行的程序。三个调试工具的基本使用方法大体相同，下面仅介绍三者共有，而且是最常用的三个命令行参数：
-f ：除了跟踪当前进程外，还跟踪其子进程。
-o file ：将输出信息写到文件file中，而不是显示到标准错误输出（stderr）。
-p pid ：绑定到一个由pid对应的正在运行的进程。此参数常用来调试后台进程。
使用上述三个参数基本上就可以完成大多数调试任务了，下面举几个命令行例子：
truss -o ls.truss ls -al： 跟踪ls -al的运行，将输出信息写到文件/tmp/ls.truss中。
strace -f -o vim.strace vim： 跟踪vim及其子进程的运行，将输出信息写到文件vim.strace。
ltrace -p 234： 跟踪一个pid为234的已经在运行的进程。
三个调试工具的输出结果格式也很相似，以strace为例：
brk(0)                                  = 0x8062aa8
brk(0x8063000)                          = 0x8063000
mmap2(NULL, 4096, PROT_READ, MAP_PRIVATE, 3, 0x92f) = 0x40016000
每一行都是一条系统调用，等号左边是系统调用的函数名及其参数，右边是该调用的返回值。 truss、strace和ltrace的工作原理大同小异，都是使用ptrace系统调用跟踪调试运行中的进程，详细原理不在本文讨论范围内，有兴趣可以参考它们的源代码。
每一行都是一条系统调用，等号左边是系统调用的函数名及其参数，右边是该调用的返回值。
strace 显示这些调用的参数并返回符号形式的值。strace 从内核接收信息，而且不需要以任何特殊的方式来构建内核。
-c 统计每一系统调用的所执行的时间,次数和出错的次数等. 
-d 输出strace关于标准错误的调试信息. 
-f 跟踪由fork调用所产生的子进程. 
-ff 如果提供-o filename,则所有进程的跟踪结果输出到相应的filename.pid中,pid是各进程的进程号. 
-F 尝试跟踪vfork调用.在-f时,vfork不被跟踪. 
-h 输出简要的帮助信息. 
-i 输出系统调用的入口指针. 
-q 禁止输出关于脱离的消息. 
-r 打印出相对时间关于,,每一个系统调用. 
-t 在输出中的每一行前加上时间信息. 
-tt 在输出中的每一行前加上时间信息,微秒级. 
-ttt 微秒级输出,以秒了表示时间. 
-T 显示每一调用所耗的时间. 
-v 输出所有的系统调用.一些调用关于环境变量,状态,输入输出等调用由于使用频繁,默认不输出. 
-V 输出strace的版本信息. 
-x 以十六进制形式输出非标准字符串 
-xx 所有字符串以十六进制形式输出. 
-a column 
设置返回值的输出位置.默认 为40. 
-e expr 
指定一个表达式,用来控制如何跟踪.格式如下: 
[qualifier=][!]value1[,value2]… 
qualifier只能是 trace,abbrev,verbose,raw,signal,read,write其中之一.value是用来限定的符号或数字.默认的 qualifier是 trace.感叹号是否定符号.例如: 
-eopen等价于 -e trace=open,表示只跟踪open调用.而-etrace!=open表示跟踪除了open以外的其他调用.有两个特殊的符号 all 和 none. 
注意有些shell使用!来执行历史记录里的命令,所以要使用\. 
-e trace=set 
只跟踪指定的系统 调用.例如:-e trace=open,close,rean,write表示只跟踪这四个系统调用.默认的为set=all. 
-e trace=file 
只跟踪有关文件操作的系统调用. 
-e trace=process 
只跟踪有关进程控制的系统调用. 
-e trace=network 
跟踪与网络有关的所有系统调用. 
-e strace=signal 
跟踪所有与系统信号有关的 系统调用 
-e trace=ipc 
跟踪所有与进程通讯有关的系统调用 
-e abbrev=set 
设定 strace输出的系统调用的结果集.-v 等与 abbrev=none.默认为abbrev=all. 
-e raw=set 
将指 定的系统调用的参数以十六进制显示. 
-e signal=set 
指定跟踪的系统信号.默认为all.如 signal=!SIGIO(或者signal=!io),表示不跟踪SIGIO信号. 
-e read=set 
输出从指定文件中读出 的数据.例如: 
-e read=3,5 
-e write=set 
输出写入到指定文件中的数据. 
-o filename 
将strace的输出写入文件filename 
-p pid 
跟踪指定的进程pid. 
-s strsize 
指定输出的字符串的最大长度.默认为32.文件名一直全部输出. 
-u username 
以username 的UID和GID执行被跟踪的命令
通用的完整用法：&lt;/p&gt;

&lt;p&gt;strace -o output.txt -T -tt -e trace=all -p 28979
上面的含义是 跟踪28979进程的所有系统调用（-e trace=all），并统计系统调用的花费时间，以及开始时间（并以可视化的时分秒格式显示），最后将记录结果存在output.txt文件里面。&lt;/p&gt;

</description>
        <pubDate>Sun, 10 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/10/strace.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/10/strace.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>netlink</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Linux中的进程间通信机制源自于Unix平台上的进程通信机制。Unix的两大分支AT&amp;amp;T Unix和BSD Unix在进程通信实现机制上的各有所不同，前者形成了运行在单个计算机上的System V IPC，后者则实现了基于socket的进程间通信机制。同时Linux也遵循IEEE制定的Posix IPC标准，在三者的基础之上实现了以下几种主要的IPC机制：管道(Pipe)及命名管道(Named Pipe)，信号(Signal)，消息队列(Message queue)，共享内存(Shared Memory)，信号量(Semaphore)，套接字(Socket)。通过这些IPC机制，用户空间进程之间可以完成互相通信。为了完成内核空间与用户空间通信，Linux提供了基于socket的Netlink通信机制，可以实现内核与用户空间数据的及时交换。
本文第2节概述相关研究工作，第3节与其他IPC机制对比，详细介绍Netlink机制及其关键技术，第4节使用KGDB+GDB组合调试，通过一个示例程序演示Netlink通信过程。第5节做总结并指出Netlink通信机制的不足之处。
2 相关研究
到目前Linux提供了9种机制完成内核与用户空间的数据交换，分别是内核启动参数、模块参数与 sysfs、sysctl、系统调用、netlink、procfs、seq_file、debugfs和relayfs，其中模块参数与sysfs、procfs、debugfs、relayfs是基于文件系统的通信机制，用于内核空间向用户控件输出信息；sysctl、系统调用是由用户空间发起的通信机制。由此可见，以上均为单工通信机制，在内核空间与用户空间的双向互动数据交换上略显不足。Netlink是基于socket的通信机制，由于socket本身的双共性、突发性、不阻塞特点，因此能够很好的满足内核与用户空间小量数据的及时交互，因此在Linux 2.6内核中广泛使用，例如SELinux，Linux系统的防火墙分为内核态的netfilter和用户态的iptables，netfilter与iptables的数据交换就是通过Netlink机制完成。 
3 Netlink机制及其关键技术
3.1 Netlink机制&lt;/p&gt;

&lt;p&gt;Linux操作系统中当CPU处于内核状态时，可以分为有用户上下文的状态和执行硬件、软件中断两种。其中当处于有用户上下文时，由于内核态和用户态的内存映射机制不同，不可直接将本地变量传给用户态的内存区；处于硬件、软件中断时，无法直接向用户内存区传递数据，代码执行不可中断。针对传统的进程间通信机制，他们均无法直接在内核态和用户态之间使用，原因如下表：
通信方法
无法介于内核态与用户态的原因
管道（不包括命名管道）
局限于父子进程间的通信。
消息队列
在硬、软中断中无法无阻塞地接收数据。
信号量
无法介于内核态和用户态使用。
内存共享
需要信号量辅助，而信号量又无法使用。
套接字
在硬、软中断中无法无阻塞地接收数据。
1*（引自 参考文献5）
    解决内核态和用户态通信机制可分为两类：
处于有用户上下文时，可以使用Linux提供的copy_from_user()和copy_to_user()函数完成，但由于这两个函数可能阻塞，因此不能在硬件、软件的中断过程中使用。
处于硬、软件中断时。
2.1   可以通过Linux内核提供的spinlock自旋锁实现内核线程与中断过程的同步，由于内核线程运行在有上下文的进程中，因此可以在内核线程中使用套接字或消息队列来取得用户空间的数据，然后再将数据通过临界区传递给中断过程.
2.2   通过Netlink机制实现。Netlink 套接字的通信依据是一个对应于进程的标识，一般定为该进程的 ID。Netlink通信最大的特点是对对中断过程的支持，它在内核空间接收用户空间数据时不再需要用户自行启动一个内核线程，而是通过另一个软中断调用用户事先指定的接收函数。通过软中断而不是自行启动内核线程保证了数据传输的及时性。
3.2 Netlink优点&lt;/p&gt;

&lt;p&gt;Netlink相对于其他的通信机制具有以下优点：
使用Netlink通过自定义一种新的协议并加入协议族即可通过socket API使用Netlink协议完成数据交换，而ioctl和proc文件系统均需要通过程序加入相应的设备或文件。
Netlink使用socket缓存队列，是一种异步通信机制，而ioctl是同步通信机制，如果传输的数据量较大，会影响系统性能。
Netlink支持多播，属于一个Netlink组的模块和进程都能获得该多播消息。
Netlink允许内核发起会话，而ioctl和系统调用只能由用户空间进程发起。&lt;/p&gt;

&lt;p&gt;Netlink套接字是用以实现用户进程与内核进程通信的一种特殊的进程间通信(IPC) ,也是网络应用程序与内核通信的最常用的接口。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Netlink 是一种特殊的 socket，它是 Linux 所特有的，类似于 BSD 中的AF_ROUTE 但又远比它的功能强大，目前在Linux 内核中
使用netlink 进行应用与内核通信的应用很多; 包括：路由 daemon（NETLINK_ROUTE），用户态 socket 协议（NETLINK_USERSOCK），
防火墙（NETLINK_FIREWALL），netfilter 子系统（NETLINK_NETFILTER），内核事件向用户态通知（NETLINK_KOBJECT_UEVENT），
通用 netlink（NETLINK_GENERIC）等。

Netlink 是一种在内核与用户应用间进行双向数据传输的非常好的方式，用户态应用使用标准的 socket API 就可以使用 netlink 提供的强大功能，
内核态需要使用专门的内核 API 来使用 netlink。
Netlink 相对于系统调用，ioctl 以及 /proc文件系统而言具有以下优点：
1，netlink使用简单，只需要在include/linux/netlink.h中增加一个新类型的 netlink 协议定义即可,(如 #define NETLINK_TEST 20 然后，内核和用户态应用就可以立即通过 socket API 使用该 netlink 协议类型进行数据交换);
2. netlink是一种异步通信机制，在内核与用户态应用之间传递的消息保存在socket缓存队列中，发送消息只是把消息保存在接收者的socket的接收队列，而不需要等待接收者收到消息；
3．使用 netlink 的内核部分可以采用模块的方式实现，使用 netlink 的应用部分和内核部分没有编译时依赖;
4．netlink 支持多播，内核模块或应用可以把消息多播给一个netlink组，属于该neilink 组的任何内核模块或应用都能接收到该消息，内核事件向用户态的通知机制就使用了这一特性；
5．内核可以使用 netlink 首先发起会话;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Netlink常用数据结构及函数&lt;/p&gt;

&lt;p&gt;　用户态应用使用标准的 socket API有（sendto()），recvfrom()； sendmsg(), recvmsg()）&lt;/p&gt;

&lt;p&gt;　下面简单介绍几种NETLINK用户态通信的常用数据结构&lt;/p&gt;

&lt;p&gt;　1、用户态数据结构&lt;/p&gt;

&lt;p&gt;Netlink通信跟常用UDP Socket通信类似：
　struct sockaddr_nl 是netlink通信地址跟普通socket struct sockaddr_in类似
  struct sockaddr_nl结构&lt;/p&gt;
</description>
        <pubDate>Sun, 10 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/10/netlink.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/10/netlink.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>linux sysfs</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;在调试驱动，或驱动涉及一些参数的输入输出时，难免需要对驱动里的某些变量或内核参数进行读写，或函数调用。此时sysfs接口就很有用了，它可以使得可以在用户空间直接对驱动的这些变量读写或调用驱动的某些函数。sysfs接口与proc文件系统很相似，有人将proc文件系统形容为Windows XP，而将sysfs接口形容为Windows 7。
而在Android系统中，振动器、背光、电源系统等往往使用sysfs接口作为内核空间和用户空间的接口，驱动程序需要提供这些接口内容。&lt;/p&gt;

&lt;p&gt;Sysfs文件系统是一个类 似于proc文件系统的特殊文件系统，用于将系统中的设备组织成层次结构，并向用户模式程序提供详细的内核数据结构信息。&lt;/p&gt;

&lt;p&gt;去/sys看一看，
localhost:/sys#ls /sys/
block/ bus/ class/ devices/ firmware/ kernel/ module/ power/
Block目录：包含所有的块设备
Devices目录：包含系统所有的设备，并根据设备挂接的总线类型组织成层次结构
Bus目录：包含系统中所有的总线类型
Drivers目录：包括内核中所有已注册的设备驱动程序
Class目录：系统中的设备类型（如网卡设备，声卡设备等） 
sys下面的目录和文件反映了整台机器的系统状况。比如bus，
localhost:/sys/bus#ls
i2c/ ide/ pci/ pci express/ platform/ pnp/ scsi/ serio/ usb/
里面就包含了系统用到的一系列总线，比如pci, ide, scsi, usb等等。比如你可以在usb文件夹中发现你使用的U盘，USB鼠标的信息。
我们要讨论一个文件系统，首先要知道这个文件系统的信息来源在哪里。所谓信息来源是指文件组织存放的地点。比如，我们挂载一个分区，
mount -t vfat /dev/hda2 /mnt/C
我们就知道挂载在/mnt/C下的是一个vfat类型的文件系统，它的信息来源是在第一块硬盘的第2个分区。
但是，你可能根本没有去关心过sysfs的挂载过程，她是这样被挂载的。
mount -t sysfs sysfs /sys
ms看不出她的信息来源在哪。sysfs是一个特殊文件系统，并没有一个实际存放文件的介质。断电后就玩完了。简而言之，sysfs的信息来源是kobject层次结构，读一个sysfs文件，就是动态的从kobject结构提取信息，生成文件。&lt;/p&gt;

&lt;p&gt;Kobject 
Kobject 是Linux 2.6引入的新的设备管理机制，在内核中由struct kobject表示。通过这个数据结构使所有设备在底层都具有统一的接口，kobject提供基本的对象管理，是构成Linux2.6设备模型的核心结 构，它与sysfs文件系统紧密关联，每个在内核中注册的kobject对象都对应于sysfs文件系统中的一个目录。Kobject是组成设备模型的基 本结构。类似于C++中的基类，它嵌入于更大的对象的对象中–所谓的容器–用来描述设备模型的组件。如bus,devices, drivers 都是典型的容器。这些容器就是通过kobject连接起来了，形成了一个树状结构。这个树状结构就与/sys向对应。
kobject 结构为一些大的数据结构和子系统提供了基本的对象管理，避免了类似机能的重复实现。这些机能包括&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;对象引用计数.&lt;/li&gt;
  &lt;li&gt;维护对象链表(集合).&lt;/li&gt;
  &lt;li&gt;对象上锁.&lt;/li&gt;
  &lt;li&gt;在用户空间的表示.
Kobject结构定义为：
struct kobject {
char * k name; 指向设备名称的指针
char name[KOBJ NAME LEN]; 设备名称
struct kref kref; 对象引用计数
struct list head entry; 挂接到所在kset中去的单元
struct kobject * parent; 指向父对象的指针
struct kset * kset; 所属kset的指针
struct kobj type * ktype; 指向其对象类型描述符的指针
struct dentry * dentry; sysfs文件系统中与该对象对应的文件节点路径指针
};
其 中的kref域表示该对象引用的计数，内核通过kref实现对象引用计数管理，内核提供两个函数kobject_get()、kobject_put() 分别用于增加和减少引用计数，当引用计数为0时，所有该对象使用的资源释放。Ktype 域是一个指向kobj type结构的指针，表示该对象的类型。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kobj type 
struct kobj_type {
void (*release)(struct kobject *);
struct sysfs_ops * sysfs_ops;
struct attribute ** default_attrs;
};
Kobj type数据结构包含三个域：一个release方法用于释放kobject占用的资源；一个sysfs ops指针指向sysfs操作表和一个sysfs文件系统缺省属性列表。Sysfs操作表包括两个函数store()和show()。当用户态读取属性 时，show()函数被调用，该函数编码指定属性值存入buffer中返回给用户态；而store()函数用于存储用户态传入的属性值。&lt;/p&gt;

&lt;p&gt;kset 
kset最重要的是建立上层(sub-system)和下层的 (kobject)的关联性。kobject 也会利用它了分辨自已是属于那一個类型，然後在/sys 下建立正确的目录位置。而kset 的优先权比较高，kobject会利用自已的&lt;em&gt;kset 找到自已所属的kset，並把&lt;/em&gt;ktype 指定成該kset下的ktype，除非沒有定义kset，才会用ktype來建立关系。Kobject通过kset组织成层次化的结构，kset是具有相 同类型的kobject的集合，在内核中用kset数据结构表示，定义为：
struct kset {
struct subsystem * subsys; 所在的subsystem的指针
struct kobj type * ktype; 指向该kset对象类型描述符的指针
struct list head list; 用于连接该kset中所有kobject的链表头
struct kobject kobj; 嵌入的kobject
struct kset hotplug ops * hotplug ops; 指向热插拔操作表的指针
};&lt;/p&gt;

&lt;p&gt;包 含在kset中的所有kobject被组织成一个双向循环链表，list域正是该链表的头。Ktype域指向一个kobj type结构，被该kset中的所有kobject共享，表示这些对象的类型。Kset数据结构还内嵌了一个kobject对象（由kobj域表示），所 有属于这个kset 的kobject对象的parent域均指向这个内嵌的对象。此外，kset还依赖于kobj维护引用计数：kset的引用计数实际上就是内嵌的 kobject对象的引用计数。&lt;/p&gt;

&lt;p&gt;subsystem 
如果說kset 是管理kobject 的集合，同理，subsystem 就是管理kset 的集合。它描述系统中某一类设备子系统，如block subsys表示所有的块设备，对应于sysfs文件系统中的block目录。类似的，devices subsys对应于sysfs中的devices目录，描述系统中所有的设备。Subsystem由struct subsystem数据结构描述，定义为：
struct subsystem {
struct kset kset; 内嵌的kset对象
struct rw semaphore rwsem; 互斥访问信号量
};
可以看出，subsystem与kset的区别就是多了一个信号量，所以在后来的代码中，subsystem已经完全被kset取缔了。
每个kset属于某个subsystem，通过设置kset结构中的subsys域指向指定的subsystem可以将一个kset加入到该subsystem。所有挂接到同一subsystem的kset共享同一个rwsem信号量，用于同步访问kset中的链表。&lt;/p&gt;

&lt;p&gt;sysfs是用于表现设备驱动模型的文件系统，它基于ramfs。要学习linux的设备驱动模型，就要先做好底层工作，总结sysfs提供给外界的API就是其中之一。sysfs文件系统中提供了四类文件的创建与管理，分别是目录、普通文件、软链接文件、二进制文件。目录层次往往代表着设备驱动模型的结构，软链接文件则代表着不同部分间的关系。比如某个设备的目录只出现在/sys/devices下，其它地方涉及到它时只好用软链接文件链接过去，保持了设备唯一的实例。而普通文件和二进制文件往往代表了设备的属性，读写这些文件需要调用相应的属性读写。
    sysfs是表现设备驱动模型的文件系统，它的目录层次实际反映的是对象的层次。为了配合这种目录，linux专门提供了两个结构作为sysfs的骨架，它们就是struct kobject和struct kset。我们知道，sysfs是完全虚拟的，它的每个目录其实都对应着一个kobject，要想知道这个目录下有哪些子目录，就要用到kset。从面向对象的角度来讲，kset继承了kobject的功能，既可以表示sysfs中的一个目录，还可以包含下层目录。对于kobject和kset，会在其它文章中专门分析到，这里简单描述只是为了更好地介绍sysfs提供的API。
sysfs 与 proc 相比有很多优点，最重要的莫过于设计上的清晰。一个 proc 虚拟文件可能有内部格式，如 /proc/scsi/scsi
，它是可读可写的，(其文件权限被错误地标记为了 0444
！，这是内核的一个BUG)，并且读写格式不一样，代表不同的操作，应用程序中读到了这个文件的内容一般还需要进行字符串解析，而在写入时需要先用字符串
格式化按指定的格式写入字符串进行操作；相比而言， sysfs 的设计原则是一个属性文件只做一件事情， sysfs
属性文件一般只有一个值，直接读取或写入。整个 /proc/scsi 目录在2.6内核中已被标记为过时(LEGACY)，它的功能已经被相应的 /sys 属性文件所完全取代。新设计的内核机制应该尽量使用 sysfs 机制，而将 proc 保留给纯净的“进程文件系统”。&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/09/sysfs.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/09/sysfs.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>proc文件系统</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;proc文件系统是一种无存储的文件系统，当读其中的文件时，其内容动态生成，当写文件时，文件所关联的写函数被调用。每个proc文件都关联的字节特定的读写函数，因而它提供了另外的一种和内核通信的机制：内核部件可以通过该文件系统向用户空间提供接口来提供查询信息、修改软件行为，因而它是一种比较重要的特殊文件系统。
由于proc文件系统以文件的形式向用户空间提供了访问接口，这些接口可以用于在运行时获取相关部件的信息或者修改部件的行为，因而它是非常方便的一个接口。内核中大量使用了该文件系统。proc文件系统就是一个文件系统，它可以挂载在目录树的任意位置，不过通常挂载在/proc下，它大致包含了如下信息：
内存管理
每个进程的相关信息
文件系统
设备驱动程序
系统总线
电源管理
终端
系统控制参数
网络
使用proc文件系统之前必须将其初始化并且挂载到系统中。proc文件系统的的初始化主要完成：
调用proc_init_inodecache创建proc文件系统所使用的专用缓冲区
调用register_filesystem注册proc文件系统，这里会提供proc文件系统自己的file_system_type，其中包括了用于mount的函数指针。在执行mount的时候会用到这些信息，并最终找到mount函数进行挂载操作
调用proc_mkdir创建一些proc文件目录
在sys文件系统下注册proc文件系统的相关信息
在proc的mount函数中会调用proc_fill_super，它会给出proc文件系统超级块所需要的信息（比如文件系统的超级块操作函数指针，超级块大小等），并且会创建proc文件系统的根目录，在创建根目录时也会指定与之对应的inode_operations和file_operations，有了这些信息后，VFS就可以在该文件系统上进行各种操作了（创建、删除、查找文件）。&lt;/p&gt;

&lt;p&gt;文件操作的过程基本都是类似的，首先VFS会找到文件对应的inode，然后调用inode上的相关函数完成对应的操作，比如当读写一个文件时，要执行的步骤：
首先要打开文件，对于proc文件系统来说，它要做的是：找到文件的，在找到文件后文件数据结构的的file_operations会被初始化为文件的inode中的file_operations（参考路径do_filp_open-&amp;gt;path_openat-&amp;gt;do_last-&amp;gt;nameidata_to_filp-&amp;gt;__dentry_open-&amp;gt;fops_get）。之后就可以调用文件的打开操作了。
写文件，这一步也很简单， 文件数据结构中的写操作即可。
这个过程也很好理解， 因为对于任何文件系统，它们都存在inode，inode被用来表示一个文件，它包含了很多有用的信息，它用于维护文件自身而不涉及文件的内容。对于存储在存储器上的文件系统，其inode的有些信息是保存在存储器上的，但是仍有一部分是动态生成的；对于无存储器的文件系统，比如proc文件系统，其inode都是动态生成的。再使用一个文件时，其inode就会被加载到内存中以供使用，如果还不存在相关的inode，则就会创建一个新的。每个文件系统的超级块的super_operations包含了为本文件系统创建和删除inode节点的函数指针，在inode的操作函数集中包含了操作inode节点的函数指针，其中包括了查找inode节点的函数。打开一个文件时，如果inode缓冲中还没有该文件的inode，则经VFS处理后最终会调用lookup_real，它会调用inode操作函数集中的lookup函数用于查找所要打开的文件的inode；如果该文件的inode已经存在则会从缓存中得到该文件对应的inode，这部分工作由do_lookup完成。
简单的说，文件操作第一步时找到inode信息（如果没有就创建并初始化），然后用inode信息初始化文件的file结构。再用file结构对文件进行操作。
加载 proc 文件系统&lt;/p&gt;

&lt;p&gt;如果系统中还没有加载 proc 文件系统，可以通过如下命令加载 proc 文件系统： 
mount -t proc proc /proc&lt;/p&gt;

&lt;p&gt;上述命令将成功加载你的 proc 文件系统。更多细节请阅读 mount 命令的 man page。&lt;/p&gt;

&lt;p&gt;察看 /proc 的文件&lt;/p&gt;

&lt;p&gt;/proc 的文件可以用于访问有关内核的状态、计算机的属性、正在运行的进程的 状态等信息。大部分 /proc 中的文件和目录提供系统物理环境最新的信息。尽管 /proc 中的文件是虚拟的，但它们仍可以使用任何文件编辑器或像’more’, ‘less’或 ‘cat’这样的程序来查看。当编辑程序试图打开一个虚拟文件时，这个文件就通过内核 中的信息被凭空地 (on the fly) 创建了。这是一些我从我的系统中得到的一些有趣 结果：&lt;/p&gt;

&lt;p&gt;$ ls -l /proc/cpuinfo
-r–r–r– 1 root root 0 Dec 25 11:01 /proc/cpuinfo&lt;/p&gt;

&lt;p&gt;得到有用的系统/内核信息&lt;/p&gt;

&lt;p&gt;proc 文件系统可以被用于收集有用的关于系统和运行中的内核的信息。下面是一些重要 的文件：&lt;/p&gt;

&lt;p&gt;/proc/cpuinfo - CPU 的信息 (型号, 家族, 缓存大小等)
/proc/meminfo - 物理内存、交换空间等的信息
/proc/mounts - 已加载的文件系统的列表
/proc/devices - 可用设备的列表
/proc/filesystems - 被支持的文件系统
/proc/modules - 已加载的模块
/proc/version - 内核版本
/proc/cmdline - 系统启动时输入的内核命令行参数
proc 中的文件远不止上面列出的这么多。想要进一步了解的读者可以对 /proc 的每一个 文件都’more’一下或读参考文献[1]获取更多的有关 /proc 目录中的文件的信息。我建议 使用’more’而不是’cat’，除非你知道这个文件很小，因为有些文件 (比如 kcore) 可能 会非常长。&lt;/p&gt;

&lt;p&gt;有关运行中的进程的信息&lt;/p&gt;

&lt;p&gt;/proc 文件系统可以用于获取运行中的进程的信息。在 /proc 中有一些编号的子目录。每个编号的目录对应一个进程 id (PID)。这样，每一个运行中的进程 /proc 中都有一个用它的 PID 命名的目录。这些子目录中包含可以提供有关进程的状态和环境的重要细节信息的文件。让我们试着查找一个运行中的进程。&lt;/p&gt;

&lt;p&gt;$ ps -aef | grep mozilla
root 32558 32425 8  22:53 pts/1  00:01:23  /usr/bin/mozilla
上述命令显示有一个正在运行的 mozilla 进程的 PID 是 32558。相对应的，/proc 中应该有一个名叫 32558 的目录&lt;/p&gt;

&lt;p&gt;通过 /proc 与内核交互&lt;/p&gt;

&lt;p&gt;上面讨论的大部分 /proc 的文件是只读的。而实际上 /proc 文件系统通过 /proc 中可读写的文件提供了对内核的交互机制。写这些文件可以改变内核 的状态，因而要慎重改动这些文件。/proc/sys 目录存放所有可读写的文件 的目录，可以被用于改变内核行为。&lt;/p&gt;

&lt;p&gt;/proc/sys/kernel - 这个目录包含反通用内核行为的信息。 /proc/sys/kernel/{domainname, hostname} 存放着机器/网络的域名和主机名。 这些文件可以用于修改这些名字。&lt;/p&gt;

&lt;p&gt;$ hostname
machinename.domainname.com&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/09/proc.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/09/proc.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>netfliter</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Netfilter是Linux 2.4.x引入的一个子系统，它作为一个通用的、抽象的框架，提供一整套的hook函数的管理机制，使得诸如数据包过滤、网络地址转换(NAT)和基于协议类型的连接跟踪成为了可能。
netfilter的架构就是在整个网络流程的若干位置放置了一些检测点HOOK），而在每个检测点上登记了一些处理函数进行处理。&lt;/p&gt;

&lt;p&gt;netfilter是由Rusty Russell提出的Linux 2.4内核防火墙框架，该框架既简洁又灵活，可实现安全策略应用中的许多功能，如数据包过滤、数据包处理、地址伪装、透明代理、动态网络地址转换（Network Address Translation，NAT），以及基于用户及媒体访问控制（Media Access Control，MAC）地址的过滤和基于状态的过滤、包速率限制等。
框架
netfilter提供了一个抽象、通用化的框架[1]，作为中间件，为每种网络协议（IPv4、IPv6等）定义一套钩子函数。Ipv4定义了5个钩子函数，这些钩子函数在数据报流过协议栈的5个关键点被调用，也就是说，IPv4协议栈上定义了5个“允许垂钓点”。在每一个“垂钓点”，都可以让netfilter放置一个“鱼钩”，把经过的网络包（Packet）钓上来，与相应的规则链进行比较，并根据审查的结果，决定包的下一步命运，即是被原封不动地放回IPv4协议栈，继续向上层递交；还是经过一些修改，再放回网络；或者干脆丢弃掉。
Ipv4中的一个数据包通过netfilter系统的过程如图1所示。
图1 Netfilter的功能框架
关键技术
netfilter主要采用连线跟踪（Connection Tracking）、包过滤（Packet Filtering）、地址转换、包处理（Packet Mangling)4种关键技术。
⒈2.1 连线跟踪
连线跟踪是包过滤、地址转换的基础，它作为一个独立的模块运行。采用连线跟踪技术在协议栈低层截取数据包，将当前数据包及其状态信息与历史数据包及其状态信息进行比较，从而得到当前数据包的控制信息，根据这些信息决定对网络数据包的操作，达到保护网络的目的。
当下层网络接收到初始化连接同步（Synchronize，SYN）包，将被netfilter规则库检查。该数据包将在规则链中依次序进行比较。如果该包应被丢弃，发送一个复位（Reset，RST）包到远端主机，否则连接接收。这次连接的信息将被保存在连线跟踪信息表中，并表明该数据包所应有的状态。这个连线跟踪信息表位于内核模式下，其后的网络包就将与此连线跟踪信息表中的内容进行比较，根据信息表中的信息来决定该数据包的操作。因为数据包首先是与连线跟踪信息表进行比较，只有SYN包才与规则库进行比较，数据包与连线跟踪信息表的比较都是在内核模式下进行的，所以速度很快。
⒈2.2 包过滤
包过滤检查通过的每个数据包的头部，然后决定如何处置它们，可以选择丢弃，让包通过，或者更复杂的操作。
⒈2.3 地址转换
网络地址转换 分为源NAT（Source NAT，SNAT）和目的NAT(Destination NAT,DNAT)2种不同的类型。SNAT是指修改数据包的源地址（改变连接的源IP）。SNAT会在数据包送出之前的最后一刻做好转换工作。地址伪装（Masquerading）是SNAT的一种特殊形式。DNAT 是指修改数据包的目标地址（改变连接的目的IP）。DNAT 总是在数据包进入以后立即完成转换。端口转发、负载均衡和透明代理都属于DNAT。
⒈2.4 包处理
利用包处理可以设置或改变数据包的服务类型（Type of Service,TOS）字段；改变包的生存期（Time to Live,TTL）字段；在包中设置标志值，利用该标志值可以进行带宽限制和分类查询。&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/netfliter.png&quot; /&gt;
	&lt;/div&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Netfilter 是Linux内核中进行数据包过滤、连接跟踪、地址转换等的主要实现框架。当我们希望过滤特定的数据包或者需要修改数据包的内容再发送出去，这些动作主要都在netfilter中完成。
iptables工具就是用户空间和内核的Netfilter模块通信的手段，iptables命令提供很多选项来实现过滤数据包的各种操作，所以，我们在定义数据包过滤规则时，并不需要去直接修改内核中的netfilter模块，后面会讲到iptables命令如何作用于内核中的netfilter。
Netfilter的实质就是定义一系列的hook点（挂钩），每个hook点上可以挂载多个hook函数，hook函数中就实现了我们要对数据包的内容做怎样的修改、以及要将数据包放行还是过滤掉。数据包进入netfilter框架后，实际上就是依次经过所有hook函数的处理，数据包的命运就掌握在这些hook函数的手里。
本文基于内核版本2.6.31。
所有的hook点都放在一个全局的二维数组，每个hook点上的hook函数按照优先级顺序注册到一个链表中，注册的接口为nf_register_hook()。这个二维数组的定义如下：
struct list_head nf_hooks[NFPROTO_NUMPROTO][NF_MAX_HOOKS]__read_mostly;
其中NFPROTO_NUMPROTO 为netfilter支持的协议类型：
enum {
   NFPROTO_UNSPEC=  0,
   NFPROTO_IPV4   =  2,
   NFPROTO_ARP    =  3,
   NFPROTO_BRIDGE=  7,
   NFPROTO_IPV6   = 10,
   NFPROTO_DECNET= 12,
   NFPROTO_NUMPROTO,
};&lt;/p&gt;

</description>
        <pubDate>Sat, 09 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/jekyll/2017/12/09/netfliter.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/jekyll/2017/12/09/netfliter.html</guid>
        
        
        <category>jekyll</category>
        
      </item>
    
      <item>
        <title>namespace</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Linux Namespace&lt;/p&gt;

&lt;p&gt;Linux Namespace是Linux提供的一种OS-level virtualization的方法。目前在Linux系统上实现OS-level virtualization的系统有Linux VServer、OpenVZ、LXC Linux Container、Virtuozzo等，其中Virtuozzo是OpenVZ的商业版本。以上种种本质来说都是使用了Linux Namespace来进行隔离。&lt;/p&gt;

&lt;p&gt;那么究竟什么是Linux Namespace？Linux很早就实现了一个系统调用chroot，该系统调用能够为进程提供一个限制的文件系统。虽然文件系统的隔离要比单纯的chroot复杂的多，但是至少chroot提供了一种简单的隔离模式：chroot内部的文件系统无法访问外部的内容。Linux Namespace在此基础上，提供了对UTS、IPC、mount、PID、network的隔离机制，例如对不同的PID namespace中的进程无法看到彼此，而且每个PID namespace中的进程PID都是单独制定的。这一点对OS-level Virtualization非常有用，这是因为：对于不同的Linux运行环境中，都有一个init进程，其PID=0，由于不同的PID namespace中都可以指定自己的0号进程，所以可以通过该技术来进行PID环境的隔离。&lt;/p&gt;

&lt;p&gt;OS-level Virtualization相比其他的虚拟化技术更加轻量级。&lt;/p&gt;

&lt;p&gt;Linux在使用Namespace的时候，需要显式的在配置中指定将那些Namespace的支持编译到内核中。&lt;/p&gt;

&lt;p&gt;进程的若干个ID的意义&lt;/p&gt;

&lt;p&gt;PID：Process ID，进程ID，即进程的唯一标识
TGID：处于某个线程组中的所有进程都有统一的线程组ID（Thread Group IP，TGID）。线程可以用clone加CLONE_THREAD来创建。线程组中的主进程成为group leader，可以通过线程组中任何线程的的task_struct-&amp;gt;group_leader成员获得。
独立进程可以合并成进程组（使用setpgrp系统调用）。进程组成员的task_struct-&amp;gt;pgrp属性值都是相同的（PGID），即进程组组长的PID。用管道连接的进程在一个进程组中。
几个进程组可以合并成一个会话。会话中所有进程都有同样的SID（Session ID，会话ID），保存在task_struct-&amp;gt;session中。SID可以通过setsid系统调用设置。&lt;/p&gt;

&lt;p&gt;引入进程PID命名空间后的PID框架
随着内核不断的添加新的内核特性,尤其是PID Namespace机制的引入,这导致PID存在命名空间的概念,并且命名空间还有层级的概念存在,高级别的可以被低级别的看到,这就导致高级别的进程有多个PID,比如说在默认命名空间下,创建了一个新的命名空间,占且叫做level1,默认命名空间这里称之为level0,在level1中运行了一个进程在level1中这个进程的pid为1,因为高级别的pid namespace需要被低级别的pid namespace所看见,所以这个进程在level0中会有另外一个pid,为xxx.套用上面说到的pid位图的概念,可想而知,对于每一个pid namespace来说都应该有一个pidmap,上文中提到的level1进程有两个pid一个是1,另一个是xxx,其中pid为1是在level1中的pidmap进行分配的,pid为xxx则是在level0的pidmap中分配的. 下面这幅图是整个pidnamespace的一个框架&lt;/p&gt;

&lt;p&gt;Linux Namespaces机制提供一种资源隔离方案。PID,IPC,Network等系统资源不再是全局性的，而是属于特定的Namespace。每个Namespace里面的资源对其他Namespace都是透明的。要创建新的Namespace，只需要在调用clone时指定相应的flag。Linux Namespaces机制为实现基于容器的虚拟化技术提供了很好的基础，LXC（Linux containers）就是利用这一特性实现了资源的隔离。不同container内的进程属于不同的Namespace，彼此透明，互不干扰。下面我们就从clone系统调用的flag出发，来介绍各个Namespace。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWPID，就会创建一个新的PID Namespace，clone出来的新进程将成为Namespace里的第一个进程。一个PID Namespace为进程提供了一个独立的PID环境，PID Namespace内的PID将从1开始，在Namespace内调用fork，vfork或clone都将产生一个在该Namespace内独立的PID。新创建的Namespace里的第一个进程在该Namespace内的PID将为1，就像一个独立的系统里的init进程一样。该Namespace内的孤儿进程都将以该进程为父进程，当该进程被结束时，该Namespace内所有的进程都会被结束。PID Namespace是层次性，新创建的Namespace将会是创建该Namespace的进程属于的Namespace的子Namespace。子Namespace中的进程对于父Namespace是可见的，一个进程将拥有不止一个PID，而是在所在的Namespace以及所有直系祖先Namespace中都将有一个PID。系统启动时，内核将创建一个默认的PID Namespace，该Namespace是所有以后创建的Namespace的祖先，因此系统所有的进程在该Namespace都是可见的。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWIPC，就会创建一个新的IPC Namespace，clone出来的进程将成为Namespace里的第一个进程。一个IPC Namespace有一组System V IPC objects 标识符构成，这标识符有IPC相关的系统调用创建。在一个IPC Namespace里面创建的IPC object对该Namespace内的所有进程可见，但是对其他Namespace不可见，这样就使得不同Namespace之间的进程不能直接通信，就像是在不同的系统里一样。当一个IPC Namespace被销毁，该Namespace内的所有IPC object会被内核自动销毁。&lt;/p&gt;

&lt;p&gt;PID Namespace和IPC Namespace可以组合起来一起使用，只需在调用clone时，同时指定CLONE_NEWPID和CLONE_NEWIPC，这样新创建的Namespace既是一个独立的PID空间又是一个独立的IPC空间。不同Namespace的进程彼此不可见，也不能互相通信，这样就实现了进程间的隔离。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWNS，就会创建一个新的mount Namespace。每个进程都存在于一个mount Namespace里面，mount Namespace为进程提供了一个文件层次视图。如果不设定这个flag，子进程和父进程将共享一个mount Namespace，其后子进程调用mount或umount将会影响到所有该Namespace内的进程。如果子进程在一个独立的mount Namespace里面，就可以调用mount或umount建立一份新的文件层次视图。该flag配合pivot_root系统调用，可以为进程创建一个独立的目录空间。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWNET，就会创建一个新的Network Namespace。一个Network Namespace为进程提供了一个完全独立的网络协议栈的视图。包括网络设备接口，IPv4和IPv6协议栈，IP路由表，防火墙规则，sockets等等。一个Network Namespace提供了一份独立的网络环境，就跟一个独立的系统一样。一个物理设备只能存在于一个Network Namespace中，可以从一个Namespace移动另一个Namespace中。虚拟网络设备(virtual network device)提供了一种类似管道的抽象，可以在不同的Namespace之间建立隧道。利用虚拟化网络设备，可以建立到其他Namespace中的物理设备的桥接。当一个Network Namespace被销毁时，物理设备会被自动移回init Network Namespace，即系统最开始的Namespace。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWUTS，就会创建一个新的UTS Namespace。一个UTS Namespace就是一组被uname返回的标识符。新的UTS Namespace中的标识符通过复制调用进程所属的Namespace的标识符来初始化。Clone出来的进程可以通过相关系统调用改变这些标识符，比如调用sethostname来改变该Namespace的hostname。这一改变对该Namespace内的所有进程可见。CLONE_NEWUTS和CLONE_NEWNET一起使用，可以虚拟出一个有独立主机名和网络空间的环境，就跟网络上一台独立的主机一样。&lt;/p&gt;

&lt;p&gt;以上所有clone flag都可以一起使用，为进程提供了一个独立的运行环境。LXC正是通过在clone时设定这些flag，为进程创建一个有独立PID，IPC，FS，Network，UTS空间的container。一个container就是一个虚拟的运行环境，对container里的进程是透明的，它会以为自己是直接在一个系统上运行的。&lt;/p&gt;

&lt;p&gt;一个container就像传统虚拟化技术里面的一台安装了OS的虚拟机，但是开销更小，部署更为便捷。&lt;/p&gt;
&lt;div class=&quot;container&quot;&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterSlider.png&quot; /&gt;
	&lt;/div&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;/div&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 08 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/08/namespace.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/08/namespace.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>vfs</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;在Linux中，文件系统主要分为下面3种：
（1）基于磁盘的文件系统（Disk-based Filesystem) 是在非易失介质上存储文件的经典方法，用以在多次会话之间保持文件的内容。如Ext2/3/4， Reiserfs, FAT等。
（2）虚拟文件系统（Virtual Filesystem） 在内核中生成，是一种用户应用程序与内核通信的方法。如proc，它不许要在任何类的硬件设备上分配存储空间，所有的信息都是动态在内存中开辟和存储。
（3）网络文件系统（Network Filesystem） 是基于磁盘的文件系统和虚拟文件系统之间的折中。这种文件系统允许访问另一台计算机上的数据，该计算机通过网络连接到本地计算机。在这种情况下，数据实际上存储在一个不同系统的硬件设备上。
由于VFS抽象层的存在，用户空间进程不会看到本地文件系统与网络文件系统之间的区别。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;VFS的模型与结构
VFS不仅为文件系统提供了方法和抽象，还支持文件系统中对象（或文件）的统一视图。并非每一种文件系统都支持VFS中的所有抽象，如FAT，因为其设计没有考虑到此类对象。定义一个最小的通用模型，来支持内核中所有文件系统都实现的那些功能，这是不实际的。因为这样会损失许多本质性的功能特性，或者导致这些特性只能通过特定文件系统的路径访问。
VFS的方案完全相反：提供一种结构模型，包含了一个强大文件系统所具备的所有组件。但该模型只存在于虚拟中，必须使用各种对象和函数指针与每种文件系统适配。所有文件系统的实现都必须提供与VFS定义的结构配合的例程，以弥合两种视图之间的差异。
VFS是由基于经典文件系统的结构衍化而来，所以VFS与Ext类文件系统类似，从而在处理Ext类文件系统的时候，Ext和VFS之间的转换，几乎不会损失时间。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一些基本概念:
文件 一组在逻辑上具有完整意义的信息项的系列。在Linux中，除了普通文件，其他诸如目录、设备、套接字等 也以文件被对待。总之，“一切皆文件”。
目录 目录好比一个文件夹，用来容纳相关文件。因为目录可以包含子目录，所以目录是可以层层嵌套，形成 文件路径。在Linux中，目录也是以一种特殊文件被对待的，所以用于文件的操作同样也可以用在目录上。
目录项 在一个文件路径中，路径中的每一部分都被称为目录项；如路径/home/source/helloworld.c中，目录 /, home, source和文件 helloworld.c都是一个目录项。
索引节点 用于存储文件的元数据的一个数据结构。文件的元数据，也就是文件的相关信息，和文件本身是两个不同 的概念。它包含的是诸如文件的大小、拥有者、创建时间、磁盘位置等和文件相关的信息。
超级块 用于存储文件系统的控制信息的数据结构。描述文件系统的状态、文件系统类型、大小、区块数、索引节 点数等，存放于磁盘的特定扇区中&lt;/p&gt;

</description>
        <pubDate>Thu, 07 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/07/vfs.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/07/vfs.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>mysql_index</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;如果想知道MySQL数据库中每个表占用的空间、表记录的行数的话，可以打开MySQL的 information_schema 数据库。在该库中有一个 TABLES 表，这个表主要字段分别是：&lt;/p&gt;

&lt;p&gt;TABLE_SCHEMA : 数据库名
TABLE_NAME：表名
ENGINE：所使用的存储引擎
TABLES_ROWS：记录数
DATA_LENGTH：数据大小
INDEX_LENGTH：索引大小&lt;/p&gt;

&lt;p&gt;其他字段请参考MySQL的手册，我们只需要了解这几个就足够了。&lt;/p&gt;

&lt;p&gt;1  首先查看某一实例下的所有占用磁盘空间（表数据+索引数据，得到的结果为B，这里做了数据处理转成M）：&lt;/p&gt;

&lt;p&gt;select concat(round((sum(DATA_LENGTH)+sum(INDEX_LENGTH))/1024/1024,2),’M’) from information_schema.tables where table_schema=’实例名称’;
 上面是查询所有的表计的累计量，下面是是查询单个表计的的SQL(按照实例名查询)：&lt;/p&gt;

&lt;p&gt;select table_name,
DATA_LENGTH/1024/1024 as tablesData,
INDEX_LENGTH/1024/1024 as indexData 
from information_schema.tables
where table_schema=’dsm’
ORDER BY  tablesData desc;&lt;/p&gt;

</description>
        <pubDate>Thu, 07 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/12/07/mysql_index.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/12/07/mysql_index.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>linux_rcu</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;1:RCU使用在读者多而写者少的情况.RCU和读写锁相似.但RCU的读者占锁没有任何的系统开销.写者与写写者之间必须要保持同步,且写者必须要等它之前的读者全部都退出之后才能释放之前的资源.
2:RCU保护的是指针.这一点尤其重要.因为指针赋值是一条单指令.也就是说是一个原子操作.因它更改指针指向没必要考虑它的同步.只需要考虑cache的影响. 
3:读者是可以嵌套的.也就是说rcu_read_lock()可以嵌套调用. 
4:读者在持有rcu_read_lock()的时候,不能发生进程上下文切换.否则,因为写者需要要等待读者完成,写者进程也会一直被阻塞.&lt;/p&gt;

&lt;p&gt;RCU（Read-Copy Update）是数据同步的一种方式，在当前的Linux内核中发挥着重要的作用。RCU主要针对的数据对象是链表，目的是提高遍历读取数据的效率，为了达到目的使用RCU机制读取数据的时候不对链表进行耗时的加锁操作。这样在同一时间可以有多个线程同时读取该链表，并且允许一个线程对链表进行修改（修改的时候，需要加锁）。RCU适用于需要频繁的读取数据，而相应修改数据并不多的情景，例如在文件系统中，经常需要查找定位目录，而对目录的修改相对来说并不多，这就是RCU发挥作用的最佳场景。&lt;/p&gt;

&lt;p&gt;在RCU的实现过程中，我们主要解决以下问题：
1，在读取过程中，另外一个线程删除了一个节点。删除线程可以把这个节点从链表中移除，但它不能直接销毁这个节点，必须等到所有的读取线程读取完成以后，才进行销毁操作。RCU中把这个过程称为宽限期（Grace period）。
2，在读取过程中，另外一个线程插入了一个新节点，而读线程读到了这个节点，那么需要保证读到的这个节点是完整的。这里涉及到了发布-订阅机制（Publish-Subscribe Mechanism）。
3， 保证读取链表的完整性。新增或者删除一个节点，不至于导致遍历一个链表从中间断开。但是RCU并不保证一定能读到新增的节点或者不读到要被删除的节点。&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_rcu.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_rcu.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Linux的mmap内存映射机制</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;一个进程应该包括一个mm_struct(memory manage struct), 该结构是进程虚拟地址空间的抽象描述,里面包括了进程虚拟空间的一些管理信息: start_code, end_code, start_data, end_data, start_brk, end_brk等等信息.另外,也有一个指向进程虚存区表(vm_area_struct: virtual memory area)的指针,该链是按照虚拟地址的增长顺序排列的.在Linux进程的地址空间被分作许多区(vma),每个区(vma)都对应虚拟地址空间上一段连续的区域, vma是可以被共享和保护的独立实体,这里的vma就是前面提到的内存对象.&lt;/p&gt;

&lt;p&gt;设备驱动的mmap实现主要是将一个物理设备的可操作区域（设备空间）映射到一个进程的虚拟地址空间。这样就可以直接采用指针的方式像访问内存的方式访问设备。在驱动中的mmap实现主要是完成一件事，就是实际物理设备的操作区域到进程虚拟空间地址的映射过程。同时也需要保证这段映射的虚拟存储器区域不会被进程当做一般的空间使用，因此需要添加一系列的保护方式。&lt;/p&gt;

&lt;p&gt;Linux提供了内存映射函数mmap,它把文件内容映射到一段内存上(准确说是虚拟内存上),通过对这段内存的读取和修改,实现对文件的读取和修改 。普通文件被映射到进程地址空间后，进程可以向访问普通内存一样对文件进行访问，不必再调用read()，write（）等操作。
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/linuxMMap.jpeg&quot; /&gt;
mmap系统调用的实现过程是
1.先通过文件系统定位要映射的文件；
2.权限检查,映射的权限不会超过文件打开的方式,也就是说如果文件是以只读方式打开,那么则不允许建立一个可写映射； 
3.创建一个vma对象,并对之进行初始化； 
4.调用映射文件的mmap函数,其主要工作是给vm_ops向量表赋值；
5.把该vma链入该进程的vma链表中,如果可以和前后的vma合并则合并；
6.如果是要求VM_LOCKED(映射区不被换出)方式映射,则发出缺页请求,把映射页面读入内存中.&lt;/p&gt;

&lt;p&gt;2、munmap函数
munmap(void * start, size_t length):
该调用可以看作是mmap的一个逆过程.它将进程中从start开始length长度的一段区域的映射关闭,如果该区域不是恰好对应一个vma,则有可能会分割几个或几个vma.
 msync(void * start, size_t length, int flags):
把映射区域的修改回写到后备存储中.因为munmap时并不保证页面回写,如果不调用msync,那么有可能在munmap后丢失对映射区的修改.其中flags可以是MS_SYNC, MS_ASYNC, MS_INVALIDATE, MS_SYNC要求回写完成后才返回, MS_ASYNC发出回写请求后立即返回, MS_INVALIDATE使用回写的内容更新该文件的其它映射.该系统调用是通过调用映射文件的sync函数来完成工作的.
brk(void * end_data_segement):
将进程的数据段扩展到end_data_segement指定的地址,该系统调用和mmap的实现方式十分相似,同样是产生一个vma,然后指定其属性.不过在此之前需要做一些合法性检查,比如该地址是否大于mm-&amp;gt;end_code, end_data_segement和mm-&amp;gt;brk之间是否还存在其它vma等等.通过brk产生的vma映射的文件为空,这和匿名映射产生的vma相似,关于匿名映射不做进一步介绍.库函数malloc就是通过brk实现的.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_mmap.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_mmap.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
  </channel>
</rss>
