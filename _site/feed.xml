<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 15 Nov 2017 19:32:32 +0800</pubDate>
    <lastBuildDate>Wed, 15 Nov 2017 19:32:32 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>spark_memory</title>
        <description>&lt;p&gt;在 spark-env.sh 中添加：&lt;/p&gt;

&lt;p&gt;export JAVA_HOME=/home/tom/jdk1.8.0_73/
export SCALA_HOME=/home/tom//scala-2.10.6
export SPARK_MASTER_IP=localhost
export SPARK_WORKER_MEMORY=4G&lt;/p&gt;

&lt;p&gt;内存设置太少会导致运行失败
可以代码中设置内存
//set new runtime options
spark.conf.set(“spark.sql.shuffle.partitions”, 1)
spark.conf.set(“spark.executor.memory”, “512m”)&lt;/p&gt;

&lt;p&gt;Spark虽然是in memory的运算平台，但从官方资料看，似乎本身对内存的要求并不是特别苛刻。官方网站只是要求内存在8GB之上即可（Impala要求机器配置在128GB）Spark建议需要提供至少75%的内存空间分配给Spark，至于其余的内存空间，则分配给操作系统与buffer cache。Spark对内存的消耗主要分为三部分（即取决于你的应用程序的需求）：
      数据集中对象的大小；
      访问这些对象的内存消耗；
      垃圾回收GC的消耗&lt;/p&gt;

&lt;p&gt;spark executor都是装载在container里运行，container默认的内存是1G（参数yarn.scheduler.minimum-allocation-mb定义），executor分配的内存是executor-memory，所以向YARN申请的内存是（executor-memory + 1）* num-executors。 Executor 内存的大小，和性能本身当然并没有直接的关系，但是几乎所有运行时性能相关的内容都或多或少间接和内存大小相关。这个参数最终会被设置到Executor的JVM的heap尺寸上。如果Executor的数量和内存大小受机器物理配置影响相对固定，那么你就需要合理规划每个分区任务的数据规模，例如采用更多的分区，用增加任务数量（进而需要更多的批次来运算所有的任务）的方式来减小每个任务所需处理的数据大小。&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/15/spark_memory.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/15/spark_memory.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>redis 持久化</title>
        <description>&lt;p&gt;edis是一个支持持久化的内存数据库，也就是说redis需要经常将内存中的数据同步到磁盘来保证持久化。redis支持四种持久化方式，一是 Snapshotting（快照）也是默认方式；二是Append-only file（缩写aof）的方式；三是虚拟内存方式；四是diskstore方式。
&lt;!-- more --&gt;
（一）Snapshotting
       快照是默认的持久化方式。这种方式是就是将内存中数据以快照的方式写入到二进制文件中,默认的文件名为dump.rdb。可以通过配置设置自动做快照持久化的方式。我们可以配置redis在n秒内如果超过m个key被修改就自动做快照&lt;/p&gt;

&lt;p&gt;save 900 1  #900秒内如果超过1个key被修改，则发起快照保存
save 300 10 #300秒内容如超过10个key被修改，则发起快照保存&lt;/p&gt;

&lt;p&gt;快照保存过程：
    1. redis调用fork,现在有了子进程和父进程。
    2. 父进程继续处理client请求，子进程负责将内存内容写入到临时文件。由于os的写时复制机制（copy on write)父子进程会共享相同的物理页面，当父进程处理写请求时os会为父进程要修改的页面创建副本，而不是写共享的页面。所以子进程的地址空间内的数据是fork时刻整个数据库的一个快照。
    3. 当子进程将快照写入临时文件完毕后，用临时文件替换原来的快照文件，然后子进程退出（fork一个进程入内在也被复制了，即内存会是原来的两倍）。&lt;/p&gt;

&lt;p&gt;client 也可以使用save或者bgsave命令通知redis做一次快照持久化。save操作是在主线程中保存快照的，由于redis是用一个主线程来处理所有 client的请求，这种方式会阻塞所有client请求。所以不推荐使用。另一点需要注意的是，每次快照持久化都是将内存数据完整写入到磁盘一次，并不是增量的只同步脏数据。如果数据量大的话，而且写操作比较多，必然会引起大量的磁盘io操作，可能会严重影响性能。
   另外由于快照方式是在一定间隔时间做一次的，所以如果redis意外down掉的话，就会丢失最后一次快照后的所有修改。如果应用要求不能丢失任何修改的话，可以采用aof持久化方式。&lt;/p&gt;

&lt;p&gt;（二）Append-only file
aof 比快照方式有更好的持久化性，是由于在使用aof持久化方式时，redis会将每一个收到的写命令都通过write函数追加到文件中(默认是appendonly.aof)。当redis重启时会通过重新执行文件中保存的写命令来在内存中重建整个数据库的内容。当然由于os会在内核中缓存 write做的修改，所以可能不是立即写到磁盘上。这样aof方式的持久化也还是有可能会丢失部分修改。不过我们可以通过配置文件告诉redis我们想要通过fsync函数强制os写入到磁盘的时机。有三种方式如下（默认是：每秒fsync一次）：&lt;/p&gt;

&lt;p&gt;appendonly yes           #启用aof持久化方式
  # appendfsync always   #每次收到写命令就立即强制写入磁盘，最慢的，但是保证完全的持久化，不推荐使用
appendfsync everysec     #每秒钟强制写入磁盘一次，在性能和持久化方面做了很好的折中，推荐
   # appendfsync no    #完全依赖os，性能最好,持久化没保证&lt;/p&gt;

&lt;p&gt;aof 的方式也同时带来了另一个问题。持久化文件会变的越来越大。例如我们调用incr test命令100次，文件中必须保存全部的100条命令，其实有99条都是多余的。因为要恢复数据库的状态其实文件中保存一条set test 100就够了。为了压缩aof的持久化文件。redis提供了bgrewriteaof命令。收到此命令redis将使用与快照类似的方式将内存中的数据以命令的方式保存到临时文件中，最后替换原来的文件。具体过程如下：
    1.  redis调用fork ，现在有父子两个进程
    2. 子进程根据内存中的数据库快照，往临时文件中写入重建数据库状态的命令&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;父进程继续处理client请求，除了把写命令写入到原来的aof文件中。同时把收到的写命令缓存起来。这样就能保证如果子进程重写失败的话并不会出问题。&lt;/li&gt;
  &lt;li&gt;当子进程把快照内容写入已命令方式写到临时文件中后，子进程发信号通知父进程。然后父进程把缓存的写命令也写入到临时文件。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;现在父进程可以使用临时文件替换老的aof文件，并重命名，后面收到的写命令也开始往新的aof文件中追加。&lt;/p&gt;

    &lt;p&gt;需要注意到是重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件，这点和快照有点类似。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（三）虚拟内存方式（desprecated）
首先说明：在Redis-2.4后虚拟内存功能已经被deprecated了，原因如下：
1）slow restart重启太慢
2）slow saving保存数据太慢
3）slow replication上面两条导致 replication 太慢
4）complex code代码过于复杂
下面还是介绍一下redis的虚拟内存。
redis的虚拟内存与os的虚拟内存不是一码事，但是思路和目的都是相同的。就是暂时把不经常访问的数据从内存交换到磁盘中，从而腾出宝贵的内存空间用于其他需要访问的数据。尤其是对于redis这样的内存数据库，内存总是不够用的。除了可以将数据分割到多个redis server外。另外的能够提高数据库容量的办法就是使用vm把那些不经常访问的数据交换的磁盘上。如果我们的存储的数据总是有少部分数据被经常访问，大部分数据很少被访问，对于网站来说确实总是只有少量用户经常活跃。当少量数据被经常访问时，使用vm不但能提高单台redis server数据库的容量，而且也不会对性能造成太多影响。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    redis没有使用os提供的虚拟内存机制而是自己在用户态实现了自己的虚拟内存机制,作者在自己的blog专门解释了其中原因。 http://antirez.com/post/redis-virtual-memory-story.html 主要的理由有两点：
   1. os 的虚拟内存是已4k页面为最小单位进行交换的。而redis的大多数对象都远小于4k，所以一个os页面上可能有多个redis对象。另外redis的集合对象类型如list,set可能存在与多个os页面上。最终可能造成只有10%key被经常访问，但是所有os页面都会被os认为是活跃的，这样只有内存真正耗尽时os才会交换页面。
   2.相比于os的交换方式。redis可以将被交换到磁盘的对象进行压缩,保存到磁盘的对象可以去除指针和对象元数据信息。一般压缩后的对象会比内存中的对象小10倍。这样redis的vm会比os vm能少做很多io操作。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是vm相关配置：
 slaveof 192.168.1.1 6379  #指定master的ip和端口
vm-enabled yes          #开启vm功能
vm-swap-file /tmp/redis.swap   #交换出来的value保存的文件路径/tmp/redis.swap
vm-max-memory 1000000  #redis使用的最大内存上限，超过上限后redis开始交换value到磁盘文件中
vm-page-size 32        #每个页面的大小32个字节
vm-pages 134217728     #最多使用在文件中使用多少页面,交换文件的大小 = vm-page-size * vm-pages
vm-max-threads 4       #用于执行value对象换入换出的工作线程数量，0表示不使用工作线程（后面介绍)&lt;/p&gt;

&lt;p&gt;redis的vm在设计上为了保证key的查找速度，只会将value交换到swap文件中。所以如果是内存问题是由于太多value很小的key造成的，那么vm并不能解决。和os一样redis也是按页面来交换对象的。redis规定同一个页面只能保存一个对象。但是一个对象可以保存在多个页面中。
在redis使用的内存没超过vm-max-memory之前是不会交换任何value的。当超过最大内存限制后，redis会选择较老的对象。如果两个对象一样老会优先交换比较大的对象，精确的公式swappability = age*log(size_in_memory)。对于vm-page-size的设置应该根据自己的应用将页面的大小设置为可以容纳大多数对象的大小。太大了会浪费磁盘空间，太小了会造成交换文件出现碎片。对于交换文件中的每个页面，redis会在内存中对应一个1bit值来记录页面的空闲状态。所以像上面配置中页面数量(vm-pages 134217728 )会占用16M内存用来记录页面空闲状态。vm-max-threads表示用做交换任务的线程数量。如果大于0推荐设为服务器的cpu core的数量。如果是0则交换过程在主线程进行。&lt;/p&gt;

&lt;p&gt;参数配置讨论完后，在来简单介绍下vm是如何工作的：
当vm-max-threads设为0时(Blocking VM)
换出：
       主线程定期检查发现内存超出最大上限后，会直接已阻塞的方式,将选中的对象保存到swap文件中，并释放对象占用的内存,此过程会一直重复直到下面条件满足
       1.内存使用降到最大限制以下
       2.swap文件满了
       3.几乎全部的对象都被交换到磁盘了
换入：
              当有client请求value被换出的key时。主线程会以阻塞的方式从文件中加载对应的value对象，加载时此时会阻塞所有client。然后处理client的请求&lt;/p&gt;

&lt;p&gt;当vm-max-threads大于0(Threaded VM)
换出：
       当主线程检测到使用内存超过最大上限，会将选中的要交换的对象信息放到一个队列中交由工作线程后台处理，主线程会继续处理client请求。
换入：
       如果有client请求的key被换出了，主线程先阻塞发出命令的client,然后将加载对象的信息放到一个队列中，让工作线程去加载。加载完毕后工作线程通知主线程。主线程再执行client的命令。这种方式只阻塞请求value被换出key的client&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   总的来说blocking vm的方式总的性能会好一些，因为不需要线程同步，创建线程和恢复被阻塞的client等开销。但是也相应的牺牲了响应性。threaded vm的方式主线程不会阻塞在磁盘io上，所以响应性更好。如果我们的应用不太经常发生换入换出，而且也不太在意有点延迟的话则推荐使用blocking vm的方式。 关于redis vm的更详细介绍可以参考下面链接：
   http://antirez.com/post/redis-virtual-memory-story.html
   http://redis.io/topics/internals-vm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;（四）diskstore方式
diskstore方式是作者放弃了虚拟内存方式后选择的一种新的实现方式，也就是传统的B-tree的方式。具体细节是：
1) 读操作，使用read through以及LRU方式。内存中不存在的数据从磁盘拉取并放入内存，内存中放不下的数据采用LRU淘汰。
2) 写操作，采用另外spawn一个线程单独处理，写线程通常是异步的，当然也可以把cache-flush-delay配置设成0，Redis尽量保证即时写入。但是在很多场合延迟写会有更好的性能，比如一些计数器用Redis存储，在短时间如果某个计数反复被修改，Redis只需要将最终的结果写入磁盘。这种做法作者叫per key persistence。由于写入会按key合并，因此和snapshot还是有差异，disk store并不能保证时间一致性。
由于写操作是单线程，即使cache-flush-delay设成0，多个client同时写则需要排队等待，如果队列容量超过cache-max-memory Redis设计会进入等待状态，造成调用方卡住。
Google Group上有热心网友迅速完成了压力测试，当内存用完之后，set每秒处理速度从25k下降到10k再到后来几乎卡住。 虽然通过增加cache-flush-delay可以提高相同key重复写入性能；通过增加cache-max-memory可以应对临时峰值写入。但是diskstore写入瓶颈最终还是在IO。
3) rdb 和新 diskstore 格式关系
rdb是传统Redis内存方式的存储格式，diskstore是另外一种格式，那两者关系如何？
·         通过BGSAVE可以随时将diskstore格式另存为rdb格式，而且rdb格式还用于Redis复制以及不同存储方式之间的中间格式。
·         通过工具可以将rdb格式转换成diskstore格式。
当然，diskstore原理很美好，但是目前还处于alpha版本，也只是一个简单demo，diskstore.c加上注释只有300行，实现的方法就是将每个value作为一个独立文件保存，文件名是key的hash值。因此diskstore需要将来有一个更高效稳定的实现才能用于生产环境。但由于有清晰的接口设计，diskstore.c也很容易换成一种B-Tree的实现。很多开发者也在积极探讨使用bdb或者innodb来替换默认diskstore.c的可行性。&lt;/p&gt;

&lt;p&gt;下面介绍一下Diskstore的算法。
其实DiskStore类似于Hash算法，首先通过SHA1算法把Key转化成一个40个字符的Hash值，然后把Hash值的前两位作为一级目录，然后把Hash值的三四位作为二级目录，最后把Hash值作为文件名，类似于“/0b/ee/0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33”形式。算法如下：
dsKeyToPath(key):
char path[1024];
char *hashKey = sha1(key);
path[0] = hashKey[0];
path[1] = hashKey[1];
path[2] = ‘/’;
path[3] = hashKey[2];
path[4] = hashKey[3];
path[5] = ‘/’;
memcpy(path + 6, hashKey, 40);
return path;&lt;/p&gt;

&lt;p&gt;存储算法（如key == apple）：
dsSet(key, value, expireTime):
    // d0be2dc421be4fcd0172e5afceea3970e2f3d940
char *hashKey = sha1(key);&lt;/p&gt;

&lt;p&gt;// d0/be/d0be2dc421be4fcd0172e5afceea3970e2f3d940
char *path = dsKeyToPath(hashKey);
FILE *fp = fopen(path, “w”);
rdbSaveKeyValuePair(fp, key, value, expireTime);
fclose(fp)&lt;/p&gt;

&lt;p&gt;获取算法：
dsGet(key):
char *hashKey = sha1(key);
char *path = dsKeyToPath(hashKey);
FILE *fp = fopen(path, “r”);
robj *val = rdbLoadObject(fp);
return val;&lt;/p&gt;

&lt;p&gt;不过DiskStore有个缺点，就是有可能发生两个不同的Key生成一个相同的SHA1 Hash值，这样就有可能出现丢失数据的问题。不过这种情况发生的几率比较少，所以是可以接受的。根据作者的意图，未来可能使用B+tree来替换这种高度依赖文件系统的实现方法。&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/11/15/redis_forever.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/11/15/redis_forever.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>spark toDF 失败原因总结</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;错误提示
value toDF is not a member of org.apache.spark.rdd.RDD[(org.apache.spark.ml.&lt;/p&gt;

&lt;p&gt;解决办法
 val conf = new SparkConf().setAppName(“SimpleParamsExample1”)
    val sc = new SparkContext(conf)&lt;/p&gt;

&lt;p&gt;val sqlContext= new org.apache.spark.sql.SQLContext(sc)
  import sqlContext.implicits._&lt;/p&gt;

&lt;p&gt;错误: 找不到或无法加载主类 example.Statistics
译器顺序：右键项目-properties-scala Compiler -Build manager ：
 set the compile order to JavaThenScala instead of Mixed&lt;/p&gt;

&lt;p&gt;右键项目-properties-scala Compiler -Standard 
 选择安装的scala 版本&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_toDF.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_toDF.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark_start问题原因及解决办法</title>
        <description>&lt;console&gt;:14: error: not found: value spark
       import spark.implicits._
              ^
&lt;console&gt;:14: error: not found: value spark
       import spark.sql
              ^
              
scala&amp;gt;  var rdd = sc.parallelize(1 to 10)
&lt;console&gt;:39: error: not found: value sc
        var rdd = sc.parallelize(1 to 10)
        
        
日志：
Caused by: java.lang.RuntimeException: java.net.ConnectException: Call From localhost/127.0.0.1 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

hadoop没有启动
启动hadoop

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)
Type in expressions to have them evaluated.
Type :help for more information.

scala&amp;gt; var rdd = sc.parallelize(1 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24
&lt;/console&gt;&lt;/console&gt;&lt;/console&gt;&lt;/console&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_start.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_start.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark_rdd创建转换</title>
        <description>&lt;p&gt;RDD创建方式
1）从Hadoop文件系统（如HDFS、Hive、HBase）输入创建。
2）从父RDD转换得到新RDD。
3）通过parallelize或makeRDD将单机数据创建为分布式RDD。
4）基于DB(Mysql)、NoSQL(HBase)、S3(SC3)、数据流创建。&lt;/p&gt;

&lt;p&gt;从集合创建RDD
parallelize
def parallelize&lt;a href=&quot;seq: Seq[T], numSlices: Int = defaultParallelism&quot;&gt;T&lt;/a&gt;(implicit arg0: ClassTag[T]): RDD[T]
从一个Seq集合创建RDD。
参数1：Seq集合，必须。
参数2：分区数，默认为该Application分配到的资源的CPU核数
scala&amp;gt; var rdd = sc.parallelize(1 to 10)&lt;/p&gt;

&lt;p&gt;makeRDD
def makeRDD&lt;a href=&quot;seq: Seq[T], numSlices: Int = defaultParallelism&quot;&gt;T&lt;/a&gt;(implicit arg0: ClassTag[T]): RDD[T]
这种用法和parallelize完全相同
def makeRDD&lt;a href=&quot;seq: Seq[(T, Seq[String])]&quot;&gt;T&lt;/a&gt;(implicit arg0: ClassTag[T]): RDD[T]
该用法可以指定每一个分区的preferredLocations。
scala&amp;gt; var rdd=sc.makeRDD(Seq((1 to 10)))&lt;/p&gt;

&lt;p&gt;从外部存储创建RDD&lt;/p&gt;

&lt;p&gt;textFile
//从hdfs文件创建.
//从hdfs文件创建&lt;br /&gt;
scala&amp;gt; var rdd = sc.textFile(“hdfs:///tmp/lxw1234/1.txt”)&lt;br /&gt;
//从本地文件创建&lt;br /&gt;
scala&amp;gt; var rdd = sc.textFile(“file:///etc/hadoop/conf/core-site.xml”)&lt;/p&gt;

&lt;p&gt;注意这里的本地文件路径需要在Driver和Executor端存在。
从其他HDFS文件格式创建
hadoopFile
sequenceFile
objectFile
newAPIHadoopFile
从Hadoop接口API创建
hadoopRDD
newAPIHadoopRDD
比如：从HBase创建RDD&lt;/p&gt;

&lt;p&gt;scala&amp;gt;  var rdd = sc.parallelize(1 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:26&lt;/console&gt;&lt;/p&gt;

&lt;p&gt;scala&amp;gt; rdd.foreach(println(_))
3
8
1
9
10
4
5
2
6
7&lt;/p&gt;

&lt;p&gt;1.RDD -&amp;gt; Dataset 
scala&amp;gt; val ds = rdd.toDS()
ds: org.apache.spark.sql.Dataset[Int] = [value: int]
scala&amp;gt; ds.foreach(println(_))
1
2
6
7
3
4
5
8
9
10&lt;/p&gt;

&lt;p&gt;2.RDD -&amp;gt; DataFrame 
val df = spark.read.json(rdd)
scala&amp;gt; val df=rdd.toDF()
df: org.apache.spark.sql.DataFrame = [value: int]&lt;/p&gt;

&lt;p&gt;scala&amp;gt; df.foreach(println(_))
[1]
[3]
[2]
[6]
[7]
[4]
[8]
[9]
[10]
[5]&lt;/p&gt;

&lt;p&gt;3.Dataset -&amp;gt; RDD
val rdd = ds.rdd&lt;/p&gt;

&lt;p&gt;4.Dataset -&amp;gt; DataFrame
val df = ds.toDF()&lt;/p&gt;

&lt;p&gt;5.DataFrame -&amp;gt; RDD
val rdd = df.toJSON.rdd&lt;/p&gt;

&lt;p&gt;6.DataFrame -&amp;gt; Dataset
val ds = df.toJSON&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_rdd.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_rdd.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark的ML和MLLib两个包区别和联系</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;ML的API是面向Dataset的（Dataframe是Dataset的子集，也就是Dataset[Row]）， mllib是面对RDD的。Dataset和RDD有啥不一样呢？Dataset的底端是RDD。Dataset对RDD进行了更深一层的优化，比如说有sql语言类似的黑魔法，Dataset支持静态类型分析所以在compile time就能报错，各种combinators（map，foreach等）性能会更好&lt;/p&gt;

&lt;p&gt;spark.mllib中的算法接口是基于RDDs的；spark.ml中的算法接口是基于DataFrames的。实际使用中推荐ml，建立在DataFrames基础上的ml中一系列算法更适合创建包含从数据清洗到特征工程再到模型训练等一系列工作的ML pipeline；而且未来mllib也会被弃用。&lt;/p&gt;

&lt;p&gt;Spark机器学习库现支持两种接口的API:RDD-based和DataFrame-based，Spark官方网站上说，RDD-based APIs在2.0后进入维护模式，主要的机器学习API是spark-ml包中的DataFrame-based API，并将在3.0后完全移除RDD-based API。&lt;/p&gt;

&lt;p&gt;DataFrame-based API 包含在org.apache.spark.ml包中，其中主要的类结构如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/sparkml.png&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_ml_mllib.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/spark_ml_mllib.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>RDD/Dataset/DataFrame互转</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;1.RDD -&amp;gt; Dataset 
val ds = rdd.toDS()&lt;/p&gt;

&lt;p&gt;2.RDD -&amp;gt; DataFrame 
val df = spark.read.json(rdd)&lt;/p&gt;

&lt;p&gt;3.Dataset -&amp;gt; RDD
val rdd = ds.rdd&lt;/p&gt;

&lt;p&gt;4.Dataset -&amp;gt; DataFrame
val df = ds.toDF()&lt;/p&gt;

&lt;p&gt;5.DataFrame -&amp;gt; RDD
val rdd = df.toJSON.rdd&lt;/p&gt;

&lt;p&gt;6.DataFrame -&amp;gt; Dataset
val ds = df.toJSON&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/rdd_dataset_dataframe.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/rdd_dataset_dataframe.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>dataSet和dataFrame的创建方法</title>
        <description>&lt;p&gt;Spark创建DataFrame的三种方法
跟关系数据库的表(Table)一样，DataFrame是Spark中对带模式(schema)行列数据的抽象。DateFrame广泛应用于使用SQL处理大数据的各种场景。&lt;/p&gt;

&lt;p&gt;通过导入(importing)Spark sql implicits, 就可以将本地序列(seq), 数组或者RDD转为DataFrame。只要这些数据的内容能指定数据类型即可。
本地seq + toDF创建DataFrame示例：&lt;/p&gt;

&lt;p&gt;import sqlContext.implicits._
val df = Seq(
  (1, “First Value”, java.sql.Date.valueOf(“2010-01-01”)),
  (2, “Second Value”, java.sql.Date.valueOf(“2010-02-01”))
).toDF(“int_column”, “string_column”, “date_column”)
注意：如果直接用toDF()而不指定列名字，那么默认列名为”_1”, “_2”, …&lt;/p&gt;

&lt;p&gt;通过case class + toDF创建DataFrame的示例&lt;/p&gt;

&lt;p&gt;// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._&lt;/p&gt;

&lt;p&gt;// Define the schema using a case class.
// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
// you can use custom classes that implement the Product interface.
case class Person(name: String, age: Int)&lt;/p&gt;

&lt;p&gt;// Create an RDD of Person objects and register it as a table.
val people = sc.textFile(“examples/src/main/resources/people.txt”).map(_.split(“,”)).map(p =&amp;gt; Person(p(0), p(1).trim.toInt)).toDF()
people.registerTempTable(“people”)&lt;/p&gt;

&lt;p&gt;// 使用 sqlContext 执行 sql 语句.
val teenagers = sqlContext.sql(“SELECT name FROM people WHERE age &amp;gt;= 13 AND age &amp;lt;= 19”)&lt;/p&gt;

&lt;p&gt;// 注：sql()函数的执行结果也是DataFrame，支持各种常用的RDD操作.
// The columns of a row in the result can be accessed by ordinal.
teenagers.map(t =&amp;gt; “Name: “ + t(0)).collect().foreach(println)&lt;/p&gt;

&lt;p&gt;方法二，Spark中使用createDataFrame函数创建DataFrame&lt;/p&gt;

&lt;p&gt;在SqlContext中使用createDataFrame也可以创建DataFrame。跟toDF一样，这里创建DataFrame的数据形态也可以是本地数组或者RDD。
通过row+schema创建示例&lt;/p&gt;

&lt;p&gt;import org.apache.spark.sql.types._
val schema = StructType(List(
    StructField(“integer_column”, IntegerType, nullable = false),
    StructField(“string_column”, StringType, nullable = true),
    StructField(“date_column”, DateType, nullable = true)
))&lt;/p&gt;

&lt;p&gt;val rdd = sc.parallelize(Seq(
  Row(1, “First Value”, java.sql.Date.valueOf(“2010-01-01”)),
  Row(2, “Second Value”, java.sql.Date.valueOf(“2010-02-01”))
))
val df = sqlContext.createDataFrame(rdd, schema)
方法三，通过文件直接创建DataFrame&lt;/p&gt;

&lt;p&gt;使用parquet文件创建&lt;/p&gt;

&lt;p&gt;val df = sqlContext.read.parquet(“hdfs:/path/to/file”)
使用json文件创建&lt;/p&gt;

&lt;p&gt;val df = spark.read.json(“examples/src/main/resources/people.json”)&lt;/p&gt;

&lt;p&gt;// Displays the content of the DataFrame to stdout
df.show()
// +—-+——-+
// | age|   name|
// +—-+——-+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +—-+——-+
使用csv文件,spark2.0+之后的版本可用&lt;/p&gt;

&lt;p&gt;//首先初始化一个SparkSession对象
val spark = org.apache.spark.sql.SparkSession.builder
        .master(“local”)
        .appName(“Spark CSV Reader”)
        .getOrCreate;&lt;/p&gt;

&lt;p&gt;//然后使用SparkSessions对象加载CSV成为DataFrame
val df = spark.read
        .format(“com.databricks.spark.csv”)
        .option(“header”, “true”) //reading the headers
        .option(“mode”, “DROPMALFORMED”)
        .load(“csv/file/path”); //.csv(“csv/file/path”) //spark 2.0 api&lt;/p&gt;

&lt;p&gt;df.show()&lt;/p&gt;

&lt;p&gt;DataSet数据集是一个强类型的域特定对象的集合，可以使用功能或关系操作并行转换.。每个数据集还有一个无类型的视图称为Dataframe，这是一个行(Row)的数据集。
在DataSet上的操作，分为transformations和actions。transformations会产生新的数据集（DataSet），而actions则是触发计算并产生结果。transformations包括：map, filter, select, and aggregate (&lt;code&gt;groupBy&lt;/code&gt;). 等操作。而actions 包括： count, show 或把数据写入文件系统中。
DataSet是懒惰(‘lazy’)的，也就是说当一个action被调用时才会触发一个计算。在内部实现，数据集表示的是一个逻辑计划，它描述了生成数据所需的计算。当action被调用时，spark的查询优化器会优化这个逻辑计划，并生成一个物理计划，该物理计划可以通过并行和分布式的方式来执行。使用&lt;code&gt;explain&lt;/code&gt;解释函数，来进行逻辑计划的探索和物理计划的优化。
为了有效地支持特定领域的对象，Encoder（编码器）是必需的。例如，给出一个Person的类，有两个字段：name(string)和age(int)，通过一个encoder来告诉spark在运行的时候产生代码把Person对象转换成一个二进制结构。这种二进制结构通常有更低的内存占用，以及优化的数据处理效率（例如在一个柱状格式）。若要了解数据的内部二进制表示，请使用schema(表结构)函数。&lt;/p&gt;

&lt;p&gt;创建一个数据集有两种方式。
1, 第一种也是最常用的一种，就是利用SparkSession的read函数在存储系统中读取一个文件。代码如下：
val people = spark.read.parquet(“…”).as[Person]&lt;/p&gt;

&lt;p&gt;2, 利用Datasets的转换(transformations)函数，例如下面的代码：利用filter来创建一个新的Dataset。&lt;/p&gt;

&lt;p&gt;数据集(Dataset)的操作是无类型的，通过各种DSL(domain-specific-language)函数，这些函数是基于数据集Dataset , 类[[Column]],和 函数[[functions]]来定义的。这些操作非常类似基于R和Python抽象出来的data frame的操作。
从Dateset中选择一列
val ageCol = people(“age”) // in Scala&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/11/14/dataSet_dataFrame.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/11/14/dataSet_dataFrame.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>php_session</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;void session_write_close ( void )&lt;/p&gt;

&lt;p&gt;End the current session and store session data.&lt;/p&gt;

&lt;p&gt;Session data is usually stored after your script terminated without the need to call session_write_close(), but as session data is locked to prevent concurrent writes only one script may operate on a session at any time. When using framesets together with sessions you will experience the frames loading one by one due to this locking. You can reduce the time needed to load all the frames by ending the session as soon as all changes to session variables are done.
也就是说session是有锁的，为防止并发的写会话数据,php自带的的文件保存会话数据是加了一个互斥锁（在session_start()的时候）。 
程序执行session_start()，此时当前程序就开始持有锁。 
程序结束，此时程序自动释放Session的锁。&lt;/p&gt;

&lt;p&gt;如果同一个客户端同时并发发送多个请求（如ajax在页面同时发送多个请求），且脚本执行时间较长，就会导致session文件阻塞，影响性能。因为对于每个请求，PHP执行session_start()，就会取得文件独占锁，只有在该请求处理结束后，才会释放独占锁。这样，同时多个请求就会引起阻塞。解决方案如下： 
修改会话变量后，立即使用session_write_close()来保存会话数据并释放文件锁。
session_start(); &lt;br /&gt;
$_SESSION[‘test’] = ‘test’;
session_write_close();&lt;/p&gt;

&lt;p&gt;(PHP 5 &amp;gt;= 5.3.3, PHP 7)
fastcgi_finish_request — 冲刷(flush)所有响应的数据给客户端
如果有锁的话会使异步作用失效&lt;/p&gt;

&lt;p&gt;There are some pitfalls  you should be aware of when using this function.&lt;/p&gt;

&lt;p&gt;The script will still occupy a FPM process after fastcgi_finish_request(). So using it excessively for long running tasks may occupy all your FPM threads up to pm.max_children. This will lead to gateway errors on the webserver.&lt;/p&gt;

&lt;p&gt;Another important thing is session handling. Sessions are locked as long as they’re active (see the documentation for session_write_close()). This means subsequent requests will block until the session is closed.&lt;/p&gt;

&lt;p&gt;You should therefore call session_write_close() as soon as possible (even before fastcgi_finish_request()) to allow subsequent requests and a good user experience.&lt;/p&gt;

&lt;p&gt;This also applies for all other locking techniques as flock or database locks for example. As long as a lock is active subsequent requests might bock.&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/11/09/php_session.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/11/09/php_session.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>jupyter_matplotlib</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;安装：
$pip install matplotlib
$jupyter notebook&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;module://ipykernel.pylab.backend_inline&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5 &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterplot.png&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# We&amp;#39;ll explain the &amp;quot;111&amp;quot; later. Basically, 1 row and 1 column.&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;An Example Axes&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Y-Axis&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;X-Axis&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterplotAxes.png&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;lightblue&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#绘制线&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;26&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;darkgreen&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;^&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#绘制散点图&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterplotLine.png&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterplotMulti.png&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 09 Nov 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/11/09/jupyter_matplotlib.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/11/09/jupyter_matplotlib.html</guid>
        
        
        <category>web</category>
        
      </item>
    
  </channel>
</rss>
