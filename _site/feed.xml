<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 16 Jan 2018 17:02:15 +0800</pubDate>
    <lastBuildDate>Tue, 16 Jan 2018 17:02:15 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>Linux、Mac上面ln命令使用说明</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;ln是linux中又一个非常重要命令，它的功能是为某一个文件在另外一个位置建立一个同不的链接，这个命令最常用的参数是 -s，具体用法是：ln –s 源文件 目标文件。&lt;/p&gt;

&lt;p&gt;当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。例如：ln –s /bin/less /usr/local/bin/less 
-s 是代号（symbolic）的意思&lt;/p&gt;

&lt;p&gt;注意：
第一，ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化；
第二，ln的链接又分软链接和硬链接两种，软链接就是ln –s ** **，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接ln ** **，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 　　&lt;/p&gt;

&lt;p&gt;如果你用ls查看一个目录时，发现有的文件后面有一个@的符号，那就是一个用ln命令生成的文件，用ls –l命令去察看，就可以看到显示的link的路径了。&lt;/p&gt;

&lt;p&gt;指令详细说明(可自行man ln查看)
[plain] view plain copy
指令名称 : ln&lt;br /&gt;
使用权限 : 所有使用者&lt;br /&gt;
使用方式 : ln [options] source dist，其中 option 的格式为 :&lt;br /&gt;
[-bdfinsvF] [-S backup-suffix] [-V {numbered,existing,simple}]&lt;br /&gt;
[–help] [–version] [–]&lt;br /&gt;
说明 : Linux/Unix 档案系统中，有所谓的连结(link)，我们可以将其视为档案的别名，而连结又可分为两种 : 硬连结(hard link)与软连结(symbolic link)，硬连结的意思是一个档案可以有多个名称，而软连结的方式则是产生一个特殊的档案，该档案的内容是指向另一个档案的位置。硬连结是存在同一个档 案系统中，而软连结却可以跨越不同的档案系统。&lt;br /&gt;
ln source dist 是产生一个连结(dist)到 source，至于使用硬连结或软链结则由参数决定。&lt;br /&gt;
不论是硬连结或软链结都不会将原本的档案复制一份，只会占用非常少量的磁碟空间。&lt;br /&gt;
-f : 链结时先将与 dist 同档名的档案删除&lt;br /&gt;
-d : 允许系统管理者硬链结自己的目录&lt;br /&gt;
-i : 在删除与 dist 同档名的档案时先进行询问&lt;br /&gt;
-n : 在进行软连结时，将 dist 视为一般的档案&lt;br /&gt;
-s : 进行软链结(symbolic link)&lt;br /&gt;
-v : 在连结之前显示其档名&lt;br /&gt;
-b : 将在链结时会被覆写或删除的档案进行备份&lt;br /&gt;
-S SUFFIX : 将备份的档案都加上 SUFFIX 的字尾&lt;br /&gt;
-V METHOD : 指定备份的方式&lt;br /&gt;
–help : 显示辅助说明&lt;br /&gt;
–version : 显示版本&lt;br /&gt;
范例 :&lt;br /&gt;
将档案 yy 产生一个 symbolic link : zz&lt;br /&gt;
ln -s yy zz&lt;br /&gt;
将档案 yy 产生一个 hard link : zz&lt;br /&gt;
ln yy xx﻿&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2018/01/16/ln.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2018/01/16/ln.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>spark on hive</title>
        <description>&lt;p&gt;$cp hive/hive/conf/hive-site.xml spark/spark/conf/
&lt;!-- more --&gt;
启动hive
启动spark&lt;/p&gt;

&lt;p&gt;import org.apache.spark.sql.SparkSession
val sparkSession = SparkSession.builder.master(“local”).enableHiveSupport().getOrCreate();&lt;/p&gt;

&lt;p&gt;Caused by: ERROR XJ040: Failed to start database ‘metastore_db’ with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@294b045b, see the next exception for details.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)
	… 150 more
Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /Users/didi/metastore_db.&lt;/p&gt;

&lt;p&gt;在使用Hive on Spark模式操作hive里面的数据时，报以上错误，原因是因为HIVE采用了derby这个内嵌数据库作为数据库，它不支持多用户同时访问,解决办法就是把derby数据库换成mysql数据库即可&lt;/p&gt;

&lt;p&gt;解决方式：&lt;/p&gt;

&lt;p&gt;不使用默认的内嵌数据库derby，采用mysql作为统计的存储信息。&lt;/p&gt;

&lt;p&gt;修改相关配置信息（hive-site.xml）：&lt;/p&gt;

&lt;property&gt;

       &lt;name&gt;hive.stats.dbclass&lt;/name&gt;

       &lt;value&gt;jdbc:mysql&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

       &lt;name&gt;hive.stats.jdbcdriver&lt;/name&gt;

       &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

       &lt;name&gt;hive.stats.dbconnectionstring&lt;/name&gt;

       &lt;value&gt;jdbc:mysql://localhost:3306/TempStatsStore&lt;/value&gt;

&lt;/property&gt;

&lt;p&gt;修改完成保存。
另外后面还有一个步骤就是要在mysql里创建TempStatsStore这个数据库（mysql里不会自动创建该库，在derby里会自动创建）&lt;/p&gt;

&lt;p&gt;方式二
mv  metastore_db metastore_db_bak&lt;/p&gt;

&lt;p&gt;scala&amp;gt; val sparkSession = SparkSession.builder.master(“local”).enableHiveSupport().getOrCreate();
18/01/13 15:55:44 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
18/01/13 15:55:45 WARN conf.HiveConf: HiveConf of name hive.conf.hidden.list does not exist
18/01/13 15:55:47 WARN conf.HiveConf: HiveConf of name hive.conf.hidden.list does not exist
18/01/13 15:55:49 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/01/13 15:55:50 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
18/01/13 15:55:51 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/01/13 15:55:51 WARN conf.HiveConf: HiveConf of name hive.conf.hidden.list does not exist
sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@14c16388&lt;/p&gt;

&lt;p&gt;beeline&amp;gt;  !connect jdbc:hive2://localhost:10000&lt;/p&gt;

&lt;p&gt;0: jdbc:hive2://localhost:10000&amp;gt; show tables;
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don’t support retries at the client level.) (state=08S01,code=1)&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“CREATE TABLE IF NOT EXISTS src (key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ “)
18/01/13 16:04:23 WARN metastore.HiveMetaStore: Location: hdfs://localhost:8020/user/hive/warehouse/src specified for non-external table:src
res8: org.apache.spark.sql.DataFrame = []&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“show tables”).collect().foreach(println)
[default,src,false]&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“show databases”).collect().foreach(println)
[default]&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“show create  table src”).collect().foreach(println)
[CREATE TABLE &lt;code&gt;src&lt;/code&gt;(&lt;code&gt;key&lt;/code&gt; int, &lt;code&gt;value&lt;/code&gt; string)
ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe’
WITH SERDEPROPERTIES (
  ‘field.delim’ = ‘	‘,
  ‘serialization.format’ = ‘	‘
)
STORED AS
  INPUTFORMAT ‘org.apache.hadoop.mapred.TextInputFormat’
  OUTPUTFORMAT ‘org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat’
TBLPROPERTIES (
  ‘transient_lastDdlTime’ = ‘1515830663’
)
]&lt;/p&gt;

&lt;p&gt;hive不支持用insert语句一条一条的进行插入操作，也不支持update操作。数据是以load的方式加载到建立好的表中。数据一旦导入就不可以修改。&lt;/p&gt;

&lt;p&gt;DML包括：INSERT插入、UPDATE更新、DELETE删除&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“insert into table src key=1234,value=’abc’”).collect().foreach(println)
org.apache.spark.sql.catalyst.parser.ParseException:
no viable alternative at input ‘key’(line 1, pos 22)&lt;/p&gt;

&lt;p&gt;== SQL ==
insert into table src key=1234,value=’abc’
———————-^^^&lt;/p&gt;

&lt;p&gt;既然Hive没有行级别的数据插入、更新和删除操作，那么往表中装载数据的唯一途径就是使用一种”大量“的数据装载操作。我们以如下格式文件演示五种数据导入Hive方式&lt;/p&gt;

&lt;p&gt;Tom         24    NanJing   Nanjing University&lt;br /&gt;
Jack        29    NanJing   Southeast China University&lt;br /&gt;
Mary Kake   21    SuZhou    Suzhou University&lt;br /&gt;
John Doe    24    YangZhou  YangZhou University&lt;br /&gt;
Bill King   23    XuZhou    Xuzhou Normal University&lt;/p&gt;

&lt;p&gt;数据格式以\t分隔，分别表示：姓名、年龄、地址、学校&lt;/p&gt;

&lt;p&gt;一、从本地文件系统中导入数据
 (1) 创建test1测试表
scala&amp;gt; sparkSession.sql(“CREATE TABLE test1(name STRING,age INT, address STRING,school STRING)   ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’  STORED AS TEXTFILE”).collect().foreach(println)
18/01/13 16:20:50 WARN metastore.HiveMetaStore: Location: hdfs://localhost:8020/user/hive/warehouse/test1 specified for non-external table:test1
(2) 从本地加载数据 
scala&amp;gt; sparkSession.sql(“ LOAD DATA LOCAL INPATH ‘/Users/didi/hive/testHive.txt’ INTO TABLE test1”).collect().foreach(println) 
(3) 查看导入结果
scala&amp;gt; sparkSession.sql(“select * from test1”).collect().foreach(println)
[Tom         24    NanJing   Nanjing University  ,null,null,null]
[Jack        29    NanJing   Southeast China University  ,null,null,null]
[Mary Kake   21    SuZhou    Suzhou University  ,null,null,null]
[John Doe    24    YangZhou  YangZhou University  ,null,null,null]
[Bill King   23    XuZhou    Xuzhou Normal University,null,null,null] 
        注意：此处使用的是LOCAL，表示从本地文件系统中加载数据到Hive中，同时没有OVERWRITE关键字，仅仅会把新增的文件增加到目标文件夹而不会删除之前的数据。如果使用OVERWRITE关键字，那么目标文件夹中之前的数据将会被先删除掉。
二、从HDFS文件系统加载数据到Hive
(1) 清空之前创建的表中数据
insert overwrite table test1  select * from test1 where 1=0;  //清空表，一般不推荐这样操作&lt;br /&gt;
 (2) 从HDFS加载数据
hive&amp;gt; LOAD DATA INPATH “/input/test1.txt”&lt;br /&gt;
    &amp;gt; OVERWRITE INTO TABLE test1;&lt;br /&gt;
Loading data to table hive.test1&lt;br /&gt;
rmr: DEPRECATED: Please use ‘rm -r’ instead.&lt;br /&gt;
Deleted hdfs://secondmgt:8020/hive/warehouse/hive.db/test1&lt;br /&gt;
Table hive.test1 stats: [numFiles=1, numRows=0, totalSize=201, rawDataSize=0]&lt;br /&gt;
OK&lt;br /&gt;
Time taken: 0.355 seconds&lt;br /&gt;
 (3) 查询结果
hive&amp;gt; select * from test1;&lt;br /&gt;
OK&lt;br /&gt;
Tom     24.0    NanJing Nanjing University&lt;br /&gt;
Jack    29.0    NanJing Southeast China University&lt;br /&gt;
Mary Kake       21.0    SuZhou  Suzhou University&lt;br /&gt;
John Doe        24.0    YangZhou        YangZhou University&lt;br /&gt;
Bill King       23.0    XuZhou  Xuzhou Normal University&lt;br /&gt;
Time taken: 0.054 seconds, Fetched: 5 row(s)&lt;br /&gt;
        注意：此处没有LOCAL关键字，表示分布式文件系统中的路径，这就是和第一种方法的主要区别，同时由日志可以发现，因为此处加了OVERWRITE关键字，执行了Deleted操作，即先删除之前存储的数据，然后再执行加载操作。
       同时，INPATH子句中使用的文件路径还有一个限制，那就是这个路径下不可以包含任何文件夹。&lt;/p&gt;

&lt;p&gt;三、通过查询语句向表中插入数据
(1) 创建test4测试表
scala&amp;gt; sparkSession.sql(“ CREATE TABLE test4(name STRING,age FLOAT,address STRING,school STRING)   ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ STORED AS TEXTFILE”).collect().foreach(println)
18/01/13 16:27:53 WARN metastore.HiveMetaStore: Location: hdfs://localhost:8020/user/hive/warehouse/test4 specified for non-external table:test4
 创建表过程基本和前面一样，此处不细讲
(2) 从查询结果中导入数据
scala&amp;gt; sparkSession.sql(“INSERT INTO TABLE test4 SELECT * FROM test1”).collect().foreach(println) 
        注意：新建表的字段数，一定要和后面SELECT中查询的字段数一样，且要注意数据类型。如test4包含四个字段：name、age、address和school，则SELECT查询出的结果也应该对应这四个字段。
(3) 查看导入结果
scala&amp;gt; sparkSession.sql(“select * from test4”).collect().foreach(println)
[Tom         24    NanJing   Nanjing University  ,null,null,null]
[Jack        29    NanJing   Southeast China University  ,null,null,null]
[Mary Kake   21    SuZhou    Suzhou University  ,null,null,null]
[John Doe    24    YangZhou  YangZhou University  ,null,null,null]
[Bill King   23    XuZhou    Xuzhou Normal University,null,null,null]
四、分区插入
        分区插入有两种，一种是静态分区，另一种是动态分区。如果混合使用静态分区和动态分区，则静态分区必须出现在动态分区之前。现分别介绍这两种分区插入
(1) 静态分区插入&lt;/p&gt;

&lt;p&gt;①创建分区表
hive&amp;gt; CREATE TABLE test2(name STRING,address STRING,school STRING)&lt;br /&gt;
    &amp;gt; PARTITIONED BY(age float)&lt;br /&gt;
    &amp;gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’&lt;br /&gt;
    &amp;gt; STORED AS TEXTFILE ;&lt;br /&gt;
OK&lt;br /&gt;
Time taken: 0.144 seconds&lt;br /&gt;
       此处创建了一个test2的分区表，以年龄分区&lt;/p&gt;

&lt;p&gt;②从查询结果中导入数据    &lt;br /&gt;
hive&amp;gt; INSERT INTO  TABLE test2 PARTITION (age=’24’) SELECT * FROM test1;&lt;br /&gt;
FAILED: SemanticException [Error 10044]: Line 1:19 Cannot insert into target table because column number/types are different ‘‘24’’: Table insclause-0 has 3 columns, but query has 4 columns.&lt;br /&gt;
 此处报了一个错误。是因为test2中是以age分区的，有三个字段，SELECT * 语句中包含有四个字段，所以出错。正确如下：
[html] view plain copy
hive&amp;gt; INSERT INTO  TABLE test2 PARTITION (age=’24’) SELECT name,address,school FROM test1;&lt;br /&gt;
 ③ 查看插入结果
hive&amp;gt; select * from test2;&lt;br /&gt;
OK&lt;br /&gt;
Tom     NanJing Nanjing University      24.0&lt;br /&gt;
Jack    NanJing Southeast China University      24.0&lt;br /&gt;
Mary Kake       SuZhou  Suzhou University       24.0&lt;br /&gt;
John Doe        YangZhou        YangZhou University     24.0&lt;br /&gt;
Bill King       XuZhou  Xuzhou Normal University        24.0&lt;br /&gt;
Time taken: 0.079 seconds, Fetched: 5 row(s)&lt;br /&gt;
 由查询结果可知，每条记录的年龄均为24，插入成功。
(2) 动态分区插入
静态分区需要创建非常多的分区，那么用户就需要写非常多的SQL！Hive提供了一个动态分区功能，其可以基于查询参数推断出需要创建的分区名称。&lt;/p&gt;

&lt;p&gt;① 创建分区表，此过程和静态分区创建表一样，此处省略&lt;/p&gt;

&lt;p&gt;② 参数设置
hive&amp;gt; set hive.exec.dynamic.partition=true;&lt;br /&gt;
hive&amp;gt; set hive.exec.dynamic.partition.mode=nonstrict;&lt;br /&gt;
 注意：动态分区默认情况下是没有开启的。开启后，默认是以”严格“模式执行的，在这种模式下要求至少有一列分区字段是静态的。这有助于阻止因设计错误导致查询产生大量的分区。但是此处我们不需要静态分区字段，估将其设为nonstrict。
③ 数据动态插入
hive&amp;gt; insert into table test2 partition (age) select name,address,school,age from test1;&lt;br /&gt;
Total jobs = 1&lt;br /&gt;
Launching Job 1 out of 1&lt;br /&gt;
Number of reduce tasks not specified. Estimated from input data size: 1&lt;br /&gt;
In order to change the average load for a reducer (in bytes):&lt;br /&gt;
  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;  
In order to limit the maximum number of reducers:  
  set hive.exec.reducers.max=&lt;number&gt;  
In order to set a constant number of reducers:  
  set mapreduce.job.reduces=&lt;number&gt;  
Starting Job = job_1419317102229_0029, Tracking URL = http://secondmgt:8088/proxy/application_1419317102229_0029/  
Kill Command = /home/hadoopUser/cloud/hadoop/programs/hadoop-2.2.0/bin/hadoop job  -kill job_1419317102229_0029  
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1  
2014-12-28 20:45:07,996 Stage-1 map = 0%,  reduce = 0%  
2014-12-28 20:45:21,488 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.67 sec  
2014-12-28 20:45:32,926 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.32 sec  
MapReduce Total cumulative CPU time: 7 seconds 320 msec  
Ended Job = job_1419317102229_0029  
Loading data to table hive.test2 partition (age=null)  
        Loading partition {age=29.0}  
        Loading partition {age=23.0}  
        Loading partition {age=21.0}  
        Loading partition {age=24.0}  
Partition hive.test2{age=21.0} stats: [numFiles=1, numRows=1, totalSize=35, rawDataSize=34]  
Partition hive.test2{age=23.0} stats: [numFiles=1, numRows=1, totalSize=42, rawDataSize=41]  
Partition hive.test2{age=24.0} stats: [numFiles=1, numRows=2, totalSize=69, rawDataSize=67]  
Partition hive.test2{age=29.0} stats: [numFiles=1, numRows=1, totalSize=40, rawDataSize=39]  
MapReduce Jobs Launched:  
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 7.32 sec   HDFS Read: 415 HDFS Write: 375 SUCCESS  
Total MapReduce CPU Time Spent: 7 seconds 320 msec  
OK  
Time taken: 41.846 seconds  
 注意：查询语句select查询出来的age字段必须放在最后，和分区字段对应，不然结果会出错
④ 查看插入结果
[html] view plain copy
hive&amp;gt; select * from test2;  
OK  
Mary Kake       SuZhou  Suzhou University       21.0  
Bill King       XuZhou  Xuzhou Normal University        23.0  
John Doe        YangZhou        YangZhou University     24.0  
Tom     NanJing Nanjing University      24.0  
Jack    NanJing Southeast China University      29.0  
五、单个查询语句中创建表并加载数据
         在实际情况中，表的输出结果可能太多，不适于显示在控制台上，这时候，将Hive的查询输出结果直接存在一个新的表中是非常方便的，我们称这种情况为CTAS（create table .. as select）
        (1) 创建表
hive&amp;gt; CREATE TABLE test3  
    &amp;gt; AS  
    &amp;gt; SELECT name,age FROM test1;  
Total jobs = 3  
Launching Job 1 out of 3  
Number of reduce tasks is set to 0 since there's no reduce operator  
Starting Job = job_1419317102229_0030, Tracking URL = http://secondmgt:8088/proxy/application_1419317102229_0030/  
Kill Command = /home/hadoopUser/cloud/hadoop/programs/hadoop-2.2.0/bin/hadoop job  -kill job_1419317102229_0030  
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0  
2014-12-28 20:59:59,375 Stage-1 map = 0%,  reduce = 0%  
2014-12-28 21:00:10,795 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.68 sec  
MapReduce Total cumulative CPU time: 2 seconds 680 msec  
Ended Job = job_1419317102229_0030  
Stage-4 is selected by condition resolver.  
Stage-3 is filtered out by condition resolver.  
Stage-5 is filtered out by condition resolver.  
Moving data to: hdfs://secondmgt:8020/hive/scratchdir/hive_2014-12-28_20-59-45_494_6763514583931347886-1/-ext-10001  
Moving data to: hdfs://secondmgt:8020/hive/warehouse/hive.db/test3  
Table hive.test3 stats: [numFiles=1, numRows=5, totalSize=63, rawDataSize=58]  
MapReduce Jobs Launched:  
Job 0: Map: 1   Cumulative CPU: 2.68 sec   HDFS Read: 415 HDFS Write: 129 SUCCESS  
Total MapReduce CPU Time Spent: 2 seconds 680 msec  
OK  
Time taken: 26.583 seconds  
 (2) 查看插入结果
hive&amp;gt; select * from test3;  
OK  
Tom     24.0  
Jack    29.0  
Mary Kake       21.0  
John Doe        24.0  
Bill King       23.0  
Time taken: 0.045 seconds, Fetched: 5 row(s)&lt;/number&gt;&lt;/number&gt;&lt;/number&gt;&lt;/p&gt;

&lt;p&gt;Exception in thread “main” java.lang.NoClassDefFoundError: scala/Product$class
	at org.apache.spark.internal.config.ConfigBuilder.&lt;init&gt;(ConfigBuilder.scala:176)
	at org.apache.spark.sql.internal.SQLConf$.buildConf(SQLConf.scala:58)
	at org.apache.spark.sql.internal.SQLConf$.&lt;init&gt;(SQLConf.scala:67)
	at org.apache.spark.sql.internal.SQLConf$.&lt;clinit&gt;(SQLConf.scala)
	at org.apache.spark.sql.internal.StaticSQLConf$.&lt;init&gt;(StaticSQLConf.scala:31)
	at org.apache.spark.sql.internal.StaticSQLConf$.&lt;clinit&gt;(StaticSQLConf.scala)
	at org.apache.spark.sql.SparkSession$Builder.enableHiveSupport(SparkSession.scala:843)
	at main.scala.hiveConnection$.main(hiveConnection.scala:6)
	at main.scala.hiveConnection.main(hiveConnection.scala)&lt;/clinit&gt;&lt;/init&gt;&lt;/clinit&gt;&lt;/init&gt;&lt;/init&gt;&lt;/p&gt;

&lt;p&gt;在使用Log4j时若提示如下信息：
log4j:WARN No appenders could be found for logger (org.apache.ibatis.logging.LogFactory).&lt;br /&gt;
log4j:WARN Please initialize the log4j system properly. 
则，解决办法为：在项目的src下面新建file名为log4j.properties文件，内容如下:&lt;/p&gt;

&lt;h1 id=&quot;configure-logging-for-testing-optionally-with-log-file&quot;&gt;Configure logging for testing: optionally with log file&lt;/h1&gt;
&lt;p&gt;#可以设置级别：debug&amp;gt;info&amp;gt;error
#debug:可以显式debug,info,error
#info:可以显式info,error
#error:可以显式error&lt;/p&gt;

&lt;p&gt;log4j.rootLogger=debug,appender1
#log4j.rootLogger=info,appender1
#log4j.rootLogger=error,appender1&lt;/p&gt;

&lt;p&gt;#输出到控制台
log4j.appender.appender1=org.apache.log4j.ConsoleAppender
#样式为TTCCLayout
log4j.appender.appender1.layout=org.apache.log4j.TTCCLayout&lt;/p&gt;

&lt;p&gt;然后，存盘退出。再次运行程序就会显示Log信息了。&lt;/p&gt;

&lt;p&gt;通过配置文件可知，我们需要配置3个方面的内容：
1、根目录（级别和目的地）；
2、目的地（控制台、文件等等）；
3、输出样式。&lt;/p&gt;

&lt;p&gt;或者，使用下面的内容也可以：
 # Configure logging for testing: optionally with log file
log4j.rootLogger=WARN, stdout
 # log4j.rootLogger=WARN, stdout, logfile
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n&lt;/p&gt;

</description>
        <pubDate>Sat, 13 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/13/spark_hive.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/13/spark_hive.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>scala maven 版本冲突问题解决</title>
        <description>&lt;p&gt;scalatest_2.10-1.9.1.jar of core build path is cross-compiled with an incompatible version of Scala (2.10.0)&lt;/p&gt;

&lt;p&gt;Eclipse - Preferences - Scala - Compiler - Build manager
uncheck withVersionClasspathVariable&lt;/p&gt;

&lt;p&gt;More than one scala library found in the build path (/home/hadoop/eclipse/plugins/org.scala-lang.scala-library_2.11.7.v20150622-112736-1fbce4612c.jar, /usr/local/spark/spark-1.5.1-bin-hadoop2.6/lib/spark-assembly-1.5.1-hadoop2.6.0.jar).At least one has an incompatible version. Please update the project build path so it contains only one compatible scala library. hello-test Unknown Scala Classpath Problem&lt;/p&gt;

&lt;p&gt;修改工程中的scala编译版本
右击 –&amp;gt; Scala –&amp;gt; set the Scala Installation&lt;/p&gt;

&lt;p&gt;也可以&lt;/p&gt;

&lt;p&gt;右击工程–&amp;gt; Properties –&amp;gt; Scala Compiler –&amp;gt; Use project Setting 中选择spark对应的scala版本，此处选择Lastest2.10 bundle&lt;/p&gt;

&lt;p&gt;上述方法仍然没有解决
原因maven pom.xml 中的版本与eclipse里面设置的版本冲出
解决办法修改pom.xml&lt;/p&gt;
&lt;properties&gt;
    &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt;
    &lt;scala.version&gt;2.12.3&lt;/scala.version&gt;
    
 问题解决
 
 Unsupported major.minor version 52.0
 You get this error because a Java 7 VM tries to load a class compiled for Java 8

Java 8 has the class file version 52.0 but a Java 7 VM can only load class files up to version 51.0

In your case the Java 7 VM is your gradle build and the class is com.android.build.gradle.AppPlugin
简单来说，就是java的编译环境版本太低，java 8 class file的版本是52，Java 7虚拟机只能支持到51。所以需要升级到java 8 vm才行


mvn -V
Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T15:58:13+08:00)
Maven home: /Users/didi/maven
Java version: 1.8.0_144, vendor: Oracle Corporation

Missing artifact org.scalatest:scalatest_2.12:jar:  2.2.4

http://mvnrepository.com/artifact/org.scalatest/scalatest_2.12/3.0.3


  &lt;dependency&gt;
      &lt;groupId&gt;org.specs2&lt;/groupId&gt;
      &lt;artifactId&gt;specs2-core_${scala.compat.version}&lt;/artifactId&gt;
      &lt;version&gt;${scala.compat.version}&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.scalatest&lt;/groupId&gt;
      &lt;artifactId&gt;scalatest_${scala.compat.version}&lt;/artifactId&gt;
      &lt;version&gt;3.0.3&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    
vi /Users/didi/maven/conf/settings.xml

 在maven的默认配置中，对于jdk的配置是1.4版本，那么创建/导入maven工程过程中，工程中未指定jdk版本。

对工程进行maven的update，就会出现工程依赖的JRE System Library会自动变成JavaSE-1.4。



解决方案1：修改maven的默认jdk配置

           maven的conf\setting.xml文件中找到jdk配置的地方，修改如下：


[html] view plaincopy在CODE上查看代码片派生到我的代码片

&lt;profile&gt;   
    &lt;id&gt;jdk1.6&lt;/id&gt;    
    &lt;activation&gt;   
        &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;    
        &lt;jdk&gt;1.6&lt;/jdk&gt;   
    &lt;/activation&gt;    
    &lt;properties&gt;   
        &lt;maven.compiler.source&gt;1.6&lt;/maven.compiler.source&gt;    
        &lt;maven.compiler.target&gt;1.6&lt;/maven.compiler.target&gt;    
        &lt;maven.compiler.compilerVersion&gt;1.6&lt;/maven.compiler.compilerVersion&gt;   
    &lt;/properties&gt;   
&lt;/profile&gt;  

解决方案2：修改项目中pom.xml文件，这样避免在导入项目时的jdk版本指定

 打开项目中pom.xml文件，修改如下：
&lt;build&gt;  
    &lt;plugins&gt;  
        &lt;plugin&gt;  
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;  
            &lt;configuration&gt;  
                &lt;source /&gt;1.6&amp;lt;/source&amp;gt;  
                &lt;target&gt;1.6&lt;/target&gt;  
            &lt;/configuration&gt;  
        &lt;/plugin&gt;  
    &lt;/plugins&gt;  
&lt;/build&gt;  

右键－》propertity  
   remove jre1.6  
      add jre1.8
      
      
&lt;!-- more --&gt;
运行成功

Could not resolve dependencies for project maven.scala:mavenScala:jar:0.0.1-SNAPSHOT: Failure to find org.specs2:specs2-core_2.12:jar:2.12 in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced

http://maven.outofmemory.cn/org.specs2/specs2-core_2.12.0-M4/3.8.4/


&lt;dependency&gt;
    &lt;groupId&gt;org.specs2&lt;/groupId&gt;
    &lt;artifactId&gt;specs2-core_2.12.0-M4&lt;/artifactId&gt;
    &lt;version&gt;3.8.4&lt;/version&gt;
&lt;/dependency&gt;


删除
&lt;dependency&gt;
    &lt;groupId&gt;org.specs2&lt;/groupId&gt;
    &lt;artifactId&gt;specs2-core_2.12.0-M4&lt;/artifactId&gt;
    &lt;version&gt;3.8.4&lt;/version&gt;
&lt;/dependency&gt;


scalac error: bad option: '-make:transitive'
解决方法：

（1）打开pom.xml，删除

       &lt;parameter value=&quot;-make:transitive&quot; /&gt;
（2）添加dependance

        &lt;dependency&gt;
            &lt;groupId&gt;org.specs2&lt;/groupId&gt;
            &lt;artifactId&gt;specs2_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.6&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;


测试报错  删除

mvn package

[INFO] Building jar: /Users/didi/PhpstormProjects/ProjGit/Spark/ScalaMaven/MavenScala/target/MavenScala-0.0.1-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ----------------


Description Resource  Path  Location  Type
Project configuration is not up-to-date with pom.xml. Select: Maven-&amp;gt;Update Project... from the project context menu or use Quick Fix.  MavenScala    line 1  Maven Configuration Problem

右键  Maven-&amp;gt;Update Project

至此没有错误了




&lt;/properties&gt;
</description>
        <pubDate>Fri, 12 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/12/scala_version.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/12/scala_version.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Eclipse中操作Hive、HDFS、spark时的jar包列表</title>
        <description>&lt;!-- more --&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/hivejar.pnghdfsjar&quot;/&amp;gt;

	&amp;lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/hdfsjar.png&quot;/&amp;gt; 右击“SaprkScala”工程，选择“Properties”，在弹出的框中，按照下图所示，依次选择“Java Build Path” –&amp;gt;“Libraties” –&amp;gt;“Add External JARs…”，导入文章“Apache Spark：将Spark部署到Hadoop 2.2.0上”中给出的 assembly/target/scala-2.9.3/目录下的spark-assembly-0.8.1-incubating- hadoop2.2.0.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;高版本的在jars 里面&lt;/p&gt;

&lt;p&gt;只需要加载所有jars即可&lt;/p&gt;

&lt;p&gt;Description	Resource	Path	Location	Type
More than one scala library found in the build path (/Users/didi/.p2/pool/plugins/org.scala-lang.scala-library_2.12.3.v20170725-052526-VFINAL-6ac6da8.jar, /Users/didi/spark/spark/jars/scala-library-2.11.8.jar).At least one has an incompatible version. Please update the project build path so it contains only one compatible scala library.	online		Unknown	Scala Classpath Problem&lt;/p&gt;

&lt;p&gt;移除
jars/scala-library-2.11.8.jar
即可&lt;/p&gt;

</description>
        <pubDate>Fri, 12 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/12/hive.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/12/hive.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Eclipse+maven+scala+spark环境搭建</title>
        <description>&lt;p&gt;1.安装Scala-IDE
在Eclipse中开发Scala程序需要有scala插件，我们现在安装scala插件 
2.安装m2e-scala插件
m2e-scala用来支持scala开发中对maven的一些定制功能。通过eclipse的Install New Software安装。 
安装过程 
   1.Help-&amp;gt;Install New Software 
   2.输入m2e-scala下载的url 
具体URL为http://alchim31.free.fr/m2e-scala/update-site/
	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/m2e_scala.png&quot; /&gt;
    3.安装完成后，可在Help-&amp;gt;Installation Details中查看 
    4.添加远程的原型或模板目录
    	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/archetypes.png&quot; /&gt;
    	Catalog file:http://repo1.maven.org/maven2/archetype-catalog.xml
Description:Remote Catalog Scala
    5、出现过mvn连不上公共库的问题;
     解决方法：vi eclipse.ini
      add : -vmargs -Djava.net.preferIPv4Stack=true&lt;/p&gt;

&lt;p&gt;3.新建Eclipse+scala+maven工程
新建maven工程
此时的maven的Archetype需要设置为 org.scala-tools.archetypes 
如果没有安装Scala-IDE的话，会找不到org.scala-tools.archetypes这个类别&lt;/p&gt;

&lt;p&gt;新建Archetype，因为maven默认没有Group Id: net.alchim31.maven Artifact Id: scala-archetype-simple Version:1.6&lt;/p&gt;

&lt;p&gt;　　Select New -&amp;gt; Project -&amp;gt; Other and then select Maven Project. On the next window, search forscala-archetype. Make sure you select the one in group net.alchim31.maven, and click Next。
　　&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/configureScala.png&quot; /&gt;configure
　　
&lt;!-- more --&gt;&lt;/p&gt;
&lt;plugin&gt;
  &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
  &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
  &lt;version&gt;3.1.3&lt;/version&gt;
  &lt;executions&gt;
    &lt;execution&gt;
      &lt;goals&gt;
        &lt;goal&gt;compile&lt;/goal&gt;
        &lt;goal&gt;testCompile&lt;/goal&gt;
      &lt;/goals&gt;
    &lt;/execution&gt;
  &lt;/executions&gt;
  
  
  scala的新版本对老版本的兼容似乎并不好。这里可以自己修正pom.xml文件，不过估计代码可能也要修改。从git上下载了一个现成的基于scala2.11.5的maven工程。
git网址：https://github.com/scala/scala-module-dependency-sample
使用git clone下来之后，在eclipse中导入maven工程（maven-sample

或者直接编译scala-maven-plugin
https://github.com/davidB/scala-maven-plugin

 运行Maven是报错：No goals have been specified for this build
 pom.xml文件&lt;build&gt;标签后面加上&lt;defaultGoal&gt;compile&lt;/defaultGoal&gt;即可  
 
 一个错误示例，子项目引用了父项目，子项目parent标签处报错如下：
Multiple annotations found at this line:
- maven-enforcer-plugin (goal &quot;enforce&quot;) is ignored by m2e.
- Plugin execution not covered by lifecycle configuration: org.codehaus.mojo:aspectj-maven-plugin:1.3.1:compile (execution: 
 default, phase: compile)
 
解决办法
官网给出解释及解决办法：http://wiki.eclipse.org/M2E_plugin_execution_not_covered

这里有人说下面这样也可以解决， 即 &lt;plugins&gt; 标签外再套一个 &lt;pluginManagement&gt; 标签，我试验是成功的：
http://stackoverflow.com/questions/6352208/how-to-solve-plugin-execution-not-covered-by-lifecycle-configuration-for-sprin
&lt;build&gt;
    &lt;pluginManagement&gt;
        &lt;plugins&gt;
            &lt;plugin&gt; ... &lt;/plugin&gt;
            &lt;plugin&gt; ... &lt;/plugin&gt;
                  ....
        &lt;/plugins&gt;
    &lt;/pluginManagement&gt;
&lt;/build&gt;

 
 
 scala配置
很多时候我们希望可以使用java+scala混合开发模式，此时只需要在maven进行如下配置即可：

&lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
      &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
      &lt;version&gt;${scala.version}&lt;/version&gt;
      &lt;scope&gt;compile&lt;/scope&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;

&lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;
        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;
        &lt;version&gt;2.15.2&lt;/version&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;id&gt;scala-compile-first&lt;/id&gt;
            &lt;goals&gt;
              &lt;goal&gt;compile&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;includes&gt;
                &lt;include&gt;**/*.scala&lt;/include&gt;
              &lt;/includes&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
          &lt;execution&gt;
            &lt;id&gt;scala-test-compile&lt;/id&gt;
            &lt;goals&gt;
              &lt;goal&gt;testCompile&lt;/goal&gt;
            &lt;/goals&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;   
&lt;/build&gt; 

可运行jar打包
&lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;shade&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;transformers&gt;
                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;
                                    &lt;mainClass&gt;{此处填写main主类}&lt;/mainClass&gt;
                                &lt;/transformer&gt;
                            &lt;/transformers&gt;
                            &lt;filters&gt; 
                                &lt;filter&gt;
                                    &lt;artifact&gt;*:*&lt;/artifact&gt;
                                    &lt;excludes&gt;
                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;
                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;
                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;
                                    &lt;/excludes&gt;
                                &lt;/filter&gt;
                            &lt;/filters&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
            
            
     
&lt;/pluginManagement&gt;&lt;/plugins&gt;&lt;/build&gt;&lt;/plugin&gt;
</description>
        <pubDate>Thu, 11 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/11/maven_scala_eclipse.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/11/maven_scala_eclipse.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>maven</title>
        <description>&lt;p&gt;1.1 常用的mvn命令
mvn archetype:create 创建 Maven 项目
mvn compile 编译主程序源代码，不会编译test目录的源代码。第一次运行时，会下载相关的依赖包，可能会比较费时
mvn test-compile 编译测试代码，compile之后会生成target文件夹，主程序编译在classes下面，测试程序放在test-classes下
mvn test 运行应用程序中的单元测试 
mvn site 生成项目相关信息的网站
mvn clean 清除目标目录中的生成结果
mvn package 依据项目生成 jar 文件，打包之前会进行编译，测试
mvn install在本地 Repository 中安装 jar。
mvn eclipse:eclipse 生成 Eclipse 项目文件及包引用定义
mvn deploy 在整合或者发布环境下执行，将最终版本的包拷贝到远程 的repository，使得其他的开发者或者工程可以共享。
一些高级功能命令
跳过测试类 ： -Dmaven.test.skip=true
下载jar包源码： -DdownloadSource=true
下载javadocs： -DdownloadJavadocs=true
2.1 编写POM
就像Make的Makefile、Ant的build.xml一样，Maven项目的核心是pom.xml。POM(Project Object Model)，项目对象模型定义了项目的基本信息，用于描述项目如何构建，声明项目依赖，等等。现在先为HelloWorld项目编写一个最简单的pom. xml，
XML头&lt;/p&gt;

&lt;p&gt;代码的第一行是XML头，指定了该xml文档的版本和编码方式。&lt;/p&gt;

&lt;p&gt;project元素&lt;/p&gt;

&lt;p&gt;XML头之后紧接着就是project元素，project是所有pom.xml的根元素，它还声明了一些POM相关的命名空间及xsd元素，虽然这些属性不是必须的，但使用这些属性能够让第三方工具（如IDE中的XMl编辑器）帮助我们快速编辑POM 。&lt;/p&gt;

&lt;p&gt;modelVersion元素&lt;/p&gt;

&lt;p&gt;根元素下的第一个子元素modelVersion，它指定了当前POM模型的版本，对于Maven 3以及Maven 3来说，它只能是4.0.0。&lt;/p&gt;

&lt;p&gt;坐标&lt;/p&gt;

&lt;p&gt;这段代码中最重要的是：包含groupId、artifactId和version的三行。这三个元素定义了一个项目基本的坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。&lt;/p&gt;

&lt;p&gt;groupId元素&lt;/p&gt;

&lt;p&gt;groupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联。譬如在googlecode上建立了一个名为myapp的项目，那么groupId就应该是com.googlecod.myapp，如果你的公司是mycom，有一个项目为myapp。耶么groupId就应该是com.mycom.myapp。&lt;/p&gt;

&lt;p&gt;artifactId元素&lt;/p&gt;

&lt;p&gt;artifactId定义了当前Maven项目在组中唯一的ID，我们为这个HelloWord项目定义artifactId为hello-world。在前面的groupld为&lt;/p&gt;

&lt;p&gt;com.googlecode.myapp的例子中，你可能会为不同的子项目（模块）分配artifactId，如myapp-util、myapp-domain、myapp-web等。&lt;/p&gt;

&lt;p&gt;version元素&lt;/p&gt;

&lt;p&gt;顾名思义，version指定了Hello World项目当前的版本0.0.1。SNAPSHOT意为快照，说明该项目还处于开发中，是不稳定的版本。随着项目的展，version会不断更新，如升级为0.0.2、0.0.3、1.0.0等。&lt;/p&gt;

&lt;p&gt;name元素&lt;/p&gt;

&lt;p&gt;最后一个name元素，声明了一个对于用户更为友好的项目名称，虽然这不是必须的，但还是推荐为每个POM声明name。以方便信息交流。&lt;/p&gt;

&lt;p&gt;没有任何实际的Java代码，我们就能够定义一个Maven项目的POM，这体现了Maven的一大优点，它能让项目对象模型最大程度地与实际代码相独立，我们可以称之为解耦，或者正交性。这在很大程度上避免了Java代码和POM代码的相互影响：比如当项目需要升级版本时，只需要修改POM。而不需要更改Java代码；而在POM稳定之后，日常的Java代码开发工作基本不涉及POM的修改。&lt;/p&gt;

&lt;p&gt;2.2 编写主代码
项目主代码和测试代码不同，项目的主代码会被打包到最终的构件中如：jar。而测试代码只在运行测试时用到，不会被打包。默认情况下，Maven假设项目主代码位于src/main/java目录，我们遵循Maven的约定，创建该目录，然后在该目录下创建文件org/hebut/test/helloworld/HelloWorld. java&lt;/p&gt;

&lt;p&gt;有两点需要注意：首先，在绝大多数情况下，应该把项目主代码放到src/main/java/目录下，而无须额外的配置，Maven会自动搜寻该目录找到项目主代码。其次，该Java类的包名是org.hebut.test.helloworld，这与之前在POM中定义的groupId和artifactld相吻合。一般来说，项目中Java类的包都应该基于项目的groupld和anifactId。这样更加清晰，更加符合逻辑，也方便搜索构件或者Java类。&lt;/p&gt;

&lt;p&gt;使用的clean命令告诉Maven清理输出目录target/，compile告诉Maven编译项目主代码，从输出中看到Maven首先执行了clean：clean任务，删除target/目录。默认情况下，Maven构建的所有输出都在target/目录中；接着执行resources：resources任务，未定义项目资源，暂且略过；&lt;/p&gt;

&lt;p&gt;最后执行compiler：compile任务，将项目主代码编译至target/classes目录，编译好的类为：&lt;/p&gt;

&lt;p&gt;org/hebut/test/helloworld/HelloWorld.Class&lt;/p&gt;

&lt;p&gt;上文提到的clean:clean、resources:resources和compiler:compile对应了一些Maven插件及插件目标，比如clean:clean是clean插件的clean目标，compiler:compile是compiler插件的compile目。&lt;/p&gt;

&lt;p&gt;至此，Maven在没有任何额外的配置的情况下就执行了项目的清理和编译任务。接下来，编写一些单元测试代码并让Maven执行自动化测试。&lt;/p&gt;

&lt;p&gt;2.3 编写测试代码
为了使项目结构保持清晰，主代码与测试代码应陔分别位于独立的目录中。正如上面所述，Maven项目中默认的主代码目录是src/main/java。对应地，Maven项目中默认的测试代码目录是src/test/java&lt;/p&gt;

&lt;p&gt;dependencies元素&lt;/p&gt;

&lt;p&gt;代码中添加了dependencies元素，该元素下可以包含多个dependency元素以声明项目的依赖。&lt;/p&gt;

&lt;p&gt;dependency元素&lt;/p&gt;

&lt;p&gt;dependency元素用以声明项目的依赖，这里添加了一个依赖groupId是junit，artifactld是junit，version是4.7。前面提到groupId、artifactId和versIon是任何一个Maven项目最基本的坐标。JUnit也不例外，有了这段声明Maven就能够自动下载junit-4.7.jar。&lt;/p&gt;

&lt;p&gt;scope元素&lt;/p&gt;

&lt;p&gt;上述POM代码中还有一个值为test的元素scope，scope为依赖范围，若依赖范围为test则表示该依赖只对测试有效。换句话说，测试代码中的import JUnit代码是没有问题的，但是如果在主代码中用import Junit代码，就会造成编译错误。如果不声明依赖范围，那么默认值就是compile，表示该依赖对主代码和测试代码都有效。&lt;/p&gt;

&lt;p&gt;默认Maven生成的JAR包只包含了编译生成的.class文件和项目资源文件，而要得到一个可以直接在命令行通过java命令运行的JAR文件，还要满足两个条件&lt;/p&gt;

&lt;p&gt;■ JAR包中的/META-INF/MANIFEST.MF元数据文件必须包含Main-Class信息。&lt;/p&gt;

&lt;p&gt;■ 项目所有的依赖都必须在Classpath中。&lt;/p&gt;

&lt;p&gt;三、使用Archetype生成项目骨架
3.1 Maven 项目约定
HelloWorld项目中有一些Maven的约定：在项目的根目录中放置pom.xml，在src/main/java目录中放置项目的主代码，在src/test/java中放置项目的测试代码。我们称这些基本的目录结构和pom. xml文件内容称为项目的骨架&lt;/p&gt;

&lt;p&gt;3.2 Maven Archetype
当第一次创建项目骨架的时候，你还会饶有兴趣地去体会这些默认约定背后的思想，第二次，第三次，你也许还会满意自己的熟练程度，但第四、第五次做同样的事情，你可能就会恼火了。为此Maven提供了Archetype以帮助我们快速勾勒出项目骨架。还是以Hello World为例，我们使用maven archetype来创建该项目的骨架，离开当前的Maven项目目录。&lt;/p&gt;

&lt;p&gt;如果是Maven 3简单地运行：&lt;/p&gt;

&lt;p&gt;mvn archetype:generate&lt;/p&gt;

&lt;p&gt;如果是Maven 2最好运行如下命令：&lt;/p&gt;

&lt;p&gt;mvn org.apache.maven.plugins:maven-archetype-plugin:2.0-alpha-5:generate&lt;/p&gt;

&lt;p&gt;m2eclipse是Eclipse中的一款Maven插件&lt;/p&gt;

&lt;!-- more --&gt;
&lt;p&gt;spark-submit 错误： ava.lang.ClassNotFoundException: WordCount&lt;/p&gt;

&lt;p&gt;跟package name有关&lt;/p&gt;

&lt;p&gt;# ./spark-submit –class spark.wordcount.WordCount  /opt/spark-wordcount-in-scala.jar&lt;/p&gt;

&lt;p&gt;–class后接的格式应该是packageName.objectName。&lt;/p&gt;

</description>
        <pubDate>Thu, 11 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/11/maven.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/11/maven.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>随机森林</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;1 什么是随机森林？
　　作为新兴起的、高度灵活的一种机器学习算法，随机森林（Random Forest，简称RF）拥有广泛的应用前景，从市场营销到医疗保健保险，既可以用来做市场营销模拟的建模，统计客户来源，保留和流失，也可用来预测疾病的风险和病患者的易感性。最初，我是在参加校外竞赛时接触到随机森林算法的。最近几年的国内外大赛，包括2013年百度校园电影推荐系统大赛、2014年阿里巴巴天池大数据竞赛以及Kaggle数据科学竞赛，参赛者对随机森林的使用占有相当高的比例。此外，据我的个人了解来看，一大部分成功进入答辩的队伍也都选择了Random Forest 或者 GBDT 算法。所以可以看出，Random Forest在准确率方面还是相当有优势的。&lt;/p&gt;

&lt;p&gt;　　那说了这么多，那随机森林到底是怎样的一种算法呢？&lt;/p&gt;

&lt;p&gt;　　如果读者接触过决策树（Decision Tree）的话，那么会很容易理解什么是随机森林。随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。随机森林的名称中有两个关键词，一个是“随机”，一个就是“森林”。“森林”我们很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，这样的比喻还是很贴切的，其实这也是随机森林的主要思想–集成思想的体现。“随机”的含义我们会在下边部分讲到。&lt;/p&gt;

&lt;p&gt;　　其实从直观角度来解释，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的 Bagging 思想。&lt;/p&gt;

&lt;p&gt;回到顶部
2 随机森林的特点
　　我们前边提到，随机森林是一种很灵活实用的方法，它有如下几个特点：&lt;/p&gt;

&lt;p&gt;在当前所有算法中，具有极好的准确率/It is unexcelled in accuracy among current algorithms；
能够有效地运行在大数据集上/It runs efficiently on large data bases；
能够处理具有高维特征的输入样本，而且不需要降维/It can handle thousands of input variables without variable deletion；
能够评估各个特征在分类问题上的重要性/It gives estimates of what variables are important in the classification；
在生成过程中，能够获取到内部生成误差的一种无偏估计/It generates an internal unbiased estimate of the generalization error as the forest building progresses；
对于缺省值问题也能够获得很好得结果/It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing
… …
　　实际上，随机森林的特点不只有这六点，它就相当于机器学习领域的Leatherman（多面手），你几乎可以把任何东西扔进去，它基本上都是可供使用的。在估计推断映射方面特别好用，以致都不需要像SVM那样做很多参数的调试。具体的随机森林介绍可以参见随机森林主页：Random Forest。&lt;/p&gt;

&lt;p&gt;回到顶部
3 随机森林的相关基础知识
　　随机森林看起来是很好理解，但是要完全搞明白它的工作原理，需要很多机器学习方面相关的基础知识。在本文中，我们简单谈一下，而不逐一进行赘述，如果有同学不太了解相关的知识，可以参阅其他博友的一些相关博文或者文献。&lt;/p&gt;

&lt;p&gt;　　1）信息、熵以及信息增益的概念&lt;/p&gt;

&lt;p&gt;　　这三个基本概念是决策树的根本，是决策树利用特征来分类时，确定特征选取顺序的依据。理解了它们，决策树你也就了解了大概。&lt;/p&gt;

&lt;p&gt;　　引用香农的话来说，信息是用来消除随机不确定性的东西。当然这句话虽然经典，但是还是很难去搞明白这种东西到底是个什么样，可能在不同的地方来说，指的东西又不一样。对于机器学习中的决策树而言，如果带分类的事物集合可以划分为多个类别当中，则某个类（xi）的信息可以定义如下:&lt;/p&gt;

&lt;p&gt;　　I(x)用来表示随机变量的信息，p(xi)指是当xi发生时的概率。&lt;/p&gt;

&lt;p&gt;　　熵是用来度量不确定性的，当熵越大，X=xi的不确定性越大，反之越小。对于机器学习中的分类问题而言，熵越大即这个类别的不确定性更大，反之越小。&lt;/p&gt;

&lt;p&gt;　　信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好。&lt;/p&gt;

&lt;p&gt;　　这方面的内容不再细述，感兴趣的同学可以看 信息&amp;amp;熵&amp;amp;信息增益 这篇博文。&lt;/p&gt;

&lt;p&gt;　　2）决策树&lt;/p&gt;

&lt;p&gt;　　决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。常见的决策树算法有C4.5、ID3和CART。&lt;/p&gt;

&lt;p&gt;　　3）集成学习　&lt;/p&gt;

&lt;p&gt;　　集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。&lt;/p&gt;

&lt;p&gt;　　随机森林是集成学习的一个子类，它依靠于决策树的投票选择来决定最后的分类结果。你可以在这找到用python实现集成学习的文档：Scikit 学习文档。&lt;/p&gt;

&lt;p&gt;回到顶部
4 随机森林的生成
　　前面提到，随机森林中有许多的分类树。我们要将一个输入样本进行分类，我们需要将输入样本输入到每棵树中进行分类。打个形象的比喻：森林中召开会议，讨论某个动物到底是老鼠还是松鼠，每棵树都要独立地发表自己对这个问题的看法，也就是每棵树都要投票。该动物到底是老鼠还是松鼠，要依据投票情况来确定，获得票数最多的类别就是森林的分类结果。森林中的每棵树都是独立的，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想（关于bagging的一个有必要提及的问题：bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性。）。下图可以形象地描述这个情况：&lt;/p&gt;

&lt;p&gt;　&lt;/p&gt;

&lt;p&gt;　　有了树我们就可以分类了，但是森林中的每棵树是怎么生成的呢？&lt;/p&gt;

&lt;p&gt;　　每棵树的按照如下规则生成：&lt;/p&gt;

&lt;p&gt;　　1）如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；&lt;/p&gt;

&lt;p&gt;　　从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本（理解这点很重要）。&lt;/p&gt;

&lt;p&gt;　　为什么要随机抽样训练集？（add @2016.05.28）&lt;/p&gt;

&lt;p&gt;　　如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；&lt;/p&gt;

&lt;p&gt;　　为什么要有放回地抽样？（add @2016.05.28）&lt;/p&gt;

&lt;p&gt;　　我理解的是这样的：如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，都是绝对”片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是”盲人摸象”。&lt;/p&gt;

&lt;p&gt;　　2）如果每个样本的特征维度为M，指定一个常数m«M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；&lt;/p&gt;

&lt;p&gt;　　3）每棵树都尽最大程度的生长，并且没有剪枝过程。&lt;/p&gt;

&lt;p&gt;　　一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。&lt;/p&gt;

&lt;p&gt;　　随机森林分类效果（错误率）与两个因素有关：&lt;/p&gt;

&lt;p&gt;森林中任意两棵树的相关性：相关性越大，错误率越大；
森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。
　　减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。&lt;/p&gt;

&lt;p&gt;回到顶部
5 袋外错误率（oob error）
　　上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。&lt;/p&gt;

&lt;p&gt;　　随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。&lt;/p&gt;

&lt;p&gt;　　我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。&lt;/p&gt;

&lt;p&gt;　　而这样的采样特点就允许我们进行oob估计，它的计算方式如下：&lt;/p&gt;

&lt;p&gt;　　（note：以样本为单位）&lt;/p&gt;

&lt;p&gt;　　1）对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；&lt;/p&gt;

&lt;p&gt;　　2）然后以简单多数投票作为该样本的分类结果；&lt;/p&gt;

&lt;p&gt;　　3）最后用误分个数占样本总数的比率作为随机森林的oob误分率。&lt;/p&gt;

&lt;p&gt;　　（文献原文：Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests.）&lt;/p&gt;

&lt;p&gt;　　oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。&lt;/p&gt;

&lt;p&gt;回到顶部
6 随机森林工作原理解释的一个简单例子
　　描述：根据已有的训练集已经生成了对应的随机森林，随机森林如何利用某一个人的年龄（Age）、性别（Gender）、教育情况（Highest Educational Qualification）、工作领域（Industry）以及住宅地（Residence）共5个字段来预测他的收入层次。&lt;/p&gt;

&lt;p&gt;　　收入层次 :&lt;/p&gt;

&lt;p&gt;　　　　Band 1 : Below $40,000&lt;/p&gt;

&lt;p&gt;　　　　Band 2: $40,000 – 150,000&lt;/p&gt;

&lt;p&gt;　　　　Band 3: More than $150,000&lt;/p&gt;

&lt;p&gt;　　随机森林中每一棵树都可以看做是一棵CART（分类回归树），这里假设森林中有5棵CART树，总特征个数N=5，我们取m=1（这里假设每个CART树对应一个不同的特征）。&lt;/p&gt;

&lt;p&gt;　　CART 1 : Variable Age&lt;/p&gt;

&lt;p&gt;　　rf1&lt;/p&gt;

&lt;p&gt;　　CART 2 : Variable Gender&lt;/p&gt;

&lt;p&gt;　　rf2&lt;/p&gt;

&lt;p&gt;　　CART 3 : Variable Education&lt;/p&gt;

&lt;p&gt;　　rf3&lt;/p&gt;

&lt;p&gt;　　CART 4 : Variable Residence&lt;/p&gt;

&lt;p&gt;　　rf4&lt;/p&gt;

&lt;p&gt;　　CART 5 : Variable Industry&lt;/p&gt;

&lt;p&gt;　　rf5&lt;/p&gt;

&lt;p&gt;　　我们要预测的某个人的信息如下：&lt;/p&gt;

&lt;p&gt;　　1. Age : 35 years ; 2. Gender : Male ; 3. Highest Educational Qualification : Diploma holder; 4. Industry : Manufacturing; 5. Residence : Metro.&lt;/p&gt;

&lt;p&gt;　　根据这五棵CART树的分类结果，我们可以针对这个人的信息建立收入层次的分布情况：&lt;/p&gt;

&lt;p&gt;　　DF&lt;/p&gt;

&lt;p&gt;　　最后，我们得出结论，这个人的收入层次70%是一等，大约24%为二等，6%为三等，所以最终认定该人属于一等收入层次（小于$40,000）。&lt;/p&gt;

&lt;p&gt;回到顶部
7 随机森林的Python实现
　　利用Python的两个模块，分别为pandas和scikit-learn来实现随机森林。
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np&lt;/p&gt;

&lt;p&gt;iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df[‘is_train’] = np.random.uniform(0, 1, len(df)) &amp;lt;= .75
df[‘species’] = pd.Factor(iris.target, iris.target_names)
df.head()&lt;/p&gt;

&lt;p&gt;train, test = df[df[‘is_train’]==True], df[df[‘is_train’]==False]&lt;/p&gt;

&lt;p&gt;features = df.columns[:4]
clf = RandomForestClassifier(n_jobs=2)
y, _ = pd.factorize(train[‘species’])
clf.fit(train[features], y)&lt;/p&gt;

&lt;p&gt;preds = iris.target_names[clf.predict(test[features])]
pd.crosstab(test[‘species’], preds, rownames=[‘actual’], colnames=[‘preds’])&lt;/p&gt;

&lt;p&gt;import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA&lt;/p&gt;

&lt;p&gt;h = .02  # step size in the mesh&lt;/p&gt;

&lt;p&gt;names = [“Nearest Neighbors”, “Linear SVM”, “RBF SVM”, “Decision Tree”,
         “Random Forest”, “AdaBoost”, “Naive Bayes”, “LDA”, “QDA”]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=”linear”, C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]&lt;/p&gt;

&lt;p&gt;X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)&lt;/p&gt;

&lt;p&gt;datasets = [make_moons(noise=0.3, random_state=0),
            make_circles(noise=0.2, factor=0.5, random_state=1),
            linearly_separable
            ]&lt;/p&gt;

&lt;p&gt;figure = plt.figure(figsize=(27, 9))
i = 1
 # iterate over datasets
for ds in datasets:
    # preprocess dataset, split into training and test part
    X, y = ds
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# just plot the dataset first
cm = plt.cm.RdBu
cm_bright = ListedColormap(['#FF0000', '#0000FF'])
ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
# Plot the training points
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
# and testing points
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())
i += 1

# iterate over classifiers
for name, clf in zip(names, classifiers):
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    if hasattr(clf, &quot;decision_function&quot;):
        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    else:
        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)

    # Plot also the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
    # and testing points
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
               alpha=0.6)

    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(name)
    ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
            size=15, horizontalalignment='right')
    i += 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;figure.subplots_adjust(left=.02, right=.98)
plt.show()
　　这里随机生成了三个样本集，分割面近似为月形、圆形和线形的。我们可以重点对比一下决策树和随机森林对样本空间的分割：&lt;/p&gt;

&lt;p&gt;　　1）从准确率上可以看出，随机森林在这三个测试集上都要优于单棵决策树，90%&amp;gt;85%，82%&amp;gt;80%，95%=95%；&lt;/p&gt;

&lt;p&gt;　　2）从特征空间上直观地可以看出，随机森林比决策树拥有更强的分割能力（非线性拟合能力）。&lt;/p&gt;
</description>
        <pubDate>Tue, 09 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/09/random_foreast.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/09/random_foreast.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>svm</title>
        <description>&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/svm.png&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/08/svm.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/08/svm.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>virtualenv</title>
        <description>&lt;p&gt;在开发Python应用程序的时候，系统安装的Python3只有一个版本：3.4。所有第三方的包都会被pip安装到Python3的site-packages目录下。&lt;/p&gt;

&lt;p&gt;如果我们要同时开发多个应用程序，那这些应用程序都会共用一个Python，就是安装在系统的Python 3。如果应用A需要jinja 2.7，而应用B需要jinja 2.6怎么办？&lt;/p&gt;

&lt;p&gt;这种情况下，每个应用可能需要各自拥有一套“独立”的Python运行环境。virtualenv就是用来为一个应用创建一套“隔离”的Python运行环境。&lt;/p&gt;

&lt;p&gt;首先，我们用pip安装virtualenv：&lt;/p&gt;

&lt;p&gt;$ pip3 install virtualenv
然后，假定我们要开发一个新的项目，需要一套独立的Python运行环境，可以这么做：&lt;/p&gt;

&lt;p&gt;第一步，创建目录：&lt;/p&gt;

&lt;p&gt;Mac:~ michael$ mkdir myproject
Mac:~ michael$ cd myproject/
Mac:myproject michael$
第二步，创建一个独立的Python运行环境，命名为venv：&lt;/p&gt;

&lt;p&gt;Mac:myproject michael$ virtualenv –no-site-packages venv
Using base prefix ‘/usr/local/…/Python.framework/Versions/3.4’
New python executable in venv/bin/python3.4
Also creating executable in venv/bin/python
Installing setuptools, pip, wheel…done.
命令virtualenv就可以创建一个独立的Python运行环境，我们还加上了参数–no-site-packages，这样，已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境。&lt;/p&gt;

&lt;p&gt;新建的Python环境被放到当前目录下的venv目录。有了venv这个Python环境，可以用source进入该环境：&lt;/p&gt;

&lt;p&gt;Mac:myproject michael$ source venv/bin/activate
(venv)Mac:myproject michael$
注意到命令提示符变了，有个(venv)前缀，表示当前环境是一个名为venv的Python环境。&lt;/p&gt;

&lt;p&gt;下面正常安装各种第三方包，并运行python命令：&lt;/p&gt;

&lt;p&gt;(venv)Mac:myproject michael$ pip install jinja2
…
Successfully installed jinja2-2.7.3 markupsafe-0.23
(venv)Mac:myproject michael$ python myapp.py
…
在venv环境下，用pip安装的包都被安装到venv这个环境下，系统Python环境不受任何影响。也就是说，venv环境是专门针对myproject这个应用创建的。&lt;/p&gt;

&lt;p&gt;退出当前的venv环境，使用deactivate命令：&lt;/p&gt;

&lt;p&gt;(venv)Mac:myproject michael$ deactivate 
Mac:myproject michael$
此时就回到了正常的环境，现在pip或python均是在系统Python环境下执行。&lt;/p&gt;

&lt;p&gt;完全可以针对每个应用创建独立的Python运行环境，这样就可以对每个应用的Python环境进行隔离。&lt;/p&gt;

&lt;p&gt;virtualenv是如何创建“独立”的Python运行环境的呢？原理很简单，就是把系统Python复制一份到virtualenv的环境，用命令source venv/bin/activate进入一个virtualenv环境时，virtualenv会修改相关环境变量，让命令python和pip均指向当前的virtualenv环境。
&lt;!-- more --&gt;&lt;/p&gt;

&lt;p&gt;如果在命令行中运行virtualenv –system-site-packages ENV, 会继承/usr/lib/python2.7/site-packages下的所有库, 最新版本virtualenv把把访问全局site-packages作为默认行为
default behavior.&lt;/p&gt;

&lt;p&gt;2.1. 激活virtualenv&lt;/p&gt;

&lt;p&gt;#ENV目录下使用如下命令
➜  ENV git:(master) ✗ source ./bin/activate  #激活当前virtualenv
(ENV)➜  ENV git:(master) ✗ #注意终端发生了变化
#ENV目录下使用如下命令
➜  ENV git:(master) ✗ source ./bin/activate  #激活当前virtualenv
(ENV)➜  ENV git:(master) ✗ #注意终端发生了变化&lt;/p&gt;

&lt;p&gt;#使用pip查看当前库
(ENV)➜  ENV git:(master) ✗ pip list
pip (1.5.6)
setuptools (3.6)
wsgiref (0.1.2) #发现在只有这三个
pip freeze  #显示所有依赖
pip freeze &amp;gt; requirement.txt  #生成requirement.txt文件
pip install -r requirement.txt  #根据requirement.txt生成相同的环境
#使用pip查看当前库
(ENV)➜  ENV git:(master) ✗ pip list
pip (1.5.6)
setuptools (3.6)
wsgiref (0.1.2) #发现在只有这三个
pip freeze  #显示所有依赖
pip freeze &amp;gt; requirement.txt  #生成requirement.txt文件
pip install -r requirement.txt  #根据requirement.txt生成相同的环境
2.2. 关闭virtualenv
使用下面命令
$ deactivate
$ deactivate
2.3. 指定python版本
可以使用-p PYTHON_EXE选项在创建虚拟环境的时候指定python版本&lt;/p&gt;

&lt;p&gt;#创建python2.7虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/bin/python2.7 ENV2.7
Running virtualenv with interpreter /usr/bin/python2.7
New python executable in ENV2.7/bin/python
Installing setuptools, pip…done.
#创建python2.7虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/bin/python2.7 ENV2.7
Running virtualenv with interpreter /usr/bin/python2.7
New python executable in ENV2.7/bin/python
Installing setuptools, pip…done.&lt;/p&gt;

&lt;p&gt;#创建python3.4虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/local/bin/python3.4 ENV3.4
Running virtualenv with interpreter /usr/local/bin/python3.4
Using base prefix ‘/Library/Frameworks/Python.framework/Versions/3.4’
New python executable in ENV3.4/bin/python3.4
Also creating executable in ENV3.4/bin/python
Installing setuptools, pip…done.
#创建python3.4虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/local/bin/python3.4 ENV3.4
Running virtualenv with interpreter /usr/local/bin/python3.4
Using base prefix ‘/Library/Frameworks/Python.framework/Versions/3.4’
New python executable in ENV3.4/bin/python3.4
Also creating executable in ENV3.4/bin/python
Installing setuptools, pip…done.
到此已经可以解决python版本冲突问题和python库不同版本的问题&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;其他
3.1. 生成可打包环境
某些特殊需求下,可能没有网络, 我们期望直接打包一个ENV, 可以解压后直接使用, 这时候可以使用virtualenv -relocatable指令将ENV修改为可更改位置的ENV&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;#对当前已经创建的虚拟环境更改为可迁移
➜  ENV3.4 git:(master) ✗ virtualenv –relocatable ./
Making script ./bin/easy_install relative
Making script ./bin/easy_install-3.4 relative
Making script ./bin/pip relative
Making script ./bin/pip3 relative
Making script ./bin/pip3.4 relative
#对当前已经创建的虚拟环境更改为可迁移
➜  ENV3.4 git:(master) ✗ virtualenv –relocatable ./
Making script ./bin/easy_install relative
Making script ./bin/easy_install-3.4 relative
Making script ./bin/pip relative
Making script ./bin/pip3 relative
Making script ./bin/pip3.4 relative
3.2. 获得帮助&lt;/p&gt;

&lt;p&gt;$ virtualenv -h
$ virtualenv -h
当前的ENV都被修改为相对路径, 可以打包当前目录, 上传到其他位置使用&lt;/p&gt;

&lt;p&gt;python模块以及导入出现ImportError: No module named ‘xxx’问题&lt;/p&gt;

&lt;p&gt;python中，每个py文件被称之为模块，每个具有__init__.py文件的目录被称为包。只要模
块或者包所在的目录在sys.path中，就可以使用import 模块或import 包来使用
如果你要使用的模块（py文件）和当前模块在同一目录，只要import相应的文件名就好，比
如在a.py中使用b.py： 
import b&lt;/p&gt;

&lt;p&gt;但是如果要import一个不同目录的文件(例如b.py)该怎么做呢？ 
首先需要使用sys.path.append方法将b.py所在目录加入到搜素目录中。然后进行import即
可，例如 
import sys 
sys.path.append(‘c:\xxxx\b.py’) # 这个例子针对 windows 用户来说的 
大多数情况，上面的代码工作的很好。但是如果你没有发现上面代码有什么问题的话，可要&lt;/p&gt;

&lt;p&gt;注意了，上面的代码有时会找不到模块或者包（ImportError: No module named 
xxxxxx），这是因为： 
sys模块是使用c语言编写的，因此字符串支持 ‘\n’, ‘\r’, ‘\t’等来表示特殊字符。所以&lt;/p&gt;

&lt;p&gt;上面代码最好写成： 
sys.path.append(‘c:\xxx\b.py’) 
或者sys.path.append(‘c:/xxxx/b.py’) 
这样可以避免因为错误的组成转义字符，而造成无效的搜索目录（sys.path）设置。&lt;/p&gt;

&lt;p&gt;sys.path是python的搜索模块的路径集，是一个list
可以在python 环境下使用sys.path.append(path)添加相关的路径，但在退出python环境后
自己添加的路径就会自动消失了！&lt;/p&gt;

&lt;p&gt;3、搜索路径和路径搜索&lt;/p&gt;

&lt;p&gt;模块的导入需要叫做“路径搜索”的过程。&lt;/p&gt;

&lt;p&gt;搜索路径：查找一组目录&lt;/p&gt;

&lt;p&gt;路径搜索：查找某个文件的操作&lt;/p&gt;

&lt;p&gt;ImportError: No module named myModule
这种错误就是说：模块不在搜索路径里，从而导致路径搜索失败！&lt;/p&gt;

&lt;p&gt;导入模块时，不带模块的后缀名，比如.py
Python搜索模块的路径：
1)、程序的主目录
2)、PTYHONPATH目录（如果已经进行了设置）
3)、标准连接库目录（一般在/usr/local/lib/python2.X/）
4)、任何的.pth文件的内容（如果存在的话）.新功能，允许用户把有效果的目录添加到模块搜索路径中去
.pth后缀的文本文件中一行一行的地列出目录。
这四个组建组合起来就变成了sys.path了，&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;blockquote&gt;
      &lt;p&gt;import sys
sys.path
导入时，Python会自动由左到右搜索这个列表中每个目录。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;关于 python ImportError: No module named ‘xxx’的问题?
解决方法如下：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;使用PYTHONPATH环境变量，在这个环境变量中输入相关的路径，不同的路径之间用逗号
（英文的！)分开，如果PYTHONPATH 变量还不存在，可以创建它！
这里的路径会自动加入到sys.path中，永久存在于sys.path中而且可以在不同的python版本
中共享，应该是一样较为方便的方法。
C:\Users\Administrator\Desktop\test\module1.py:
def func1():
 print(“func1”)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将C:\Users\Administrator\Desktop\test添加到PYTHONPATH即可直接import module1,然后
调用：module1.func1()即可。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;将自己做的py文件放到 site_packages 目录下&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用pth文件，在 site-packages 文件中创建 .pth文件，将模块的路径写进去，一行一
个路径，以下是一个示例，pth文件也可以使用注释：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;pth-file-for-the--my-project这行是注释命名为xxxpth文件&quot;&gt;.pth file for the  my project(这行是注释)，命名为xxx.pth文件&lt;/h1&gt;
&lt;p&gt;C:\Users\Administrator\Desktop\test
这个不失为一个好的方法，但存在管理上的问题，而且不能在不同的python版本中共享。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;在调用文件中添加sys.path.append(“模块文件目录”)；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;直接把模块文件拷贝到$python_dir/Lib目录下。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;通过以上5个方法就可以直接使用import module_name了。&lt;/p&gt;

</description>
        <pubDate>Sun, 07 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/07/virtualenv.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/07/virtualenv.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>pip</title>
        <description>&lt;p&gt;1、pip下载安装
1.1 pip下载
 # wget “https://pypi.python.org/packages/source/p/pip/pip-1.5.4.tar.gz#md5=834b2904f92d46aaa333267fb1c922bb” –no-check-certificate
 # wget “https://pypi.python.org/packages/source/p/pip/pip-1.5.4.tar.gz#md5=834b2904f92d46aaa333267fb1c922bb” –no-check-certificate
1.2 pip安装
 # tar -xzvf pip-1.5.4.tar.gz
 # cd pip-1.5.4
 # python setup.py install
 # tar -xzvf pip-1.5.4.tar.gz
 # cd pip-1.5.4
 # python setup.py install&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;pip使用详解
2.1 pip安装包
 # pip install SomePackage
  […]
  Successfully installed SomePackage
 # pip install SomePackage
  […]
  Successfully installed SomePackage
2.2 pip查看已安装的包
 # pip show –files SomePackage
  Name: SomePackage
  Version: 1.0
  Location: /my/env/lib/pythonx.x/site-packages
  Files:
../somepackage/&lt;strong&gt;init&lt;/strong&gt;.py
[…]
  # pip show –files SomePackage
  Name: SomePackage
  Version: 1.0
  Location: /my/env/lib/pythonx.x/site-packages
  Files:
../somepackage/&lt;strong&gt;init&lt;/strong&gt;.py
[…]
2.3 pip检查哪些包需要更新
 # pip list –outdated
  SomePackage (Current: 1.0 Latest: 2.0)
 # pip list –outdated
  SomePackage (Current: 1.0 Latest: 2.0)
2.4 pip升级包
 # pip install –upgrade SomePackage
  […]
  Found existing installation: SomePackage 1.0
  Uninstalling SomePackage:
 Successfully uninstalled SomePackage
  Running setup.py install for SomePackage
  Successfully installed SomePackage
2.5 pip卸载包
$ pip uninstall SomePackage
  Uninstalling SomePackage:
 /my/env/lib/pythonx.x/site-packages/somepackage
  Proceed (y/n)? y
  Successfully uninstalled SomePackage&lt;/li&gt;
  &lt;li&gt;pip参数解释
 # pip –help
Usage: &lt;br /&gt;
  pip &lt;command /&gt; [options]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Commands:
  install                     安装包.
  uninstall                   卸载包.
  freeze                      按着一定格式输出已安装包列表
  list                        列出已安装包.
  show                        显示包详细信息.
  search                      搜索包，类似yum里的search.
  wheel                       Build wheels from your requirements.
  zip                         不推荐. Zip individual packages.
  unzip                       不推荐. Unzip individual packages.
  bundle                      不推荐. Create pybundles.
  help                        当前帮助.&lt;/p&gt;

&lt;p&gt;General Options:
  -h, –help                  显示帮助.
  -v, –verbose               更多的输出，最多可以使用3次
  -V, –version               现实版本信息然后退出.
  -q, –quiet                 最少的输出.
  –log-file &lt;path&gt;           覆盖的方式记录verbose错误日志，默认文件：/root/.pip/pip.log
  --log &lt;path&gt;                不覆盖记录verbose输出的日志.
  --proxy &lt;proxy&gt;             Specify a proxy in the form [user:passwd@]proxy.server:port.
  --timeout &lt;sec&gt;             连接超时时间 (默认15秒).
  --exists-action &lt;action&gt;    Default action when a path already exists: (s)witch, (i)gnore, (w)ipe, (b)ackup.
  --cert &lt;path&gt;               证书.&lt;/path&gt;&lt;/action&gt;&lt;/sec&gt;&lt;/proxy&gt;&lt;/path&gt;&lt;/path&gt;&lt;/p&gt;

&lt;p&gt;#install pip3 for python 3.x
 pip3 install –upgrade pip
 2 Collecting pip
 3   Downloading pip-9.0.1-py2.py3-none-any.whl (1.3MB)
 4     100% |████████████████████████████████| 1.3MB 3.2kB/s 
 5 Installing collected packages: pip
 6   Found existing installation: pip 8.1.1
 7     Uninstalling pip-8.1.1:
 8       Successfully uninstalled pip-8.1.1
 9 Successfully installed pip-9.0.1&lt;/p&gt;

</description>
        <pubDate>Sun, 07 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/07/pip.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/07/pip.html</guid>
        
        
        <category>web</category>
        
      </item>
    
  </channel>
</rss>
