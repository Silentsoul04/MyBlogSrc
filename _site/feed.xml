<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 07 Jan 2018 14:17:18 +0800</pubDate>
    <lastBuildDate>Sun, 07 Jan 2018 14:17:18 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>virtualenv</title>
        <description>&lt;p&gt;在开发Python应用程序的时候，系统安装的Python3只有一个版本：3.4。所有第三方的包都会被pip安装到Python3的site-packages目录下。&lt;/p&gt;

&lt;p&gt;如果我们要同时开发多个应用程序，那这些应用程序都会共用一个Python，就是安装在系统的Python 3。如果应用A需要jinja 2.7，而应用B需要jinja 2.6怎么办？&lt;/p&gt;

&lt;p&gt;这种情况下，每个应用可能需要各自拥有一套“独立”的Python运行环境。virtualenv就是用来为一个应用创建一套“隔离”的Python运行环境。&lt;/p&gt;

&lt;p&gt;首先，我们用pip安装virtualenv：&lt;/p&gt;

&lt;p&gt;$ pip3 install virtualenv
然后，假定我们要开发一个新的项目，需要一套独立的Python运行环境，可以这么做：&lt;/p&gt;

&lt;p&gt;第一步，创建目录：&lt;/p&gt;

&lt;p&gt;Mac:~ michael$ mkdir myproject
Mac:~ michael$ cd myproject/
Mac:myproject michael$
第二步，创建一个独立的Python运行环境，命名为venv：&lt;/p&gt;

&lt;p&gt;Mac:myproject michael$ virtualenv –no-site-packages venv
Using base prefix ‘/usr/local/…/Python.framework/Versions/3.4’
New python executable in venv/bin/python3.4
Also creating executable in venv/bin/python
Installing setuptools, pip, wheel…done.
命令virtualenv就可以创建一个独立的Python运行环境，我们还加上了参数–no-site-packages，这样，已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境。&lt;/p&gt;

&lt;p&gt;新建的Python环境被放到当前目录下的venv目录。有了venv这个Python环境，可以用source进入该环境：&lt;/p&gt;

&lt;p&gt;Mac:myproject michael$ source venv/bin/activate
(venv)Mac:myproject michael$
注意到命令提示符变了，有个(venv)前缀，表示当前环境是一个名为venv的Python环境。&lt;/p&gt;

&lt;p&gt;下面正常安装各种第三方包，并运行python命令：&lt;/p&gt;

&lt;p&gt;(venv)Mac:myproject michael$ pip install jinja2
…
Successfully installed jinja2-2.7.3 markupsafe-0.23
(venv)Mac:myproject michael$ python myapp.py
…
在venv环境下，用pip安装的包都被安装到venv这个环境下，系统Python环境不受任何影响。也就是说，venv环境是专门针对myproject这个应用创建的。&lt;/p&gt;

&lt;p&gt;退出当前的venv环境，使用deactivate命令：&lt;/p&gt;

&lt;p&gt;(venv)Mac:myproject michael$ deactivate 
Mac:myproject michael$
此时就回到了正常的环境，现在pip或python均是在系统Python环境下执行。&lt;/p&gt;

&lt;p&gt;完全可以针对每个应用创建独立的Python运行环境，这样就可以对每个应用的Python环境进行隔离。&lt;/p&gt;

&lt;p&gt;virtualenv是如何创建“独立”的Python运行环境的呢？原理很简单，就是把系统Python复制一份到virtualenv的环境，用命令source venv/bin/activate进入一个virtualenv环境时，virtualenv会修改相关环境变量，让命令python和pip均指向当前的virtualenv环境。
&lt;!-- more --&gt;&lt;/p&gt;

&lt;p&gt;如果在命令行中运行virtualenv –system-site-packages ENV, 会继承/usr/lib/python2.7/site-packages下的所有库, 最新版本virtualenv把把访问全局site-packages作为默认行为
default behavior.&lt;/p&gt;

&lt;p&gt;2.1. 激活virtualenv&lt;/p&gt;

&lt;p&gt;#ENV目录下使用如下命令
➜  ENV git:(master) ✗ source ./bin/activate  #激活当前virtualenv
(ENV)➜  ENV git:(master) ✗ #注意终端发生了变化
#ENV目录下使用如下命令
➜  ENV git:(master) ✗ source ./bin/activate  #激活当前virtualenv
(ENV)➜  ENV git:(master) ✗ #注意终端发生了变化&lt;/p&gt;

&lt;p&gt;#使用pip查看当前库
(ENV)➜  ENV git:(master) ✗ pip list
pip (1.5.6)
setuptools (3.6)
wsgiref (0.1.2) #发现在只有这三个
pip freeze  #显示所有依赖
pip freeze &amp;gt; requirement.txt  #生成requirement.txt文件
pip install -r requirement.txt  #根据requirement.txt生成相同的环境
#使用pip查看当前库
(ENV)➜  ENV git:(master) ✗ pip list
pip (1.5.6)
setuptools (3.6)
wsgiref (0.1.2) #发现在只有这三个
pip freeze  #显示所有依赖
pip freeze &amp;gt; requirement.txt  #生成requirement.txt文件
pip install -r requirement.txt  #根据requirement.txt生成相同的环境
2.2. 关闭virtualenv
使用下面命令
$ deactivate
$ deactivate
2.3. 指定python版本
可以使用-p PYTHON_EXE选项在创建虚拟环境的时候指定python版本&lt;/p&gt;

&lt;p&gt;#创建python2.7虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/bin/python2.7 ENV2.7
Running virtualenv with interpreter /usr/bin/python2.7
New python executable in ENV2.7/bin/python
Installing setuptools, pip…done.
#创建python2.7虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/bin/python2.7 ENV2.7
Running virtualenv with interpreter /usr/bin/python2.7
New python executable in ENV2.7/bin/python
Installing setuptools, pip…done.&lt;/p&gt;

&lt;p&gt;#创建python3.4虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/local/bin/python3.4 ENV3.4
Running virtualenv with interpreter /usr/local/bin/python3.4
Using base prefix ‘/Library/Frameworks/Python.framework/Versions/3.4’
New python executable in ENV3.4/bin/python3.4
Also creating executable in ENV3.4/bin/python
Installing setuptools, pip…done.
#创建python3.4虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/local/bin/python3.4 ENV3.4
Running virtualenv with interpreter /usr/local/bin/python3.4
Using base prefix ‘/Library/Frameworks/Python.framework/Versions/3.4’
New python executable in ENV3.4/bin/python3.4
Also creating executable in ENV3.4/bin/python
Installing setuptools, pip…done.
到此已经可以解决python版本冲突问题和python库不同版本的问题&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;其他
3.1. 生成可打包环境
某些特殊需求下,可能没有网络, 我们期望直接打包一个ENV, 可以解压后直接使用, 这时候可以使用virtualenv -relocatable指令将ENV修改为可更改位置的ENV&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;#对当前已经创建的虚拟环境更改为可迁移
➜  ENV3.4 git:(master) ✗ virtualenv –relocatable ./
Making script ./bin/easy_install relative
Making script ./bin/easy_install-3.4 relative
Making script ./bin/pip relative
Making script ./bin/pip3 relative
Making script ./bin/pip3.4 relative
#对当前已经创建的虚拟环境更改为可迁移
➜  ENV3.4 git:(master) ✗ virtualenv –relocatable ./
Making script ./bin/easy_install relative
Making script ./bin/easy_install-3.4 relative
Making script ./bin/pip relative
Making script ./bin/pip3 relative
Making script ./bin/pip3.4 relative
3.2. 获得帮助&lt;/p&gt;

&lt;p&gt;$ virtualenv -h
$ virtualenv -h
当前的ENV都被修改为相对路径, 可以打包当前目录, 上传到其他位置使用&lt;/p&gt;

&lt;p&gt;python模块以及导入出现ImportError: No module named ‘xxx’问题&lt;/p&gt;

&lt;p&gt;python中，每个py文件被称之为模块，每个具有__init__.py文件的目录被称为包。只要模
块或者包所在的目录在sys.path中，就可以使用import 模块或import 包来使用
如果你要使用的模块（py文件）和当前模块在同一目录，只要import相应的文件名就好，比
如在a.py中使用b.py： 
import b&lt;/p&gt;

&lt;p&gt;但是如果要import一个不同目录的文件(例如b.py)该怎么做呢？ 
首先需要使用sys.path.append方法将b.py所在目录加入到搜素目录中。然后进行import即
可，例如 
import sys 
sys.path.append(‘c:\xxxx\b.py’) # 这个例子针对 windows 用户来说的 
大多数情况，上面的代码工作的很好。但是如果你没有发现上面代码有什么问题的话，可要&lt;/p&gt;

&lt;p&gt;注意了，上面的代码有时会找不到模块或者包（ImportError: No module named 
xxxxxx），这是因为： 
sys模块是使用c语言编写的，因此字符串支持 ‘\n’, ‘\r’, ‘\t’等来表示特殊字符。所以&lt;/p&gt;

&lt;p&gt;上面代码最好写成： 
sys.path.append(‘c:\xxx\b.py’) 
或者sys.path.append(‘c:/xxxx/b.py’) 
这样可以避免因为错误的组成转义字符，而造成无效的搜索目录（sys.path）设置。&lt;/p&gt;

&lt;p&gt;sys.path是python的搜索模块的路径集，是一个list
可以在python 环境下使用sys.path.append(path)添加相关的路径，但在退出python环境后
自己添加的路径就会自动消失了！&lt;/p&gt;

&lt;p&gt;3、搜索路径和路径搜索&lt;/p&gt;

&lt;p&gt;模块的导入需要叫做“路径搜索”的过程。&lt;/p&gt;

&lt;p&gt;搜索路径：查找一组目录&lt;/p&gt;

&lt;p&gt;路径搜索：查找某个文件的操作&lt;/p&gt;

&lt;p&gt;ImportError: No module named myModule
这种错误就是说：模块不在搜索路径里，从而导致路径搜索失败！&lt;/p&gt;

&lt;p&gt;导入模块时，不带模块的后缀名，比如.py
Python搜索模块的路径：
1)、程序的主目录
2)、PTYHONPATH目录（如果已经进行了设置）
3)、标准连接库目录（一般在/usr/local/lib/python2.X/）
4)、任何的.pth文件的内容（如果存在的话）.新功能，允许用户把有效果的目录添加到模块搜索路径中去
.pth后缀的文本文件中一行一行的地列出目录。
这四个组建组合起来就变成了sys.path了，&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;blockquote&gt;
      &lt;p&gt;import sys
sys.path
导入时，Python会自动由左到右搜索这个列表中每个目录。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;关于 python ImportError: No module named ‘xxx’的问题?
解决方法如下：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;使用PYTHONPATH环境变量，在这个环境变量中输入相关的路径，不同的路径之间用逗号
（英文的！)分开，如果PYTHONPATH 变量还不存在，可以创建它！
这里的路径会自动加入到sys.path中，永久存在于sys.path中而且可以在不同的python版本
中共享，应该是一样较为方便的方法。
C:\Users\Administrator\Desktop\test\module1.py:
def func1():
 print(“func1”)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将C:\Users\Administrator\Desktop\test添加到PYTHONPATH即可直接import module1,然后
调用：module1.func1()即可。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;将自己做的py文件放到 site_packages 目录下&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用pth文件，在 site-packages 文件中创建 .pth文件，将模块的路径写进去，一行一
个路径，以下是一个示例，pth文件也可以使用注释：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;pth-file-for-the--my-project这行是注释命名为xxxpth文件&quot;&gt;.pth file for the  my project(这行是注释)，命名为xxx.pth文件&lt;/h1&gt;
&lt;p&gt;C:\Users\Administrator\Desktop\test
这个不失为一个好的方法，但存在管理上的问题，而且不能在不同的python版本中共享。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;在调用文件中添加sys.path.append(“模块文件目录”)；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;直接把模块文件拷贝到$python_dir/Lib目录下。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;通过以上5个方法就可以直接使用import module_name了。&lt;/p&gt;

</description>
        <pubDate>Sun, 07 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/07/virtualenv.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/07/virtualenv.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>pip</title>
        <description>&lt;p&gt;1、pip下载安装
1.1 pip下载
 # wget “https://pypi.python.org/packages/source/p/pip/pip-1.5.4.tar.gz#md5=834b2904f92d46aaa333267fb1c922bb” –no-check-certificate
 # wget “https://pypi.python.org/packages/source/p/pip/pip-1.5.4.tar.gz#md5=834b2904f92d46aaa333267fb1c922bb” –no-check-certificate
1.2 pip安装
 # tar -xzvf pip-1.5.4.tar.gz
 # cd pip-1.5.4
 # python setup.py install
 # tar -xzvf pip-1.5.4.tar.gz
 # cd pip-1.5.4
 # python setup.py install&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;pip使用详解
2.1 pip安装包
 # pip install SomePackage
  […]
  Successfully installed SomePackage
 # pip install SomePackage
  […]
  Successfully installed SomePackage
2.2 pip查看已安装的包
 # pip show –files SomePackage
  Name: SomePackage
  Version: 1.0
  Location: /my/env/lib/pythonx.x/site-packages
  Files:
../somepackage/&lt;strong&gt;init&lt;/strong&gt;.py
[…]
  # pip show –files SomePackage
  Name: SomePackage
  Version: 1.0
  Location: /my/env/lib/pythonx.x/site-packages
  Files:
../somepackage/&lt;strong&gt;init&lt;/strong&gt;.py
[…]
2.3 pip检查哪些包需要更新
 # pip list –outdated
  SomePackage (Current: 1.0 Latest: 2.0)
 # pip list –outdated
  SomePackage (Current: 1.0 Latest: 2.0)
2.4 pip升级包
 # pip install –upgrade SomePackage
  […]
  Found existing installation: SomePackage 1.0
  Uninstalling SomePackage:
 Successfully uninstalled SomePackage
  Running setup.py install for SomePackage
  Successfully installed SomePackage
2.5 pip卸载包
$ pip uninstall SomePackage
  Uninstalling SomePackage:
 /my/env/lib/pythonx.x/site-packages/somepackage
  Proceed (y/n)? y
  Successfully uninstalled SomePackage&lt;/li&gt;
  &lt;li&gt;pip参数解释
 # pip –help
Usage: &lt;br /&gt;
  pip &lt;command /&gt; [options]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Commands:
  install                     安装包.
  uninstall                   卸载包.
  freeze                      按着一定格式输出已安装包列表
  list                        列出已安装包.
  show                        显示包详细信息.
  search                      搜索包，类似yum里的search.
  wheel                       Build wheels from your requirements.
  zip                         不推荐. Zip individual packages.
  unzip                       不推荐. Unzip individual packages.
  bundle                      不推荐. Create pybundles.
  help                        当前帮助.&lt;/p&gt;

&lt;p&gt;General Options:
  -h, –help                  显示帮助.
  -v, –verbose               更多的输出，最多可以使用3次
  -V, –version               现实版本信息然后退出.
  -q, –quiet                 最少的输出.
  –log-file &lt;path&gt;           覆盖的方式记录verbose错误日志，默认文件：/root/.pip/pip.log
  --log &lt;path&gt;                不覆盖记录verbose输出的日志.
  --proxy &lt;proxy&gt;             Specify a proxy in the form [user:passwd@]proxy.server:port.
  --timeout &lt;sec&gt;             连接超时时间 (默认15秒).
  --exists-action &lt;action&gt;    Default action when a path already exists: (s)witch, (i)gnore, (w)ipe, (b)ackup.
  --cert &lt;path&gt;               证书.&lt;/path&gt;&lt;/action&gt;&lt;/sec&gt;&lt;/proxy&gt;&lt;/path&gt;&lt;/path&gt;&lt;/p&gt;

&lt;p&gt;#install pip3 for python 3.x
 pip3 install –upgrade pip
 2 Collecting pip
 3   Downloading pip-9.0.1-py2.py3-none-any.whl (1.3MB)
 4     100% |████████████████████████████████| 1.3MB 3.2kB/s 
 5 Installing collected packages: pip
 6   Found existing installation: pip 8.1.1
 7     Uninstalling pip-8.1.1:
 8       Successfully uninstalled pip-8.1.1
 9 Successfully installed pip-9.0.1&lt;/p&gt;

</description>
        <pubDate>Sun, 07 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/07/pip.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/07/pip.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>信息熵</title>
        <description>&lt;p&gt;熵定义如下：
变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大。香农不是用钱，而是用 “比特”（bit）这个概念来度量信息量。 信息量的比特数和所有可能情况的对数函数 log 有关。（二进制位数）常用的汉字（一级二级国标）大约有 7000 字。假如每个字等概率，那么我们大约需要 13 个比特（即 13 位二进制数）表示一个汉字。&lt;/p&gt;

&lt;p&gt;信息量可以表示为：
-log2(pi)
即信息占的bit数
信息熵其实是一个随机变量信息量的数学期望。
H(X)=−∑P(x)logP(x))&lt;/p&gt;

&lt;p&gt;信息熵（又叫香农熵 Shannon entropy）反映了一个系统的无序化（有序化）程度，一个系统越有序，信息熵就越低，反之就越高&lt;/p&gt;

&lt;p&gt;相对熵（relative entropy）
所谓相对，自然在两个随机变量之间。又称互熵，Kullback–Leibler divergence（K-L 散度）等。设 p(x)和 q(x) 是 X取值的两个概率分布，则 p 对 q的相对熵为： 
D(p||q)=∑i=1np(x)logp(x)q(x)&lt;/p&gt;

&lt;p&gt;在一定程度上，熵可以度量两个随机变量的距离。KL 散度是两个概率分布 P 和 Q 差别的非对称性的度量。KL 散度是用来度量使用基于 Q 的编码来编码来自 P 的样本平均所需的额外的位元数。
典型情况下，P 表示数据的真实分布，Q 表示数据的理论分布，模型分布，或 P 的近似分布。
相对熵的性质，相对熵（KL散度）有两个主要的性质。如下
（1）尽管 KL 散度从直观上是个度量或距离函数，但它并不是一个真正的度量或者距离，因为它不具有对称性，即
D(p||q)≠D(q||p)
（2）相对熵的值为非负值，即
D(p||q)≥0&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;交叉熵（cross entropy）
H(p,q)=−∑xp(x)logq(x)
在学习决策树时，最重要的步骤是构建决策树。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其中，最重要的步骤是根据属性划分数据集，其中先使用哪个属性，后使用哪个属性，是决定决策树构建的好坏的重要标准。&lt;/p&gt;

&lt;p&gt;其中，使用属性构建数据集，最重要的参考标准，就是使划分后的信息增益最大。&lt;/p&gt;

&lt;p&gt;熵：表示随机变量不确定性，即混乱程度的量化指标。&lt;/p&gt;

&lt;p&gt;熵越大，不确定性越大，越无序；越小，确定性越大，越有序。&lt;/p&gt;

&lt;p&gt;同理，一条信息的信息量大小，与不确定性直接相关。&lt;/p&gt;

&lt;p&gt;不确定性越大，信息量越大，熵越大；&lt;/p&gt;

&lt;p&gt;确定性越大，信息量越小，熵越小。&lt;/p&gt;

&lt;p&gt;熵的单位是bit。&lt;/p&gt;

&lt;p&gt;不计算信息量等，直接存储一个文件，需要的是正常的存储空间大小。&lt;/p&gt;

&lt;p&gt;通过压缩算法，仅保留有用信息的情况下，存储的是文件的信息量。&lt;/p&gt;

&lt;p&gt;两者数量上的差距，是冗余度。&lt;/p&gt;

&lt;p&gt;由此可见：冗余度越大，可压缩的空间越大。反之，亦然。&lt;/p&gt;

&lt;p&gt;另一种度量集合无序程度的方法是：Gini impurity，基尼不纯度。&lt;/p&gt;

&lt;p&gt;信息增益
信息增益 = 信息熵 - 条件熵
信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度&lt;/p&gt;

&lt;p&gt;条件熵&lt;/p&gt;

&lt;p&gt;条件熵是用来解释信息增益而引入的概念，概率定义：随机变量X在给定条件下随机变量Y的条件熵，对定义描述为：X给定条件下Y的条件干率分布的熵对X的数学期望，在机器学习中为选定某个特征后的熵，公式如下：
H(Y|X)=∑p(x)H(Y|X=x)&lt;/p&gt;

&lt;p&gt;3、信息增益
信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好，在概率中定义为：待分类的集合的熵和选定某个特征的条件熵之差（这里只的是经验熵或经验条件熵，由于真正的熵并不知道，是根据样本计算出来的），公式如下：
IG=H(Y)-H(Y|X)&lt;/p&gt;

&lt;p&gt;随机变量X（嫁与不嫁）的信息熵为：
嫁的个数为6个，占1/2，那么信息熵为-1/2log1/2-1/2log1/2 = -log1/2=0.301
现在假如我知道了一个男生的身高信息。身高有三个可能的取值{矮，中，高}
矮包括{1,2,3,5,6,11,12}，嫁的个数为1个，不嫁的个数为6个
中包括{8,9} ，嫁的个数为2个，不嫁的个数为0个
高包括{4,7,10}，嫁的个数为3个，不嫁的个数为0个&lt;/p&gt;

&lt;p&gt;我们先求出公式对应的:
H(Y|X = 矮) = -1/7log1/7-6/7log6/7=0.178
H(Y|X=中) = -1log1-0 = 0
H(Y|X=高） = -1log1-0=0
p(X = 矮) = 7/12,p(X =中) = 2/12,p(X=高) = 3/12
则可以得出条件熵为：
7/12&lt;em&gt;0.178+2/12&lt;/em&gt;0+3/12*0 = 0.103
那么我们知道信息熵与条件熵相减就是我们的信息增益，为0.301-0.103=0.198&lt;/p&gt;

&lt;p&gt;信息增益是针对一个一个的特征而言的，就是看一个特征t，系统有它和没它的时候信息量各是多少，两者的差值就是这个特征给系统带来的信息量，即增益。系统含有特征t的时候信息量很好计算，就是刚才的式子，它表示的是包含所有特征时系统的信息量。&lt;/p&gt;

&lt;p&gt;用决策树来预测：&lt;/p&gt;

&lt;p&gt;决策树的形式类似于“如果天气怎么样，去玩；否则，怎么着怎么着”的树形分叉。那么问题是用哪个属性（即变量，如天气、温度、湿度和风力）最适合充当这颗树的根节点，在它上面没有其他节点，其他的属性都是它的后续节点。&lt;/p&gt;

&lt;p&gt;那么借用上面所述的能够衡量一个属性区分以上数据样本的能力的“信息增益”（Information Gain）理论。&lt;/p&gt;

&lt;p&gt;如果一个属性的信息增益量越大，这个属性作为一棵树的根节点就能使这棵树更简洁，比如说一棵树可以这么读成，如果风力弱，就去玩；风力强，再按天气、温度等分情况讨论，此时用风力作为这棵树的根节点就很有价值。如果说，风力弱，再又天气晴朗，就去玩；如果风力强，再又怎么怎么分情况讨论，这棵树相比就不够简洁了。&lt;/p&gt;

&lt;p&gt;ID3使用信息增益，c4.5使用信息增益比来选择特征&lt;/p&gt;

&lt;p&gt;作者：西门吹风
链接：https://www.zhihu.com/question/22928442/answer/117189907
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。&lt;/p&gt;

&lt;p&gt;而信息增益（Info-Gain）就是指划分前后信息熵的变化：在ID3算法中，信息增益（Info-Gain）越大，划分越好，决策树算法本质上就是要找出每一列的最佳划分以及不同列划分的先后顺序及排布。后面回到题中的问题，C4.5中使用信息增益率（Gain-ratio），ID3算法使用信息增益（Info-Gain），二者有何区别？根据前文的例子，Info-Gain在面对类别较少的离散数据时效果较好，上例中的outlook、temperature等数据都是离散数据，而且每个类别都有一定数量的样本，这种情况下使用ID3与C4.5的区别并不大。但如果面对连续的数据（如体重、身高、年龄、距离等），或者每列数据没有明显的类别之分（最极端的例子的该列所有数据都独一无二），在这种情况下，我们分析信息增益（Info-Gain）的效果：根据公式，E(S)为初始label列的熵，并未发生变化，则IGain(S,A)的大小取决于E(A)的大小，E(A)越小，IGain(S,A)越大，而根据前文例子，，若某一列数据没有重复，ID3算法倾向于把每个数据自成一类，此时，这样E(A)为最小，IGain(S,A)最大，程序会倾向于选择这种划分，这样划分效果极差。为了解决这个问题，引入了信息增益率（Gain-ratio）的概念，计算方式如下：，这里Info为划分行为带来的信息，信息增益率如下计算：这样就减轻了划分行为本身的影响。评论中很多人对文中信息量和熵的部分有疑问，这确实是个很绕的问题，
目前对于信息的定义主要有以下几种：香农（C. E. Shannon）信息是不确定性的消除维纳信息定义 信息是独立于物质、能量的一种属性标示。逆香农信息定义 信息是确定性的增加。邓宇等人的定义 信息是事物现象及其属性标识的集合。上面几种定义各不相同，答主比较倾向于逆香农信息定义，但本质上还是香农那一套。&lt;/p&gt;

&lt;p&gt;基尼不纯度：将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率。
公式：
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/gini.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;基尼不纯度的大概意思是 一个随机事件变成它的对立事件的概率
      例如 一个随机事件X ，P(X=0) = 0.5 ,P(X=1)=0.5
      那么基尼不纯度就为   P(X=0)&lt;em&gt;(1 - P(X=0)) +   P(X=1)&lt;/em&gt;(1 - P(X=1))  = 0.5&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   一个随机事件Y ，P(Y=0) = 0.1 ,P(Y=1)=0.9
  那么基尼不纯度就为P(Y=0)*(1 - P(Y=0)) +   P(Y=1)*(1 - P(Y=1))  = 0.18
 很明显 X比Y更混乱，因为两个都为0.5 很难判断哪个发生。而Y就确定得多，Y=0发生的概率很大。而基尼不纯度也就越小。
所以基尼不纯度也可以作为 衡量系统混乱程度的 标准
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Fri, 05 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/05/shang.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/05/shang.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Zookeeper与Paxos</title>
        <description>&lt;p&gt;　Zookeeper是一个开源的分布式协调服务，其设计目标是将那些复杂的且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一些列简单的接口提供给用户使用。其是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/发布、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。其可以保证如下分布式一致性特性。&lt;/p&gt;

&lt;p&gt;　　① 顺序一致性，从同一个客户端发起的事务请求，最终将会严格地按照其发起顺序被应用到Zookeeper中去。&lt;/p&gt;

&lt;p&gt;　　② 原子性，所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，即整个集群要么都成功应用了某个事务，要么都没有应用。&lt;/p&gt;

&lt;p&gt;　　③ 单一视图，无论客户端连接的是哪个Zookeeper服务器，其看到的服务端数据模型都是一致的。&lt;/p&gt;

&lt;p&gt;　　④ 可靠性，一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会一直被保留，除非有另一个事务对其进行了变更。&lt;/p&gt;

&lt;p&gt;　　⑤ 实时性，Zookeeper保证在一定的时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。&lt;/p&gt;

&lt;p&gt;　　2.1 设计目标&lt;/p&gt;

&lt;p&gt;　　Zookeeper致力于提供一个高性能、高可用、且具有严格的顺序访问控制能力（主要是写操作的严格顺序性）的分布式协调服务，其具有如下的设计目标。&lt;/p&gt;

&lt;p&gt;　　① 简单的数据模型，Zookeeper使得分布式程序能够通过一个共享的树形结构的名字空间来进行相互协调，即Zookeeper服务器内存中的数据模型由一系列被称为ZNode的数据节点组成，Zookeeper将全量的数据存储在内存中，以此来提高服务器吞吐、减少延迟的目的。&lt;/p&gt;

&lt;p&gt;　　② 可构建集群，一个Zookeeper集群通常由一组机器构成，组成Zookeeper集群的而每台机器都会在内存中维护当前服务器状态，并且每台机器之间都相互通信。&lt;/p&gt;

&lt;p&gt;　　③ 顺序访问，对于来自客户端的每个更新请求，Zookeeper都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序。&lt;/p&gt;

&lt;p&gt;　　④ 高性能，Zookeeper将全量数据存储在内存中，并直接服务于客户端的所有非事务请求，因此它尤其适用于以读操作为主的应用场景。&lt;/p&gt;

&lt;p&gt;　　2.2 基本概念&lt;/p&gt;

&lt;p&gt;　　① 集群角色，最典型的集群就是Master/Slave模式（主备模式），此情况下把所有能够处理写操作的机器称为Master机器，把所有通过异步复制方式获取最新数据，并提供读服务的机器为Slave机器。Zookeeper引入了Leader、Follower、Observer三种角色，Zookeeper集群中的所有机器通过Leaser选举过程来选定一台被称为Leader的机器，Leader服务器为客户端提供写服务，Follower和Observer提供读服务，但是Observer不参与Leader选举过程，不参与写操作的过半写成功策略，Observer可以在不影响写性能的情况下提升集群的性能。&lt;/p&gt;

&lt;p&gt;　　② 会话，指客户端会话，一个客户端连接是指客户端和服务端之间的一个TCP长连接，Zookeeper对外的服务端口默认为2181，客户端启动的时候，首先会与服务器建立一个TCP连接，从第一次连接建立开始，客户端会话的生命周期也开始了，通过这个连接，客户端能够心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接受来自服务器的Watch事件通知。&lt;/p&gt;

&lt;p&gt;　　③ 数据节点，第一类指构成集群的机器，称为机器节点，第二类是指数据模型中的数据单元，称为数据节点-Znode，Zookeeper将所有数据存储在内存中，数据模型是一棵树，由斜杠/进行分割的路径，就是一个ZNode，如/foo/path1，每个ZNode都会保存自己的数据内存，同时还会保存一些列属性信息。ZNode分为持久节点和临时节点两类，持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上，而临时节点的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。另外，Zookeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL。一旦节点被标记上这个属性，那么在这个节点被创建的时候，Zookeeper会自动在其节点后面追加一个整形数字，其是由父节点维护的自增数字。&lt;/p&gt;

&lt;p&gt;　　④ 版本，对于每个ZNode，Zookeeper都会为其维护一个叫作Stat的数据结构，Stat记录了这个ZNode的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）、aversion（当前ZNode的ACL版本）。&lt;/p&gt;

&lt;p&gt;　　⑤ Watcher，Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，Zookeeper服务端会将事件通知到感兴趣的客户端。&lt;/p&gt;

&lt;p&gt;　　⑥ ACL，Zookeeper采用ACL（Access Control Lists）策略来进行权限控制，其定义了如下五种权限：&lt;/p&gt;

&lt;p&gt;　　　　· CREATE：创建子节点的权限。&lt;/p&gt;

&lt;p&gt;　　　　· READ：获取节点数据和子节点列表的权限。&lt;/p&gt;

&lt;p&gt;　　　　· WRITE：更新节点数据的权限。&lt;/p&gt;

&lt;p&gt;　　　　· DELETE：删除子节点的权限。&lt;/p&gt;

&lt;p&gt;　　　　· ADMIN：设置节点ACL的权限。&lt;/p&gt;

&lt;p&gt;　　2.3 ZAB协议&lt;/p&gt;

&lt;p&gt;　　Zookeeper使用了Zookeeper Atomic Broadcast（ZAB，Zookeeper原子消息广播协议）的协议作为其数据一致性的核心算法。ZAB协议是为Zookeeper专门设计的一种支持崩溃恢复的原子广播协议。&lt;/p&gt;

&lt;p&gt;　　Zookeeper依赖ZAB协议来实现分布式数据的一致性，基于该协议，Zookeeper实现了一种主备模式的系统架构来保持集群中各副本之间的数据的一致性，即其使用一个单一的诸进程来接收并处理客户端的所有事务请求，并采用ZAB的原子广播协议，将服务器数据的状态变更以事务Proposal的形式广播到所有的副本进程中，ZAB协议的主备模型架构保证了同一时刻集群中只能够有一个主进程来广播服务器的状态变更，因此能够很好地处理客户端大量的并发请求。&lt;/p&gt;

&lt;p&gt;　　ZAB协议的核心是定义了对于那些会改变Zookeeper服务器数据状态的事务请求的处理方式，即：所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader服务器，余下的服务器则称为Follower服务器，Leader服务器负责将一个客户端事务请求转化成一个事务Proposal（提议），并将该Proposal分发给集群中所有的Follower服务器，之后Leader服务器需要等待所有Follower服务器的反馈，一旦超过半数的Follower服务器进行了正确的反馈后，那么Leader就会再次向所有的Follower服务器分发Commit消息，要求其将前一个Proposal进行提交。&lt;/p&gt;

&lt;p&gt;　　ZAB一些包括两种基本的模式：崩溃恢复和消息广播。&lt;/p&gt;

&lt;p&gt;　　当整个服务框架启动过程中或Leader服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB协议就会进入恢复模式并选举产生新的Leader服务器。当选举产生了新的Leader服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式，状态同步时指数据同步，用来保证集群在过半的机器能够和Leader服务器的数据状态保持一致。&lt;/p&gt;

&lt;p&gt;　　当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进入消息广播模式，当一台同样遵守ZAB协议的服务器启动后加入到集群中，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么加入的服务器就会自觉地进入数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。Zookeeper只允许唯一的一个Leader服务器来进行事务请求的处理，Leader服务器在接收到客户端的事务请求后，会生成对应的事务提议并发起一轮广播协议，而如果集群中的其他机器收到客户端的事务请求后，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。&lt;/p&gt;

&lt;p&gt;　　当Leader服务器出现崩溃或者机器重启、集群中已经不存在过半的服务器与Leader服务器保持正常通信时，那么在重新开始新的一轮的原子广播事务操作之前，所有进程首先会使用崩溃恢复协议来使彼此到达一致状态，于是整个ZAB流程就会从消息广播模式进入到崩溃恢复模式。一个机器要成为新的Leader，必须获得过半机器的支持，同时由于每个机器都有可能会崩溃，因此，ZAB协议运行过程中，前后会出现多个Leader，并且每台机器也有可能会多次成为Leader，进入崩溃恢复模式后，只要集群中存在过半的服务器能够彼此进行正常通信，那么就可以产生一个新的Leader并再次进入消息广播模式。如一个由三台机器组成的ZAB服务，通常由一个Leader、2个Follower服务器组成，某一个时刻，加入其中一个Follower挂了，整个ZAB集群是不会中断服务的。&lt;/p&gt;

&lt;p&gt;　　① 消息广播，ZAB协议的消息广播过程使用原子广播协议，类似于一个二阶段提交过程，针对客户端的事务请求，Leader服务器会为其生成对应的事务Proposal，并将其发送给集群中其余所有的机器，然后再分别收集各自的选票，最后进行事务提交。&lt;/p&gt;

&lt;p&gt;　　在ZAB的二阶段提交过程中，移除了中断逻辑，所有的Follower服务器要么正常反馈Leader提出的事务Proposal，要么就抛弃Leader服务器，同时，ZAB协议将二阶段提交中的中断逻辑移除意味着我们可以在过半的Follower服务器已经反馈Ack之后就开始提交事务Proposal，而不需要等待集群中所有的Follower服务器都反馈响应，但是，在这种简化的二阶段提交模型下，无法处理Leader服务器崩溃退出而带来的数据不一致问题，因此ZAB采用了崩溃恢复模式来解决此问题，另外，整个消息广播协议是基于具有FIFO特性的TCP协议来进行网络通信的，因此能够很容易保证消息广播过程中消息接受与发送的顺序性。再整个消息广播过程中，Leader服务器会为每个事务请求生成对应的Proposal来进行广播，并且在广播事务Proposal之前，Leader服务器会首先为这个事务Proposal分配一个全局单调递增的唯一ID，称之为事务ID（ZXID），由于ZAB协议需要保证每个消息严格的因果关系，因此必须将每个事务Proposal按照其ZXID的先后顺序来进行排序和处理。&lt;/p&gt;

&lt;p&gt;　　② 崩溃恢复，在Leader服务器出现崩溃，或者由于网络原因导致Leader服务器失去了与过半Follower的联系，那么就会进入崩溃恢复模式，在ZAB协议中，为了保证程序的正确运行，整个恢复过程结束后需要选举出一个新的Leader服务器，因此，ZAB协议需要一个高效且可靠的Leader选举算法，从而保证能够快速地选举出新的Leader，同时，Leader选举算法不仅仅需要让Leader自身知道已经被选举为Leader，同时还需要让集群中的所有其他机器也能够快速地感知到选举产生的新的Leader服务器。&lt;/p&gt;

&lt;p&gt;　　③ 基本特性，ZAB协议规定了如果一个事务Proposal在一台机器上被处理成功，那么应该在所有的机器上都被处理成功，哪怕机器出现故障崩溃。ZAB协议需要确保那些已经在Leader服务器上提交的事务最终被所有服务器都提交，假设一个事务在Leader服务器上被提交了，并且已经得到了过半Follower服务器的Ack反馈，但是在它Commit消息发送给所有Follower机器之前，Leader服务挂了。如下图所示&lt;/p&gt;

&lt;p&gt;　　在集群正常运行过程中的某一个时刻，Server1是Leader服务器，其先后广播了P1、P2、C1、P3、C2（C2是Commit Of Proposal2的缩写），其中，当Leader服务器发出C2后就立即崩溃退出了，针对这种情况，ZAB协议就需要确保事务Proposal2最终能够在所有的服务器上都被提交成功，否则将出现不一致。&lt;/p&gt;

&lt;p&gt;　　ZAB协议需要确保丢弃那些只在Leader服务器上被提出的事务。如果在崩溃恢复过程中出现一个需要被丢弃的提议，那么在崩溃恢复结束后需要跳过该事务Proposal，如下图所示&lt;/p&gt;

&lt;p&gt;　　假设初始的Leader服务器Server1在提出一个事务Proposal3之后就崩溃退出了，从而导致集群中的其他服务器都没有收到这个事务Proposal，于是，当Server1恢复过来再次加入到集群中的时候，ZAB协议需要确保丢弃Proposal3这个事务。&lt;/p&gt;

&lt;p&gt;　　在上述的崩溃恢复过程中需要处理的特殊情况，就决定了ZAB协议必须设计这样的Leader选举算法：能够确保提交已经被Leader提交的事务的Proposal，同时丢弃已经被跳过的事务Proposal。如果让Leader选举算法能够保证新选举出来的Leader服务器拥有集群中所有机器最高编号（ZXID最大）的事务Proposal，那么就可以保证这个新选举出来的Leader一定具有所有已经提交的提议，更为重要的是如果让具有最高编号事务的Proposal机器称为Leader，就可以省去Leader服务器查询Proposal的提交和丢弃工作这一步骤了。&lt;/p&gt;

&lt;p&gt;　　④ 数据同步，完成Leader选举后，在正式开始工作前，Leader服务器首先会确认日志中的所有Proposal是否都已经被集群中的过半机器提交了，即是否完成了数据同步。Leader服务器需要确所有的Follower服务器都能够接收到每一条事务Proposal，并且能够正确地将所有已经提交了的事务Proposal应用到内存数据库中。Leader服务器会为每个Follower服务器维护一个队列，并将那些没有被各Follower服务器同步的事务以Proposal消息的形式逐个发送给Follower服务器，并在每一个Proposal消息后面紧接着再发送一个Commit消息，以表示该事务已经被提交，等到Follower服务器将所有其尚未同步的事务Proposal都从Leader服务器上同步过来并成功应用到本地数据库后，Leader服务器就会将该Follower服务器加入到真正的可用Follower列表并开始之后的其他流程。&lt;/p&gt;

&lt;p&gt;　　下面分析ZAB协议如何处理需要丢弃的事务Proposal的，ZXID是一个64位的数字，其中32位可以看做是一个简单的单调递增的计数器，针对客户端的每一个事务请求，Leader服务器在产生一个新的事务Proposal时，都会对该计数器进行加1操作，而高32位则代表了Leader周期epoch的编号，每当选举产生一个新的Leader时，就会从这个Leader上取出其本地日志中最大事务Proposal的ZXID，并解析出epoch值，然后加1，之后以该编号作为新的epoch，低32位则置为0来开始生成新的ZXID，ZAB协议通过epoch号来区分Leader周期变化的策略，能够有效地避免不同的Leader服务器错误地使用不同的ZXID编号提出不一样的事务Proposal的异常情况。当一个包含了上一个Leader周期中尚未提交过的事务Proposal的服务器启动时，其肯定无法成为Leader，因为当前集群中一定包含了一个Quorum（过半）集合，该集合中的机器一定包含了更高epoch的事务的Proposal，因此这台机器的事务Proposal并非最高，也就无法成为Leader。&lt;/p&gt;

&lt;p&gt;　　2.4 ZAB协议原理&lt;/p&gt;

&lt;p&gt;　　ZAB主要包括消息广播和崩溃恢复两个过程，进一步可以分为三个阶段，分别是发现（Discovery）、同步（Synchronization）、广播（Broadcast）阶段。ZAB的每一个分布式进程会循环执行这三个阶段，称为主进程周期。&lt;/p&gt;

&lt;p&gt;　　· 发现，选举产生PL(prospective leader)，PL收集Follower epoch(cepoch)，根据Follower的反馈，PL产生newepoch(每次选举产生新Leader的同时产生新epoch)。&lt;/p&gt;

&lt;p&gt;　　· 同步，PL补齐相比Follower多数派缺失的状态、之后各Follower再补齐相比PL缺失的状态，PL和Follower完成状态同步后PL变为正式Leader(established leader)。&lt;/p&gt;

&lt;p&gt;　　· 广播，Leader处理客户端的写操作，并将状态变更广播至Follower，Follower多数派通过之后Leader发起将状态变更落地(deliver/commit)。&lt;/p&gt;

&lt;p&gt;　　在正常运行过程中，ZAB协议会一直运行于阶段三来反复进行消息广播流程，如果出现崩溃或其他原因导致Leader缺失，那么此时ZAB协议会再次进入发现阶段，选举新的Leader。&lt;/p&gt;

&lt;p&gt;　　2.4.1 运行分析&lt;/p&gt;

&lt;p&gt;　　每个进程都有可能处于如下三种状态之一&lt;/p&gt;

&lt;p&gt;　　· LOOKING：Leader选举阶段。&lt;/p&gt;

&lt;p&gt;　　· FOLLOWING：Follower服务器和Leader服务器保持同步状态。&lt;/p&gt;

&lt;p&gt;　　· LEADING：Leader服务器作为主进程领导状态。&lt;/p&gt;

&lt;p&gt;　　所有进程初始状态都是LOOKING状态，此时不存在Leader，此时，进程会试图选举出一个新的Leader，之后，如果进程发现已经选举出新的Leader了，那么它就会切换到FOLLOWING状态，并开始和Leader保持同步，处于FOLLOWING状态的进程称为Follower，LEADING状态的进程称为Leader，当Leader崩溃或放弃领导地位时，其余的Follower进程就会转换到LOOKING状态开始新一轮的Leader选举。&lt;/p&gt;

&lt;p&gt;　　一个Follower只能和一个Leader保持同步，Leader进程和所有与所有的Follower进程之间都通过心跳检测机制来感知彼此的情况。若Leader能够在超时时间内正常收到心跳检测，那么Follower就会一直与该Leader保持连接，而如果在指定时间内Leader无法从过半的Follower进程那里接收到心跳检测，或者TCP连接断开，那么Leader会放弃当前周期的领导，比你转换到LOOKING状态，其他的Follower也会选择放弃这个Leader，同时转换到LOOKING状态，之后会进行新一轮的Leader选举，并在选举产生新的Leader之后开始新的一轮主进程周期。&lt;/p&gt;

&lt;p&gt;　　2.5 ZAB与Paxos的联系和区别&lt;/p&gt;

&lt;p&gt;　　联系：&lt;/p&gt;

&lt;p&gt;　　① 都存在一个类似于Leader进程的角色，由其负责协调多个Follower进程的运行。&lt;/p&gt;

&lt;p&gt;　　② Leader进程都会等待超过半数的Follower做出正确的反馈后，才会将一个提议进行提交。&lt;/p&gt;

&lt;p&gt;　　③ 在ZAB协议中，每个Proposal中都包含了一个epoch值，用来代表当前的Leader周期，在Paxos算法中，同样存在这样的一个标识，名字为Ballot。&lt;/p&gt;

&lt;p&gt;　　区别：&lt;/p&gt;

&lt;p&gt;　　Paxos算法中，新选举产生的主进程会进行两个阶段的工作，第一阶段称为读阶段，新的主进程和其他进程通信来收集主进程提出的提议，并将它们提交。第二阶段称为写阶段，当前主进程开始提出自己的提议。&lt;/p&gt;

&lt;p&gt;　　ZAB协议在Paxos基础上添加了同步阶段，此时，新的Leader会确保存在过半的Follower已经提交了之前的Leader周期中的所有事务Proposal。&lt;/p&gt;

&lt;p&gt;　　ZAB协议主要用于构建一个高可用的分布式数据主备系统，而Paxos算法则用于构建一个分布式的一致性状态机系统。
&lt;!-- more --&gt;
Apache Zookeeper是由Apache Hadoop的子项目发展而来，于2010年11月正式成为Apache顶级项目。Zookeeper为分布式应用提供高效且可靠的分布式协调服务，提供了统一命名服务、配置管理、分布式锁等分布式的基础服务。Zookeeper并没有直接采用Paxos算法，而是采用了一种被称为ZAB(Zookeeper Atomic Broadcast)的一致性协议。&lt;/p&gt;

&lt;p&gt;初识Zookeeper
Zookeeper是一个开源的分布式协调服务，由Yahoo创建，是Google Chubby的开源实现。Zookeeper将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。&lt;/p&gt;

&lt;p&gt;分布式应用程序可以基于Zookeeper实现例如数据发布/订阅、负载均衡、命名服务、协调通知、集群管理、Master选举、分布式锁、分布式队列等功能。Zookeeper可以保证如下分布式一致性特性。&lt;/p&gt;

&lt;p&gt;1、顺序一致性：从同一个客户端发起的事务请求，最终将会严格地按照其发起顺序被应用到Zookeeper中；&lt;/p&gt;

&lt;p&gt;2、原子性：所有事务的请求结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么在整个集群中所有机器上都成功应用了某一个事务，要么都没有应用，没有中间状态；&lt;/p&gt;

&lt;p&gt;3、单一视图：无论客户端连接的是哪个Zookeeper服务器，其看到的服务端数据模型都是一致的。&lt;/p&gt;

&lt;p&gt;4、实时性：Zookeeper仅仅保证在一定的时间内，客户端最终一定能够从服务端上读到最终的数据状态。&lt;/p&gt;

&lt;p&gt;Zookeeper的设计目标
目标一：简单的数据模型&lt;/p&gt;

&lt;p&gt;Zookeeper使得分布式程序能通过一个共享式的、树形结构的名字空间来进行相互协调。&lt;/p&gt;

&lt;p&gt;树形结构的名字空间是指Zookeeper服务器内存中的一个数据模型，由一系列被称为ZNode的数据节点组成，类似一个文件系统。&lt;/p&gt;

&lt;p&gt;目标二：可以构建集群&lt;/p&gt;

&lt;p&gt;组成Zookeeper集群的每台机器都会在内存中维护当前服务器状态，并且每台机器都保持通讯。只要集群中超过一半机器能够正常工作，整个集群就能对外正常服务。&lt;/p&gt;

&lt;p&gt;目标三：顺序访问&lt;/p&gt;

&lt;p&gt;对于来自客户端的每个更新请求，Zookeeper都会分配一个全局唯一递增编号，这个编号反映了所有事物操作的先后顺序，应用程序可以使用Zookeeper的这个特性来实现更加高层的同步原语。&lt;/p&gt;

&lt;p&gt;目标四：高性能&lt;/p&gt;

&lt;p&gt;Zookeeper将全量数据存储在内存中直接对外服务(除事务请求)，尤其适合于读操作为主的应用场景。&lt;/p&gt;

&lt;p&gt;Zookeeper基本概念&lt;/p&gt;

&lt;p&gt;集群角色
最典型的集群角色是Master/Slave模式，Master处理所有写操作，Slave提供读服务。&lt;/p&gt;

&lt;p&gt;在Zookeeper中，有Leader、Follower、Observer三种角色。集群中所有机器通过一个Leader选举来决定一台机器作为Leader，Leader为客户端提供读和写服务。Follower和Observer都提供读服务，区别在于Observer机器不参与选举，也不参与写操作的“过半写成功”策略，因此Observe可以在不影响写性能的情况下提升集群的读性能。&lt;/p&gt;

&lt;p&gt;会话
客户端与Zookeeper是TCP长连接，默认对外端口是2181，通过这个连接，客户端保持和服务器的心跳以维护连接，也能向Zookeeper发送请求并响应，同时还可以接收到注册通知。&lt;/p&gt;

&lt;p&gt;数据节点(ZNode)
Zookeeper所有数据都在内存中，模型类似一颗文件树，ZNode Tree，每个ZNode节点都会保存自己的数据内容和一系列属性。&lt;/p&gt;

&lt;p&gt;ZNode分为持久节点和临时节点，后者和客户端会话绑定。&lt;/p&gt;

&lt;p&gt;版本
每个ZNode，Zookeeper都会维护一个Stat数据结构记录这个ZNode的三个数据版本：当前ZNode版本version、当前ZNode子节点版本cversion、和当前Node的ACL版本aversion。&lt;/p&gt;

&lt;p&gt;Watcher
事件监听是Zookeeper的重要特性，他允许客户端在指定节点注册Watcher，并且在事件被触发后通知客户端。&lt;/p&gt;

&lt;p&gt;ACL
Access Control Lists，定义了5中权限：&lt;/p&gt;

&lt;p&gt;Create、Read、Write、Delete、Admin&lt;/p&gt;

&lt;p&gt;ZAB协议
在ZooKeeper中所有的事务请求都由一个主服务器也就是Leader来处理，其他服务器为Follower，Leader将客户端的事务请求转换为事务Proposal，并且将Proposal分发给集群中其他所有的Follower，然后Leader等待Follwer反馈，当有过半数（&amp;gt;=N/2+1）的Follower反馈信息后，Leader将再次向集群内Follower广播Commit信息，Commit为将之前的Proposal提交。&lt;/p&gt;

&lt;p&gt;ZAB协议中存在着三种状态，每个节点都属于以下三种中的一种：&lt;/p&gt;

&lt;p&gt;1.Looking：系统刚启动时或者Leader崩溃后正处于选举状态&lt;/p&gt;

&lt;p&gt;2.Following：Follower节点所处的状态，Follower与Leader处于数据同步阶段；&lt;/p&gt;

&lt;p&gt;3.Leading：Leader所处状态，当前集群中有一个Leader为主进程；&lt;/p&gt;

&lt;p&gt;ZooKeeper启动时所有节点初始状态为Looking，这时集群会尝试选举出一个Leader节点，选举出的Leader节点切换为Leading状态；当节点发现集群中已经选举出Leader则该节点会切换到Following状态，然后和Leader节点保持同步；当Follower节点与Leader失去联系时Follower节点则会切换到Looking状态，开始新一轮选举；在ZooKeeper的整个生命周期中每个节点都会在Looking、Following、Leading状态间不断转换；&lt;/p&gt;

&lt;p&gt;选举出Leader节点后ZAB进入原子广播阶段，这时Leader为和自己同步的每个节点Follower创建一个操作序列，一个时期一个Follower只能和一个Leader保持同步，Leader节点与Follower节点使用心跳检测来感知对方的存在；当Leader节点在超时时间内收到来自Follower的心跳检测那Follower节点会一直与该节点保持连接；若超时时间内Leader没有接收到来自过半Follower节点的心跳检测或TCP连接断开，那Leader会结束当前周期的领导，切换到Looking状态，所有Follower节点也会放弃该Leader节点切换到Looking状态，然后开始新一轮选举。&lt;/p&gt;

&lt;p&gt;ZAB协议定义了选举（election）、发现（discovery）、同步（sync）、广播(Broadcast)四个阶段；ZAB选举（election）时当Follower存在ZXID（事务ID）时判断所有Follower节点的事务日志，只有lastZXID的节点才有资格成为Leader，这种情况下选举出来的Leader总有最新的事务日志，基于这个原因所以ZooKeeper实现的时候把发现（discovery）与同步（sync）合并为恢复（recovery）阶段；&lt;/p&gt;

&lt;p&gt;1.Election：在Looking状态中选举出Leader节点，Leader的lastZXID总是最新的；&lt;/p&gt;

&lt;p&gt;2.Discovery：Follower节点向准Leader推送FOllOWERINFO，该信息中包含了上一周期的epoch，接受准Leader的NEWLEADER指令，检查newEpoch有效性，准Leader要确保Follower的epoch与ZXID小于或等于自身的；&lt;/p&gt;

&lt;p&gt;3.sync：将Follower与Leader的数据进行同步，由Leader发起同步指令，最总保持集群数据的一致性；&lt;/p&gt;

&lt;p&gt;4.Broadcast：Leader广播Proposal与Commit，Follower接受Proposal与Commit；&lt;/p&gt;

&lt;p&gt;5.Recovery：在Election阶段选举出Leader后本阶段主要工作就是进行数据的同步，使Leader具有highestZXID，集群保持数据的一致性；&lt;/p&gt;

&lt;p&gt;选举（Election）&lt;/p&gt;

&lt;p&gt;election阶段必须确保选出的Leader具有highestZXID，否则在Recovery阶段没法保证数据的一致性，Recovery阶段Leader要求Follower向自己同步数据没有Follower要求Leader保持数据同步，所有选举出来的Leader要具有最新的ZXID；&lt;/p&gt;

&lt;p&gt;在选举的过程中会对每个Follower节点的ZXID进行对比只有highestZXID的Follower才可能当选Leader；&lt;/p&gt;

&lt;p&gt;选举流程：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;每个Follower都向其他节点发送选自身为Leader的Vote投票请求，等待回复；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Follower接受到的Vote如果比自身的大（ZXID更新）时则投票，并更新自身的Vote，否则拒绝投票；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;每个Follower中维护着一个投票记录表，当某个节点收到过半的投票时，结束投票并把该Follower选为Leader，投票结束；&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZAB协议中使用ZXID作为事务编号，ZXID为64位数字，低32位为一个递增的计数器，每一个客户端的一个事务请求时Leader产生新的事务后该计数器都会加1，高32位为Leader周期epoch编号，当新选举出一个Leader节点时Leader会取出本地日志中最大事务Proposal的ZXID解析出对应的epoch把该值加1作为新的epoch，将低32位从0开始生成新的ZXID；ZAB使用epoch来区分不同的Leader周期；&lt;/p&gt;

&lt;p&gt;恢复（Recovery）&lt;/p&gt;

&lt;p&gt;在election阶段选举出来的Leader已经具有最新的ZXID，所有本阶段的主要工作是根据Leader的事务日志对Follower节点数据进行更新；&lt;/p&gt;

&lt;p&gt;Leader：Leader生成新的ZXID与epoch，接收Follower发送过来的FOllOWERINFO（含有当前节点的LastZXID）然后往Follower发送NEWLEADER；Leader根据Follower发送过来的LastZXID根据数据更新策略向Follower发送更新指令；&lt;/p&gt;

&lt;p&gt;同步策略：&lt;/p&gt;

&lt;p&gt;1.SNAP：如果Follower数据太老，Leader将发送快照SNAP指令给Follower同步数据；&lt;/p&gt;

&lt;p&gt;2.DIFF：Leader发送从Follolwer.lastZXID到Leader.lastZXID议案的DIFF指令给Follower同步数据；&lt;/p&gt;

&lt;p&gt;3.TRUNC：当Follower.lastZXID比Leader.lastZXID大时，Leader发送从Leader.lastZXID到Follower.lastZXID的TRUNC指令让Follower丢弃该段数据；&lt;/p&gt;

&lt;p&gt;Follower：往Leader发送FOLLOERINFO指令，Leader拒绝就转到Election阶段；接收Leader的NEWLEADER指令，如果该指令中epoch比当前Follower的epoch小那么Follower转到Election阶段；Follower还有主要工作是接收SNAP/DIFF/TRUNC指令同步数据与ZXID，同步成功后回复ACKNETLEADER，然后进入下一阶段；Follower将所有事务都同步完成后Leader会把该节点添加到可用Follower列表中；&lt;/p&gt;

&lt;p&gt;SNAP与DIFF用于保证集群中Follower节点已经Committed的数据的一致性，TRUNC用于抛弃已经被处理但是没有Committed的数据；&lt;/p&gt;

&lt;p&gt;广播(Broadcast)&lt;/p&gt;

&lt;p&gt;客户端提交事务请求时Leader节点为每一个请求生成一个事务Proposal，将其发送给集群中所有的Follower节点，收到过半Follower的反馈后开始对事务进行提交，ZAB协议使用了原子广播协议；在ZAB协议中只需要得到过半的Follower节点反馈Ack就可以对事务进行提交，这也导致了Leader几点崩溃后可能会出现数据不一致的情况，ZAB使用了崩溃恢复来处理数字不一致问题；消息广播使用了TCP协议进行通讯所有保证了接受和发送事务的顺序性。广播消息时Leader节点为每个事务Proposal分配一个全局递增的ZXID（事务ID），每个事务Proposal都按照ZXID顺序来处理；&lt;/p&gt;

&lt;p&gt;Leader节点为每一个Follower节点分配一个队列按事务ZXID顺序放入到队列中，且根据队列的规则FIFO来进行事务的发送。Follower节点收到事务Proposal后会将该事务以事务日志方式写入到本地磁盘中，成功后反馈Ack消息给Leader节点，Leader在接收到过半Follower节点的Ack反馈后就会进行事务的提交，以此同时向所有的Follower节点广播Commit消息，Follower节点收到Commit后开始对事务进行提交。&lt;/p&gt;

&lt;p&gt;总的来说，ZAB主要是用来构建一个高可用的分布式数据主备系统，而Paxos则是构建一个分布式的一致性状态机系统。&lt;/p&gt;

&lt;p&gt;&amp;lt;Google的Chubby，Apache的Zookeeper都是基于它的理论来实现的，Paxos还被认为是到目前为止唯一的分布式一致性算法，其它的算法都是Paxos的改进或简化。有个问题要提一下，Paxos有一个前提：没有拜占庭将军问题。就是说Paxos只有在一个可信的计算环境中才能成立，这个环境是不会被入侵所破坏的。&lt;/p&gt;

&lt;p&gt;”&lt;/p&gt;

&lt;p&gt;Paxos 这个算法是Leslie Lamport在1990年提出的一种基于消息传递的一致性算法.Paxos 算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。&lt;/p&gt;

&lt;p&gt;part-time parliament Paxos Made Simple里这样描述Paxos算法执行过程：&lt;/p&gt;

&lt;p&gt;prepare 阶段：
proposer 选择一个提案编号 n 并将 prepare 请求发送给 acceptors 中的一个多数派；
acceptor 收到 prepare 消息后，如果提案的编号大于它已经回复的所有 prepare 消息，则 acceptor 将自己上次接受的提案回复给 proposer，并承诺不再回复小于 n 的提案；
批准阶段：
当一个 proposer 收到了多数 acceptors 对 prepare 的回复后，就进入批准阶段。它要向回复 prepare 请求的 acceptors 发送 accept 请求，包括编号 n 和根据 P2c 决定的 value（如果根据 P2c 没有已经接受的 value，那么它可以自由决定 value）。
在不违背自己向其他 proposer 的承诺的前提下，acceptor 收到 accept 请求后即接受这个请求。
wiki上是两个阶段，论文里却是说三阶段，而且默认就有了个proposer相当于leader。查资料有大侠列出了第三个阶段(http://www.wuzesheng.com/?p=2724)：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Learn阶段:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;当各个Acceptor达到一致之后，需要将达到一致的结果通知给所有的Learner.&lt;/p&gt;

&lt;p&gt;zookeeper采用org.apache.zookeeper.server.quorum.FastLeaderElection作为其缺省选举算法&lt;/p&gt;

&lt;p&gt;这篇文章http://csrd.aliapp.com/?p=162&amp;amp;replytocom=782 直接用paxos实现作为标题，提到  zookeeper在选举leader的时候采用了paxos算法(主要是fast paxos)&lt;/p&gt;

&lt;p&gt;偶然看到下边有人反驳：&lt;/p&gt;

&lt;p&gt;魏讲文：“&lt;/p&gt;

&lt;p&gt;FastLeaderElection根本不是Paxos，也不是Fast Paxos的实现。
FastLeaderElection源码与Paxos的论文相去甚远。&lt;/p&gt;

&lt;p&gt;Paxos与 FastPaxos算法中也有一个leader选举的问题。&lt;/p&gt;

&lt;p&gt;FastLeaderElection对于zookeeper来讲，只是相当于Paxos中的leader选举。&lt;/p&gt;

&lt;p&gt;”&lt;/p&gt;

&lt;p&gt;二、资料证实&lt;/p&gt;

&lt;p&gt;好的，查查资料，分析源码开始调研&lt;/p&gt;

&lt;p&gt;首先是魏讲文的反驳 ：&lt;/p&gt;

&lt;p&gt;There is a very common misunderstanding that the leader election algorithm in zookeeper is paxos or fast paxos. The leader election algorithm is not paxos or fast paxos, please consider the following facts:&lt;/p&gt;

&lt;p&gt;There is no the concept of proposal number in the leader election in zookeeper. the proposal number is a key concept to paxos. Some one think the epoch is the proposal number, but different followers may produce proposal with the same epoch which is not allowed in paxos.&lt;/p&gt;

&lt;p&gt;Fast paxos requires at least 3t + 1 acceptors, where t is the number of servers which are allowed to fail [3]. This is conflict with the fact that a zookeeper cluster with 3 servers works well even if one server failed.&lt;/p&gt;

&lt;p&gt;The leader election algorithm must make sure P1. However paxos does provide such guarantee.&lt;/p&gt;

&lt;p&gt;In paxos, a leader is also required to achieve progress. There are some similarities between the leader in paxos and the leader in zookeeper. Even if more than one servers believe they are the leader, the consistency is preserved both in zookeeper and in paxos. this is very clearly discussed in [1] and [2].&lt;/p&gt;

&lt;p&gt;然后是作者三次对比&lt;/p&gt;

&lt;p&gt;1）邮件列表&lt;/p&gt;

&lt;p&gt;Our protocol instead, has only two phases, just like a two-phase 
commit protocol. Of course, for Paxos, we can ignore the first phase in runs in 
which we have a single proposer as we can run phase 1 for multiple instances at 
a time, which is what Ben called previously Multi-Paxos, I believe. The trick 
with skipping phase 1 is to deal with leader switching. 
2）出书的访谈&lt;/p&gt;

&lt;p&gt;We made a few interesting observations about Paxos when contrasting it to Zab, like problems you could run into if you just implemented Paxos alone. Not that Paxos is broken or anything, just that in our setting, there were some properties it was not giving us. Some people still like to map Zab to Paxos, and they are not completely off, but the way we see it, Zab matches a service like ZooKeeper well.&lt;/p&gt;

&lt;p&gt;zk的分布式一致性算法有了个名称叫Zab&lt;/p&gt;

&lt;p&gt;3）论文&lt;/p&gt;

&lt;p&gt;We use an algorithm that shares some of the character- istics of Paxos, but that combines transaction logging needed for consensus with write-ahead logging needed for data tree recovery to enable an efficient implementa- tion.&lt;/p&gt;

&lt;p&gt;三、leader选举分析&lt;/p&gt;

&lt;p&gt;在我理解首先在选举时，并不能用到paxos算法，paxos里选总统也好，zk选leader也好，跟搞个提案让大部分人同意是有区别的。选主才能保证不会出现多proposer的并发提案冲突&lt;/p&gt;

&lt;p&gt;谁去作为proposer发提案？是paxos算法进行下去的前提。而提出提案让大部分follower同意则可用到类似paxos的算法实现一致性。zookeeper使用的是Zab算法实现一致性。&lt;/p&gt;

&lt;p&gt;zk的选主策略：&lt;/p&gt;

&lt;p&gt;there can be at most one leader (proposer) at any time, and we guarantee this by making sure 
that a quorum of replicas recognize the leader as a leader by committing to an 
epoch change. This change in epoch also allows us to get unique zxids since the 
epoch forms part of the zxid.&lt;/p&gt;

&lt;p&gt;每个server有一个id，收到提交的事务时则有一个zxid，随更新数据的变动，事务编号递增，server id各不同。首先选zxid最大的作为leader，如果zxid比较不出来，则选server id最大的为leader&lt;/p&gt;

&lt;p&gt;zxid包含一个epoch数字，epoch指示一个server作为leader的时期，随新的leader诞生而递增。&lt;/p&gt;

&lt;p&gt;再看代码：&lt;/p&gt;

&lt;p&gt;四、zookeeper数据更新原理分析&lt;/p&gt;

&lt;p&gt;了解完选主的做法后，来了解下同步数据的做法，同步数据则采用Zab协议：Zookeeper Atomic broadcast protocol，是个类似两阶段提交的协议：&lt;/p&gt;

&lt;p&gt;The leader sends a PROPOSAL message, p, to all followers.&lt;/p&gt;

&lt;p&gt;Upon receiving p, a follower responds to the leader with an ACK, informing the&lt;/p&gt;

&lt;p&gt;leader that it has accepted the proposal.&lt;/p&gt;

&lt;p&gt;Uponreceivingacknowledgmentsfromaquorum(thequorumincludestheleader&lt;/p&gt;

&lt;p&gt;itself), the leader sends a message informing the followers to COMMIT it.&lt;/p&gt;

&lt;p&gt;跟paxos的区别是leaer发送给所有follower，而不是大多数，所有follower都要确认并通知leader，而不是大多数。&lt;/p&gt;

&lt;p&gt;保证机制：按顺序广播的两个事务， T 和 Tʹ ，T在前则Tʹ 生效前必须提交T。如果有一个server 提交了T 和 Tʹ ，则所有其他server必须也在Tʹ前提交T。&lt;/p&gt;

&lt;p&gt;五、leader的探活&lt;/p&gt;

&lt;p&gt;为解决leader crash的问题，避免出现多个leader导致事务混乱，Zab算法保证：&lt;/p&gt;

&lt;p&gt;1、新事务开启时，leader必须提交上个epoch期间提交的所有事务&lt;/p&gt;

&lt;p&gt;2、任何时候都不会有两个leader同时获得足够多的支持者。&lt;/p&gt;

&lt;p&gt;一个新leader的起始状态需要大多数server同意&lt;/p&gt;

&lt;p&gt;六、observer&lt;/p&gt;

&lt;p&gt;zk里的第三种角色，观察者和follower的区别就是没有选举权。它主要是1、为系统的读请求扩展性存在 2、满足多机房部署需求，中心机房部署leader、follower，其他机房部署observer，读取配置优先读本地。&lt;/p&gt;

&lt;p&gt;七、总结&lt;/p&gt;

&lt;p&gt;我认为zookeeper只能说是受paxos算法影响，角色划分类似，提案通过方式类似，实现更为简单直观。选主基于voteid(server-id)和zxid做大小优先级排序，信息同步则使用两阶段提交，leader获取follower的全部同意后才提交事务，更新状态。observer角色则是为了增加系统吞吐和满足跨机房部署。&lt;/p&gt;

&lt;p&gt;参考文献&lt;/p&gt;

&lt;p&gt;[1] Reed, B., &amp;amp; Junqueira, F. P. (2008). A simple totally ordered broadcast protocol. Second
Workshop on Large-Scale Distributed Systems and Middleware (LADIS 2008). Yorktown
Heights, NY: ACM. ISBN: 978-1-60558-296-2.
[2] Lamport, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 1825.
[3] F. Junqueira, Y. Mao, and K. Marzullo. Classic paxos vs. fast paxos: caveat emptor. In
Proceedings of the 3rd USENIX/IEEE/IFIP Workshop on Hot Topics in System Dependability
(HotDep.07). Citeseer, 2007.&lt;/p&gt;

&lt;p&gt;[4]O’Reilly.ZooKeeper.Distributed process coordination.2013&lt;/p&gt;

&lt;p&gt;[5] http://agapple.iteye.com/blog/1184023  zookeeper项目使用几点小结
1.1 基本定义
算法中的参与者主要分为三个角色，同时每个参与者又可兼领多个角色:&lt;/p&gt;

&lt;p&gt;⑴proposer 提出提案，提案信息包括提案编号和提议的value;&lt;/p&gt;

&lt;p&gt;⑵acceptor 收到提案后可以接受(accept)提案;&lt;/p&gt;

&lt;p&gt;⑶learner 只能”学习”被批准的提案;&lt;/p&gt;

&lt;p&gt;算法保重一致性的基本语义:&lt;/p&gt;

&lt;p&gt;⑴决议(value)只有在被proposers提出后才能被批准(未经批准的决议称为”提案(proposal)”);&lt;/p&gt;

&lt;p&gt;⑵在一次Paxos算法的执行实例中，只批准(chosen)一个value;&lt;/p&gt;

&lt;p&gt;⑶learners只能获得被批准(chosen)的value;&lt;/p&gt;

&lt;p&gt;有上面的三个语义可演化为四个约束:&lt;/p&gt;

&lt;p&gt;⑴P1:一个acceptor必须接受(accept)第一次收到的提案;&lt;/p&gt;

&lt;p&gt;⑵P2a:一旦一个具有value v的提案被批准(chosen)，那么之后任何acceptor 再次接受(accept)的提案必须具有value v;&lt;/p&gt;

&lt;p&gt;⑶P2b:一旦一个具有value v的提案被批准(chosen)，那么以后任何 proposer 提出的提案必须具有value v;&lt;/p&gt;

&lt;p&gt;⑷P2c:如果一个编号为n的提案具有value v，那么存在一个多数派，要么他们中所有人都没有接受(accept)编号小于n的任何提案，要么他们已经接受(accpet)的所有编号小于n的提案中编号最大的那个提案具有value v;&lt;/p&gt;

&lt;p&gt;1.2 基本算法(basic paxos)
算法(决议的提出与批准)主要分为两个阶段:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;prepare阶段：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(1). 当Porposer希望提出方案V1，首先发出prepare请求至大多数Acceptor。Prepare请求内容为序列号&lt;SN1&gt;;&lt;/SN1&gt;&lt;/p&gt;

&lt;p&gt;(2). 当Acceptor接收到prepare请求&lt;SN1&gt;时，检查自身上次回复过的prepare请求&lt;SN2&gt;&lt;/SN2&gt;&lt;/SN1&gt;&lt;/p&gt;

&lt;p&gt;a). 如果SN2&amp;gt;SN1，则忽略此请求，直接结束本次批准过程;&lt;/p&gt;

&lt;p&gt;b). 否则检查上次批准的accept请求&amp;lt;SNx，Vx&amp;gt;，并且回复&amp;lt;SNx，Vx&amp;gt;；如果之前没有进行过批准，则简单回复&lt;OK&gt;;&lt;/OK&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;accept批准阶段：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(1a). 经过一段时间，收到一些Acceptor回复，回复可分为以下几种:&lt;/p&gt;

&lt;p&gt;a). 回复数量满足多数派，并且所有的回复都是&lt;OK&gt;，则Porposer发出accept请求，请求内容为议案&amp;lt;SN1，V1&amp;gt;;&lt;/OK&gt;&lt;/p&gt;

&lt;p&gt;b). 回复数量满足多数派，但有的回复为：&amp;lt;SN2，V2&amp;gt;，&amp;lt;SN3，V3&amp;gt;……则Porposer找到所有回复中超过半数的那个，假设为&amp;lt;SNx，Vx&amp;gt;，则发出accept请求，请求内容为议案&amp;lt;SN1，Vx&amp;gt;;&lt;/p&gt;

&lt;p&gt;c). 回复数量不满足多数派，Proposer尝试增加序列号为SN1+，转1继续执行;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     (1b). 经过一段时间，收到一些Acceptor回复，回复可分为以下几种:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;a). 回复数量满足多数派，则确认V1被接受;&lt;/p&gt;

&lt;p&gt;b). 回复数量不满足多数派，V1未被接受，Proposer增加序列号为SN1+，转1继续执行;&lt;/p&gt;

&lt;p&gt;(2). 在不违背自己向其他proposer的承诺的前提下，acceptor收到accept 请求后即接受并回复这个请求。&lt;/p&gt;

&lt;p&gt;1.3 算法优化(fast paxos)
Paxos算法在出现竞争的情况下，其收敛速度很慢，甚至可能出现活锁的情况，例如当有三个及三个以上的proposer在发送prepare请求后，很难有一个proposer收到半数以上的回复而不断地执行第一阶段的协议。因此，为了避免竞争，加快收敛的速度，在算法中引入了一个Leader这个角色，在正常情况下同时应该最多只能有一个参与者扮演Leader角色，而其它的参与者则扮演Acceptor的角色，同时所有的人又都扮演Learner的角色。&lt;/p&gt;

&lt;p&gt;在这种优化算法中，只有Leader可以提出议案，从而避免了竞争使得算法能够快速地收敛而趋于一致，此时的paxos算法在本质上就退变为两阶段提交协议。但在异常情况下，系统可能会出现多Leader的情况，但这并不会破坏算法对一致性的保证，此时多个Leader都可以提出自己的提案，优化的算法就退化成了原始的paxos算法。&lt;/p&gt;

&lt;p&gt;一个Leader的工作流程主要有分为三个阶段：&lt;/p&gt;

&lt;p&gt;(1).学习阶段 向其它的参与者学习自己不知道的数据(决议);&lt;/p&gt;

&lt;p&gt;(2).同步阶段 让绝大多数参与者保持数据(决议)的一致性;&lt;/p&gt;

&lt;p&gt;(3).服务阶段 为客户端服务，提议案;&lt;/p&gt;

&lt;p&gt;1.3.1 学习阶段
当一个参与者成为了Leader之后，它应该需要知道绝大多数的paxos实例，因此就会马上启动一个主动学习的过程。假设当前的新Leader早就知道了1-134、138和139的paxos实例，那么它会执行135-137和大于139的paxos实例的第一阶段。如果只检测到135和140的paxos实例有确定的值，那它最后就会知道1-135以及138-140的paxos实例。&lt;/p&gt;

&lt;p&gt;1.3.2 同步阶段
此时的Leader已经知道了1-135、138-140的paxos实例，那么它就会重新执行1-135的paxos实例，以保证绝大多数参与者在1-135的paxos实例上是保持一致的。至于139-140的paxos实例，它并不马上执行138-140的paxos实例，而是等到在服务阶段填充了136、137的paxos实例之后再执行。这里之所以要填充间隔，是为了避免以后的Leader总是要学习这些间隔中的paxos实例，而这些paxos实例又没有对应的确定值。&lt;/p&gt;

&lt;p&gt;1.3.4 服务阶段
Leader将用户的请求转化为对应的paxos实例，当然，它可以并发的执行多个paxos实例，当这个Leader出现异常之后，就很有可能造成paxos实例出现间断。&lt;/p&gt;

&lt;p&gt;1.3.5 问题
(1).Leader的选举原则&lt;/p&gt;

&lt;p&gt;(2).Acceptor如何感知当前Leader的失败，客户如何知道当前的Leader&lt;/p&gt;

&lt;p&gt;(3).当出现多Leader之后，如何kill掉多余的Leader&lt;/p&gt;

&lt;p&gt;(4).如何动态的扩展Acceptor&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Zookeeper
2.1 整体架构
在Zookeeper集群中，主要分为三者角色，而每一个节点同时只能扮演一种角色，这三种角色分别是：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(1). Leader 接受所有Follower的提案请求并统一协调发起提案的投票，负责与所有的Follower进行内部的数据交换(同步);&lt;/p&gt;

&lt;p&gt;(2). Follower 直接为客户端服务并参与提案的投票，同时与Leader进行数据交换(同步);&lt;/p&gt;

&lt;p&gt;(3). Observer 直接为客户端服务但并不参与提案的投票，同时也与Leader进行数据交换(同步);&lt;/p&gt;

&lt;p&gt;2.2 QuorumPeer的基本设计&lt;/p&gt;

&lt;p&gt;Zookeeper对于每个节点QuorumPeer的设计相当的灵活，QuorumPeer主要包括四个组件：客户端请求接收器(ServerCnxnFactory)、数据引擎(ZKDatabase)、选举器(Election)、核心功能组件(Leader/Follower/Observer)。其中：&lt;/p&gt;

&lt;p&gt;(1). ServerCnxnFactory负责维护与客户端的连接(接收客户端的请求并发送相应的响应);&lt;/p&gt;

&lt;p&gt;(2). ZKDatabase负责存储/加载/查找数据(基于目录树结构的KV+操作日志+客户端Session);&lt;/p&gt;

&lt;p&gt;(3). Election负责选举集群的一个Leader节点;&lt;/p&gt;

&lt;p&gt;(4). Leader/Follower/Observer一个QuorumPeer节点应该完成的核心职责;&lt;/p&gt;

&lt;p&gt;2.3 QuorumPeer工作流程&lt;/p&gt;

&lt;p&gt;2.3.1 Leader职责&lt;/p&gt;

&lt;p&gt;Follower确认: 等待所有的Follower连接注册，若在规定的时间内收到合法的Follower注册数量，则确认成功；否则，确认失败。&lt;/p&gt;

&lt;p&gt;2.3.2 Follower职责&lt;/p&gt;

&lt;p&gt;2.4 选举算法
2.4.1 LeaderElection选举算法&lt;/p&gt;

&lt;p&gt;选举线程由当前Server发起选举的线程担任，他主要的功能对投票结果进行统计，并选出推荐的Server。选举线程首先向所有Server发起一次询问(包括自己)，被询问方，根据自己当前的状态作相应的回复，选举线程收到回复后，验证是否是自己发起的询问(验证xid 是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议 的&lt;/p&gt;

&lt;p&gt;leader 相关信息(id,zxid)，并将这些 信息存储到当次选举的投票记录表中，当向所有Serve r&lt;/p&gt;

&lt;p&gt;都询问完以后，对统计结果进行筛选并进行统计，计算出当次询问后获胜的是哪一个Server，并将当前zxid最大的Server 设置为当前Server要推荐的Server(有可能是自己，也有可以是其它的Server，根据投票结果而定，但是每一个Server在第一次投票时都会投自己)，如果此时获胜的Server获得n/2 + 1的Server票数，设置当前推荐的leader为获胜的Server。根据获胜的Server相关信息设置自己的状态。每一个Server都重复以上流程直到选举出Leader。&lt;/p&gt;

&lt;p&gt;初始化选票(第一张选票): 每个quorum节点一开始都投给自己;&lt;/p&gt;

&lt;p&gt;收集选票: 使用UDP协议尽量收集所有quorum节点当前的选票(单线程/同步方式)，超时设置200ms;&lt;/p&gt;

&lt;p&gt;统计选票: 1).每个quorum节点的票数;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     2).为自己产生一张新选票(zxid、myid均最大);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;选举成功: 某一个quorum节点的票数超过半数;&lt;/p&gt;

&lt;p&gt;更新选票: 在本轮选举失败的情况下，当前quorum节点会从收集的选票中选取合适的选票(zxid、myid均最大)作为自己下一轮选举的投票;&lt;/p&gt;

&lt;p&gt;异常问题的处理&lt;/p&gt;

&lt;p&gt;1). 选举过程中，Server的加入&lt;/p&gt;

&lt;p&gt;当一个Server启动时它都会发起一次选举，此时由选举线程发起相关流程，那么每个 Serve r都会获得当前zxi d最大的哪个Serve r是谁，如果当次最大的Serve r没有获得n/2+1 个票数，那么下一次投票时，他将向zxid最大的Server投票，重复以上流程，最后一定能选举出一个Leader。&lt;/p&gt;

&lt;p&gt;2). 选举过程中，Server的退出&lt;/p&gt;

&lt;p&gt;只要保证n/2+1个Server存活就没有任何问题，如果少于n/2+1个Server 存活就没办法选出Leader。&lt;/p&gt;

&lt;p&gt;3). 选举过程中，Leader死亡&lt;/p&gt;

&lt;p&gt;当选举出Leader以后，此时每个Server应该是什么状态(FLLOWING)都已经确定，此时由于Leader已经死亡我们就不管它，其它的Fllower按正常的流程继续下去，当完成这个流程以后，所有的Fllower都会向Leader发送Ping消息，如果无法ping通，就改变自己的状为(FLLOWING ==&amp;gt; LOOKING)，发起新的一轮选举。&lt;/p&gt;

&lt;p&gt;4). 选举完成以后，Leader死亡&lt;/p&gt;

&lt;p&gt;处理过程同上。&lt;/p&gt;

&lt;p&gt;5). 双主问题&lt;/p&gt;

&lt;p&gt;Leader的选举是保证只产生一个公认的Leader的，而且Follower重新选举与旧Leader恢复并退出基本上是同时发生的，当Follower无法ping同Leader是就认为Leader已经出问题开始重新选举，Leader收到Follower的ping没有达到半数以上则要退出Leader重新选举。&lt;/p&gt;

&lt;p&gt;2.4.2 FastLeaderElection选举算法
FastLeaderElection是标准的fast paxos的实现，它首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决 epoch 和 zxid 的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息。&lt;/p&gt;

&lt;p&gt;FastLeaderElection算法通过异步的通信方式来收集其它节点的选票，同时在分析选票时又根据投票者的当前状态来作不同的处理，以加快Leader的选举进程。&lt;/p&gt;

&lt;p&gt;每个Server都一个接收线程池和一个发送线程池, 在没有发起选举时，这两个线程池处于阻塞状态，直到有消息到来时才解除阻塞并处理消息，同时每个Serve r都有一个选举线程(可以发起选举的线程担任)。&lt;/p&gt;

&lt;p&gt;1). 主动发起选举端(选举线程)的处理&lt;/p&gt;

&lt;p&gt;首先自己的 logicalclock加1，然后生成notification消息，并将消息放入发送队列中， 系统中配置有几个Server就生成几条消息，保证每个Server都能收到此消息，如果当前Server 的状态是LOOKING就一直循环检查接收队列是否有消息，如果有消息，根据消息中对方的状态进行相应的处理。&lt;/p&gt;

&lt;p&gt;2).主动发送消息端(发送线程池)的处理&lt;/p&gt;

&lt;p&gt;将要发送的消息由Notification消息转换成ToSend消息，然后发送对方，并等待对方的回复。&lt;/p&gt;

&lt;p&gt;3). 被动接收消息端(接收线程池)的处理&lt;/p&gt;

&lt;p&gt;将收到的消息转换成Notification消息放入接收队列中，如果对方Server的epoch小于logicalclock则向其发送一个消息(让其更新epoch)；如果对方Server处于Looking状态，自己则处于Following或Leading状态，则也发送一个消息(当前Leader已产生，让其尽快收敛)。&lt;/p&gt;

&lt;p&gt;2.4.3 AuthFastLeaderElection选举算法
AuthFastLeaderElection算法同FastLeaderElection算法基本一致，只是在消息中加入了认证信息，该算法在最新的Zookeeper中也建议弃用。
2.6 Zookeeper中的请求处理流程
2.6.1 Follower节点处理用户的读写请求&lt;/p&gt;

&lt;p&gt;2.6.2 Leader节点处理写请求&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;值得注意的是， Follower/Leader上的读操作时并行的，读写操作是串行的，当CommitRequestProcessor处理一个写请求时，会阻塞之后所有的读写请求。
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/04/zookeeper.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/04/zookeeper.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>安全散列算法SHA256</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;安全散列算法SHA（Secure Hash Algorithm）是美国国家安全局 （NSA） 设计，美国国家标准与技术研究院（NIST） 发布的一系列密码散列函数，包括 SHA-1、SHA-224、SHA-256、SHA-384 和 SHA-512 等变体。主要适用于数字签名标准（DigitalSignature Standard DSS）里面定义的数字签名算法（Digital Signature Algorithm DSA）。下面以 SHA-1为例，介绍该算法计算消息摘要的原理。&lt;/p&gt;

&lt;p&gt;对于长度小于2^64位的消息，SHA1会产生一个160位的消息摘要。当接收到消息的时候，这个消息摘要可以用来验证数据的完整性。在传输的过程中，数据很可能会发生变化，那么这时候就会产生不同的消息摘要。
　　SHA1有如下特性：不可以从消息摘要中复原信息；两个不同的消息不会产生同样的消息摘要。&lt;/p&gt;

&lt;p&gt;　　一、术语和概念&lt;/p&gt;

&lt;p&gt;　　（一）位(Bit)，字节（Byte）和字（Word）&lt;/p&gt;

&lt;p&gt;　　SHA1始终把消息当成一个位（bit）字符串来处理。本文中，一个“字”（Word）是32位，而一个“字节”（Byte）是8位。比如，字符串“abc”可以被转换成一个位字符串：01100001 01100010 01100011。它也可以被表示成16进制字符串:0x616263.&lt;/p&gt;

&lt;p&gt;　　（二）运算符和符号&lt;/p&gt;

&lt;p&gt;　　下面的逻辑运算符都被运用于“字”（Word）&lt;/p&gt;

&lt;p&gt;　　X^Y = X，Y逻辑与&lt;/p&gt;

&lt;p&gt;　　X \/ Y = X，Y逻辑或&lt;/p&gt;

&lt;p&gt;　　X XOR Y= X，Y逻辑异或&lt;/p&gt;

&lt;p&gt;　　~X = X逻辑取反&lt;/p&gt;

&lt;p&gt;　　X+Y定义如下：&lt;/p&gt;

&lt;p&gt;　　字 X 和Y 代表两个整数 x 和y, 其中0 &amp;lt;= x &amp;lt; 2^32 且 0 &amp;lt;= y &amp;lt; 2^32. 令整数z= (x + y) mod 2^32. 这时候 0 &amp;lt;= z &amp;lt; 2^32. 将z转换成字Z,那么就是 Z = X + Y.&lt;/p&gt;

&lt;p&gt;　　循环左移位操作符Sn(X)。X是一个字，n是一个整数，0&amp;lt;=n&amp;lt;=32。Sn(X)= (X&amp;lt;&amp;gt;32-n)&lt;/p&gt;

&lt;p&gt;　　X&amp;lt;定义如下：抛弃最左边的n位数字，将各个位依次向左移动n位，然后用0填补右边的n位（最后结果还是32位）。X»n是抛弃右边的n位，将各个位依次向右移动n位，然后在左边的n位填0。因此可以叫Sn(X)位循环移位运算&lt;/p&gt;

&lt;p&gt;　　二、SHA1算法描述&lt;/p&gt;

&lt;p&gt;　　在SHA1算法中，我们必须把原始消息（字符串，文件等）转换成位字符串。SHA1算法只接受位作为输入。假设我们对字符串“abc”产生消息摘要。首先，我们将它转换成位字符串如下：&lt;/p&gt;

&lt;p&gt;　　01100001 0110001001100011&lt;/p&gt;

&lt;p&gt;　　―――――――――――――&lt;/p&gt;

&lt;p&gt;　　‘a’=97 ‘b’=98‘c’=99&lt;/p&gt;

&lt;p&gt;　　这个位字符串的长度为24。下面我们需要5个步骤来计算MD5。&lt;/p&gt;

&lt;p&gt;　　（一）补位&lt;/p&gt;

&lt;p&gt;　　消息必须进行补位，以使其长度在对512取模以后的余数是448。也就是说，（补位后的消息长度）%512 = 448。即使长度已经满足对512取模后余数是448，补位也必须要进行。&lt;/p&gt;

&lt;p&gt;　　补位是这样进行的：先补一个1，然后再补0，直到长度满足对512取模后余数是448。总而言之，补位是至少补一位，最多补512位。还是以前面的“abc”为例显示补位的过程。&lt;/p&gt;

&lt;p&gt;　　原始信息：01100001 01100010 01100011&lt;/p&gt;

&lt;p&gt;　　补位第一步：0110000101100010 01100011 1&lt;/p&gt;

&lt;p&gt;　　首先补一个“1”&lt;/p&gt;

&lt;p&gt;　　补位第二步：0110000101100010 01100011 10…..0&lt;/p&gt;

&lt;p&gt;　　然后补423个“0”&lt;/p&gt;

&lt;p&gt;　　我们可以把最后补位完成后的数据用16进制写成下面的样子&lt;/p&gt;

&lt;p&gt;　　61626380 0000000000000000 00000000&lt;/p&gt;

&lt;p&gt;　　00000000 0000000000000000 00000000&lt;/p&gt;

&lt;p&gt;　　00000000 0000000000000000 00000000&lt;/p&gt;

&lt;p&gt;　　00000000 00000000&lt;/p&gt;

&lt;p&gt;　　现在，数据的长度是448了，我们可以进行下一步操作。&lt;/p&gt;

&lt;p&gt;　　（二）补长度&lt;/p&gt;

&lt;p&gt;　　所谓的补长度是将原始数据的长度补到已经进行了补位操作的消息后面。通常用一个64位的数据来表示原始消息的长度。如果消息长度不大于2^64，那么第一个字就是0。在进行了补长度的操作以后，整个消息就变成下面这样了（16进制格式）&lt;/p&gt;

&lt;p&gt;　　61626380 0000000000000000 00000000&lt;/p&gt;

&lt;p&gt;　　00000000 0000000000000000 00000000&lt;/p&gt;

&lt;p&gt;　　00000000 0000000000000000 00000000&lt;/p&gt;

&lt;p&gt;　　00000000 0000000000000000 00000018&lt;/p&gt;

&lt;p&gt;　　如果原始的消息长度超过了512，我们需要将它补成512的倍数。然后我们把整个消息分成一个一个512位的数据块，分别处理每一个数据块，从而得到消息摘要。&lt;/p&gt;

&lt;p&gt;　　（三）使用的常量&lt;/p&gt;

&lt;p&gt;　　一系列的常量字K(0),K(1), … , K(79)，如果以16进制给出。它们如下：&lt;/p&gt;

&lt;p&gt;　　Kt = 0x5A827999 (0&amp;lt;= t &amp;lt;= 19)&lt;/p&gt;

&lt;p&gt;　　Kt = 0x6ED9EBA1 (20&amp;lt;= t &amp;lt;= 39)&lt;/p&gt;

&lt;p&gt;　　Kt = 0x8F1BBCDC (40&amp;lt;= t &amp;lt;= 59)&lt;/p&gt;

&lt;p&gt;　　Kt = 0xCA62C1D6 (60&amp;lt;= t &amp;lt;= 79).&lt;/p&gt;

&lt;p&gt;　　（四）需要使用的函数&lt;/p&gt;

&lt;p&gt;　　在SHA1中我们需要一系列的函数。每个函数ft (0 &amp;lt;= t &amp;lt;= 79)都操作32位字B，C，D并且产生32位字作为输出。ft(B,C,D)可以如下定义&lt;/p&gt;

&lt;p&gt;　　ft(B,C,D) = (B ANDC) or ((NOT B) AND D) ( 0 &amp;lt;= t &amp;lt;= 19)&lt;/p&gt;

&lt;p&gt;　　ft(B,C,D) = B XOR CXOR D (20 &amp;lt;= t &amp;lt;= 39)&lt;/p&gt;

&lt;p&gt;　　ft(B,C,D) = (B ANDC) or (B AND D) or (C AND D) (40 &amp;lt;= t &amp;lt;= 59)&lt;/p&gt;

&lt;p&gt;　　ft(B,C,D) = B XOR CXOR D (60 &amp;lt;= t &amp;lt;= 79).&lt;/p&gt;

&lt;p&gt;　　（五）计算消息摘要&lt;/p&gt;

&lt;p&gt;　　必须使用进行了补位和补长度后的消息来计算消息摘要。计算需要两个缓冲区，每个都由5个32位的字组成，还需要一个80个32位字的缓冲区。第一个5个字的缓冲区被标识为A，B，C，D，E。第二个5个字的缓冲区被标识为H0,H1, H2, H3, H4。80个字的缓冲区被标识为W0,W1,…, W79&lt;/p&gt;

&lt;p&gt;　　另外还需要一个一个字的TEMP缓冲区。&lt;/p&gt;

&lt;p&gt;　　为了产生消息摘要，在第4部分中定义的16个字的数据块M1,M2,…, Mn&lt;/p&gt;

&lt;p&gt;　　会依次进行处理，处理每个数据块Mi 包含80个步骤。&lt;/p&gt;

&lt;p&gt;　　在处理每个数据块之前，缓冲区{Hi} 被初始化为下面的值（16进制）&lt;/p&gt;

&lt;p&gt;　　H0 = 0x67452301&lt;/p&gt;

&lt;p&gt;　　H1 = 0xEFCDAB89&lt;/p&gt;

&lt;p&gt;　　H2 = 0x98BADCFE&lt;/p&gt;

&lt;p&gt;　　H3 = 0x10325476&lt;/p&gt;

&lt;p&gt;　　H4 = 0xC3D2E1F0.&lt;/p&gt;

&lt;p&gt;　　现在开始处理M1, M2,… , Mn。为了处理 Mi,需要进行下面的步骤&lt;/p&gt;

&lt;p&gt;　　(1). 将Mi 分成 16 个字 W0, W1, … , W15,W0 是最左边的字&lt;/p&gt;

&lt;p&gt;　　(2). 对于t = 16 到 79 令 Wt = S1(Wt-3 XOR Wt-8XOR Wt- 14 XOR Wt-16).&lt;/p&gt;

&lt;p&gt;　　(3). 令A = H0, B = H1, C = H2, D = H3, E = H4.&lt;/p&gt;

&lt;p&gt;　　(4) 对于t = 0 到 79，执行下面的循环&lt;/p&gt;

&lt;p&gt;　　TEMP = S5(A) +ft(B,C,D) + E + Wt + Kt;&lt;/p&gt;

&lt;p&gt;　　E = D; D = C; C =S30(B); B = A; A = TEMP;&lt;/p&gt;

&lt;p&gt;　　(5). 令H0 = H0 + A, H1 = H1 + B, H2 = H2 + C, H3 = H3 + D, H4 = H4 + E.&lt;/p&gt;

&lt;p&gt;　　在处理完所有的 Mn, 后，消息摘要是一个160位的字符串，以下面的顺序标识&lt;/p&gt;

&lt;p&gt;　　H0 H1 H2 H3 H4.&lt;/p&gt;

&lt;p&gt;　　对于SHA256、SHA384、SHA512。你也可以用相似的办法来计算消息摘要。对消息进行补位的算法完全是一样的。&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/04/sha.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/04/sha.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>raft</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;分布式系统的Raft算法&lt;/p&gt;

&lt;p&gt;　　过去, Paxos一直是分布式协议的标准，但是Paxos难于理解，更难以实现，Google的分布式锁系统Chubby作为Paxos实现曾经遭遇到很多坑。&lt;/p&gt;

&lt;p&gt;　　来自Stanford的新的分布式协议研究称为Raft，它是一个为真实世界应用建立的协议，主要注重协议的落地性和可理解性。&lt;/p&gt;

&lt;p&gt;　　在了解Raft之前，我们先了解Consensus一致性这个概念，它是指多个服务器在状态达成一致，但是在一个分布式系统中，因为各种意外可能，有的服务器可能会崩溃或变得不可靠，它就不能和其他服务器达成一致状态。这样就需要一种Consensus协议，一致性协议是为了确保容错性，也就是即使系统中有一两个服务器当机，也不会影响其处理过程。&lt;/p&gt;

&lt;p&gt;　　为了以容错方式达成一致，我们不可能要求所有服务器100%都达成一致状态，只要超过半数的大多数服务器达成一致就可以了，假设有N台服务器，N/2 +1 就超过半数，代表大多数了。&lt;/p&gt;

&lt;p&gt;　　Paxos和Raft都是为了实现Consensus一致性这个目标，这个过程如同选举一样，参选者需要说服大多数选民(服务器)投票给他，一旦选定后就跟随其操作。Paxos和Raft的区别在于选举的具体过程不同。&lt;/p&gt;

&lt;p&gt;　　在Raft中，任何时候一个服务器可以扮演下面角色之一：&lt;/p&gt;

&lt;p&gt;Leader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader.
Follower: 类似选民，完全被动
Candidate候选人: 类似Proposer律师，可以被选为一个新的领导人。
Raft阶段分为两个，首先是选举过程，然后在选举出来的领导人带领进行正常操作，比如日志复制等。下面用图示展示这个过程：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;任何一个服务器都可以成为一个候选者Candidate，它向其他服务器Follower发出要求选举自己的请求：&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;其他服务器同意了，发出OK。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;注意如果在这个过程中，有一个Follower当机，没有收到请求选举的要求，因此候选者可以自己选自己，只要达到N/2 + 1 的大多数票，候选人还是可以成为Leader的。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;这样这个候选者就成为了Leader领导人，它可以向选民也就是Follower们发出指令，比如进行日志复制。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;以后通过心跳进行日志复制的通知&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果一旦这个Leader当机崩溃了，那么Follower中有一个成为候选者，发出邀票选举。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Follower同意后，其成为Leader，继续承担日志复制等指导工作：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;值得注意的是，整个选举过程是有一个时间限制的，如下图：&lt;/p&gt;

&lt;p&gt;　　Splite Vote是因为如果同时有两个候选人向大家邀票，这时通过类似加时赛来解决，两个候选者在一段timeout比如300ms互相不服气的等待以后，因为双方得到的票数是一样的，一半对一半，那么在300ms以后，再由这两个候选者发出邀票，这时同时的概率大大降低，那么首先发出邀票的的候选者得到了大多数同意，成为领导者Leader，而另外一个候选者后来发出邀票时，那些Follower选民已经投票给第一个候选者，不能再投票给它，它就成为落选者了，最后这个落选者也成为普通Follower一员了。&lt;/p&gt;

&lt;p&gt;日志复制
　　下面以日志复制为例子说明Raft算法，假设Leader领导人已经选出，这时客户端发出增加一个日志的要求，比如日志是”sally”：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Leader要求Followe遵从他的指令，都将这个新的日志内容追加到他们各自日志中：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3.大多数follower服务器将日志写入磁盘文件后，确认追加成功，发出Commited Ok:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在下一个心跳heartbeat中，Leader会通知所有Follwer更新commited 项目。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于每个新的日志记录，重复上述过程。&lt;/p&gt;

&lt;p&gt;如果在这一过程中，发生了网络分区或者网络通信故障，使得Leader不能访问大多数Follwers了，那么Leader只能正常更新它能访问的那些Follower服务器，而大多数的服务器Follower因为没有了Leader，他们重新选举一个候选者作为Leader，然后这个Leader作为代表于外界打交道，如果外界要求其添加新的日志，这个新的Leader就按上述步骤通知大多数Followers，如果这时网络故障修复了，那么原先的Leader就变成Follower，在失联阶段这个老Leader的任何更新都不能算commit，都回滚，接受新的Leader的新的更新。&lt;/p&gt;

&lt;p&gt;总结：目前几乎所有语言都已经有支持Raft算法的库包，具体可参考：raftconsensus.github.io
CAP定理
分布式领域CAP理论，
Consistency(一致性), 数据一致更新，所有数据变动都是同步的
Availability(可用性), 好的响应性能
Partition tolerance(分区容错性) 可靠性&lt;/p&gt;

&lt;p&gt;定理：任何分布式系统只可同时满足二点，没法三者兼顾。
忠告：架构师不要将精力浪费在如何设计能满足三者的完美分布式系统，而是应该进行取舍。&lt;/p&gt;

&lt;p&gt;关系数据库的ACID模型拥有 高一致性 + 可用性 很难进行分区：
Atomicity原子性：一个事务中所有操作都必须全部完成，要么全部不完成。
Consistency一致性. 在事务开始或结束时，数据库应该在一致状态。
Isolation隔离层. 事务将假定只有它自己在操作数据库，彼此不知晓。
Durability. 一旦事务完成，就不能返回。
跨数据库事务：2PC (two-phase commit)， 2PC is the anti-scalability pattern (Pat Helland) 是反可伸缩模式的，JavaEE中的JTA事务可以支持2PC。因为2PC是反模式，尽量不要使用2PC，使用BASE来回避。&lt;/p&gt;

&lt;p&gt;BASE模型反ACID模型，完全不同ACID模型，牺牲高一致性，获得可用性或可靠性：
Basically Available基本可用。支持分区失败(e.g. sharding碎片划分数据库)
Soft state软状态 状态可以有一段时间不同步，异步。
Eventually consistent最终一致，最终数据是一致的就可以了，而不是时时高一致。&lt;/p&gt;

&lt;p&gt;BASE思想的主要实现有
1.按功能划分数据库
2.sharding碎片&lt;/p&gt;

&lt;p&gt;BASE思想主要强调基本的可用性，如果你需要High 可用性，也就是纯粹的高性能，那么就要以一致性或容错性为牺牲，BASE思想的方案在性能上还是有潜力可挖的。&lt;/p&gt;

&lt;p&gt;现在NOSQL运动丰富了拓展了BASE思想，可按照具体情况定制特别方案，比如忽视一致性，获得高可用性等等，NOSQL应该有下面两个流派：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Key-Value存储，如Amaze Dynamo等，可根据CAP三原则灵活选择不同倾向的数据库产品。&lt;/li&gt;
  &lt;li&gt;领域模型 + 分布式缓存 + 存储 （Qi4j和NoSql运动），可根据CAP三原则结合自己项目定制灵活的分布式方案，难度高。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这两者共同点：都是关系数据库SQL以外的可选方案，逻辑随着数据分布，任何模型都可以自己持久化，将数据处理和数据存储分离，将读和写分离，存储可以是异步或同步，取决于对一致性的要求程度。&lt;/p&gt;

&lt;p&gt;不同点：NOSQL之类的Key-Value存储产品是和关系数据库头碰头的产品BOX，可以适合非Java如PHP RUBY等领域，是一种可以拿来就用的产品，而领域模型 + 分布式缓存 + 存储是一种复杂的架构解决方案，不是产品，但这种方式更灵活，更应该是架构师必须掌握的。&lt;/p&gt;

&lt;p&gt;分布式Paxos算法
分布式系统Paxos算法&lt;/p&gt;

&lt;p&gt;　　这是一个有关Paxos算法非常形象的讲解与示范。Paxos是能够基于一大堆完全不可靠的网络条件下却能可靠确定地实现共识一致性的算法。也就是说：它允许一组不一定可靠的处理器（服务器）在某些条件得到满足情况下就能达成确定的安全的共识，如果条件不能满足也确保这组处理器（服务器）保持一致。&lt;/p&gt;

&lt;p&gt;什么是共识？
　　具体来说是这样：分布式系统中由于网络之间通讯可能会中断，虽然概率很低，但是没有100%完美的网络因此，依靠网络通讯的计算机之间要达成共识就比较困难，假设有X, Y和Z三台计算机谋划在周一攻击人类世界，它们的攻击计划是只要所有计算机可用于战斗时就一起进行攻击，不落下任何一台机器，但是当他们决定具体什么时间开始攻击时，在这个关键问题上往往会出错。&lt;/p&gt;

&lt;p&gt;　　一个基本问题是，每台机器都有自己的攻击时间建议，计算机X可以建议在08:00时间，因为这个时间正是周一早晨，而人们刚刚过完狂欢的周末休息天，但是计算机Z认为13:00比较好，理由当然也有一大堆，让这三台计算机基于某个时刻达成共识是非常困难的，因此，也给人类反击留下了机会。&lt;/p&gt;

&lt;p&gt;　　另外一个情况是，这三台计算机是位于世界不同的位置，之间通讯或许通过电缆或者其他不太可靠的网络设备通讯，如果X建议在08:00，它必须确认它的这个建议能够到达活着的Y和Z，以免一个人战斗。&lt;/p&gt;

&lt;p&gt;问题是：我们不能准确地知道某个计算机的延迟的原因：是因为性能慢了还是已经是彻底死机不能用？&lt;/p&gt;

&lt;p&gt;　　那么，X怎么知道其他两个计算机是可用的呢？也就是说，当X和其他两个计算机通讯发现得到响应要过很长时间，它不能确定这两台计算机到底还能不能继续活下去，也许这次通讯有延迟了，下一次它们又活过来了没有延迟了，也许下次延迟更长了一点，也许下次延迟稍微短了一点，这些随机概率问题使得X不能确定Y和Z到底是出了什么问题造成延迟的，是因为处理了某个特别耗费CPU的任务还是因为死锁等原因？当然，有些天真的设计者会说，只要我们将性能监控到位，如果延迟超过一定时间，我们人工介入告诉X确切情况就可以，那么这种人工介入的分布式系统不是一个天然自洽的自动化系统了，不在我们讨论范围之内，而且这样的系统会让人疲于奔命。&lt;/p&gt;

&lt;p&gt;　　因为X不能确定Y或Z是否可用，所以X仅仅只能和Y与Z中一台基于攻击时间达成共识，就无法完全三台机器全部投入战斗的计划。注意的是，X Y Z三台中任何一台都可能会出现延迟，这就造成了三台机器之间任何通讯都是无法确认可靠的，比如X发出消息给Z，Z确认后回执给X，但是这段时间X突然死机了，那么Z要等待X多长时间才能知道它收到确认呢？还是再次等待X回复确认的确认，这样无限循环下去也不能解决它们之间通讯可能出现随机不可靠的问题。&lt;/p&gt;

&lt;p&gt;　　所有关键问题在于：由于这三台机器之间通讯是无法保证100%可靠，它们就不能就任何事情达成共识。&lt;/p&gt;

&lt;p&gt;　　下面以分布式拍卖案例说明这种情况以及Paxos的基本原理？&lt;/p&gt;

&lt;p&gt;　　在传统拍卖场景中，价高者先得，这些拍卖者都是在同一个房间，彼此能够直接看得到对方的报价，如果我们假设分布式拍卖是将这些拍卖者分离到不同的地方，这样我们可以用拍卖者之间的联系模拟分布式计算机之间的通讯。&lt;/p&gt;

&lt;p&gt;　　假设拍卖者各自在自己家里拍卖，通过邮局信件发出自己的拍卖信息，拍卖者之间除非等到邮局投递人告诉他们彼此之间的报价，否则是无法知道对方报价的。如果邮局信件投递这个环节出了问题，投递速度慢了甚至无法投递了，那么整个拍卖程序就无法继续进行下去。&lt;/p&gt;

&lt;p&gt;Paxos解决共识思路
　　Paxos是一个解决共识问题consensus problem的算法，现实中Paxos的实现以及成为一些世界级软件的心脏，如Cassandra, Google的 Spanner数据库, 分布式锁服务Chubby. 一个被Paxos管理的系统实际上谈论的是值 状态和跟踪等问题，其目标是建造更高可用性和强一致性的分布式系统。&lt;/p&gt;

&lt;p&gt;　　Paxos完成一次写操作需要两次来回，分别是prepare/promise, 和 propose/accept：&lt;/p&gt;

&lt;p&gt;paxos&lt;/p&gt;

&lt;p&gt;　　第一次由提交者Leader向所有其他服务器发出prepare消息请求准备，所有服务器中大多数如果回复诺言承诺就表示准备好了，可以接受写入；第二次提交者向所有服务器发出正式建议propose，所有服务器中大多数如果回复已经接收就表示成功了。&lt;/p&gt;

&lt;p&gt;　　为了详细描述这个两段过程，首先让我们定义一下我们将使用的一些名词术语：&lt;/p&gt;

&lt;p&gt;一个进程是系统中计算机的一个. 人们使用有关复制或节点等词语表达，都差不多。
一个客户端是属于系统中一个成员的计算机，但是询问系统值是什么或者要求系统获取一个新的值。
　　Paxos构建分布式数据库的小片段: 它仅仅实现进程将一个新的东西精确地写入系统中，进程是由Paxos的一个实例管治，可以失败或者不知道任何东西、或者大多数进程都知道一个同样的值，这就是共识，Paxos并不真的告诉我们如何用它来构建数据库或类似的东西，它只是负责独立节点之间通讯的进程， 这些进程服务器会基于一个新值执行决定，Paxos会存储一个值数据，只是一次性的，一旦你第一次设置以后就不能再改变它。&lt;/p&gt;

&lt;p&gt;Paxo读操作
　　其实Paxos精华是在写操作，将读操作放在写操作前面讲解，是着重Paxos以大多数服务器达成共识为重要标志，通过读取判断是否达成共识这一状态。&lt;/p&gt;

&lt;p&gt;　　为了从Paxos系统中读取一个值数据，客户端会请求读取所有进程中存储的当前值，然后从大多数进程服务器中获得这个值，如果数量凑不够大多数或者没有足够的客户端响应，读取操作失败，下面图示你会看到一个客户端询问其他节点他们的值是多少，这些节点返回值给客户端，当客户端获得了大多数节点的响应，返回的值都是同样的，它就算成功地读操作了，并顺便保存读结果。&lt;/p&gt;

&lt;p&gt;　　与单节点系统(只有一台服务器)相比这有些奇怪，这两个系统中，客户端都需要观察系统已决定状态，但是在非分布式系统中像MySQL或一个memcached服务器中, 客户端只需直接向标准的状态存储的服务器地址获取状态即可，在简单的Paxos中, 客户端也是同样的方式观察状态，但是因为并没有标准的状态存储的服务器地址，它需要询问所有的成员，以便能够确定仅有一个会报告值数据，实际上是大多数节点都持有的值数据，如果客户端询问一个节点，有可能这个节点进程已经过期，得到了错误的值数据，进程失效过期的原因有很多：由于不可靠的网络导致本应送达到它们的消息丢失了；或者他们也许当机然后使用了一个过期状态恢复；或者算法还在运行计算中，进程并没有正好得到消息等等。在现实中使用Paxos实现时，其实不需要每个节点都进行一次读取，会有更好的读取方式，但是他们都是拓展的原始 Paxos 算法。&lt;/p&gt;

&lt;p&gt;Paxos写操作
　　当一个客户端要求写入系统一个新值时，让我们看看Paxos让我们集群的进程都做了什么？下面的过程都是只有一个值的写入，最终我们能用这个进程作为原始数据，允许值数据在彼此之间一个个设置，但是基本的Paxos算法管治了一个新值数据的写操作流程， 然后做重复的事情。&lt;/p&gt;

&lt;p&gt;　　首先Paxos管理的系统中一个客户端要求写入一个新值，客户端这里如图所示是红圈，其它进程是蓝圈， Paxos能保证客户端发送它们的写请求到Paxos集群中任何成员, 这里演示中客户端随机挑选进程中任意一个，这种方式是重要且巧妙的，意味著没有任何单点风险，意味着我们的Paxos管治系统能继续保持在线可用，无论任何一个节点当机或其他不可用原因无响应。如果我们设计一个特定节点作为“推荐人proposer”或者 “the master” 等, 如果这个主节点死机，那么整个系统就崩溃了。&lt;/p&gt;

&lt;p&gt;　　当写请求被接受后，Paxos进程会接受这个写新值到系统中请求“建议”， “建议”是Paxos中一个正式概念: 向一个Paxos管治的系统建议可能会成功或失败，需要步骤来确保共识能够达成维系，这个建议以准备消息从那些与客户端连接的进程节点们被发往整个系统。&lt;/p&gt;

&lt;p&gt;序列号
　　这个准备消息保存在被建议的值数据中，它们也称为序列号sequence number，序列号是由建议进程产生的，它定义了接受进程应该准备接受带有序列号的建议，这个序列号是关键: 它用于表明新旧建议之间的区别，如果两个进程试图获得需要设置一个值，Paxos认为最后一个进程应该有优先权，这样让进程分辨哪个是最后一个，这样它就能设置最新的值。&lt;/p&gt;

&lt;p&gt;　　这些接受的进程能够进行在系统中关键的检查：这个在到来的准备消息中序列号是我见过的最高级别吗？如果是，那就很cool, 我能准备好接受将要到来的值数据，那就不要管之前听到的任何其他值数据了，你能看到这个过程在右边演示中：客户端每隔一段向一个进程建议一个新值，这个进程发送准备消息给其他进程，然后那些进程注意到这是一个成功的更高的超过旧的新序列号，然后就放手那些旧建议。&lt;/p&gt;

&lt;p&gt;　　这里有一个顺序的设计(先发送准备消息)，这是为避免单点风险，如果没有这个顺序，Paxos中成员就无法分辨哪个建议是他们可以有信心地准备接受的。&lt;/p&gt;

&lt;p&gt;　　我们不能想象有另外不同的共识算法，不是按照如下步骤：首先发送第一个消息询问其他进程，以确保将设置的新值是最新的值，尽管方式可以再简单些，但是可能就不能满足共识算法安全的需求了，如果两个进程正好同时建议不同的值，如下所示：&lt;/p&gt;

&lt;p&gt;　　大自然经常会这样欺骗我们，每个包都能另外一半的进程相信它们接受的也许是正确也许是错误的值，系统将进入一个僵局，存在两个相同数量的组却有不同的值，那么就无法确定大多数这个概念了，这个僵局能够被第一个Paxos消息避免，因为Paxos的序列号，那些有问题的建议将有被其他更低的序列号，这样序列号更高的建议就会毫不含糊地被大多数进程接收，它们也首先获得了更高的序列号，然后如果接受到更低的序列号就会拒绝，Paxos 就是这样通过用序列号控制整个系统的时间节奏。&lt;/p&gt;

&lt;p&gt;上图演示了客户端首先发一个准备消息给Paxos进程，Paxos进程会检查下一步将到来的建议的序列号，以分辨是否准备接受这个新值，所有进程都是这样消除歧义，共识由此达成。
　　注意：保证没有两个建议使用相同的序列号是很重要的，这是确保他们的顺序，这样每个序列号只有一个建议，这样才能比较两个建议，实现Paxos时，全局唯一有序的序列号实际是精确系统时间和集群中节点数量的拷贝，随着时间不断增加，从来不会重复。&lt;/p&gt;

&lt;p&gt;Paxos第一阶段：准备Perpare/诺言Promises
　　Paxos的第一阶段是prepare/promise，准备阶段就是将建议值发送到各个目标节点。&lt;/p&gt;

&lt;p&gt;　　当建议被发到目标节点后，进程会会检查建议中的序列号，是否是它们见到过的最高级，如果是最高级，它们会发出一个promise不再接受比这个新序列号更旧的建议了，这个诺言promise作为消息从许下诺言的进程发到提交建议新值的进程服务器，这个诺言消息给提交建议的进程后，提交建议的进程需要自己统计一下有多少其他进程已经发回它们的诺言promise了，如果判断数量上是否达到大多数？如果大多数进程已经同意接受这个建议或者更高级序列号的建议，这个提交建议的进程就能知道它获得了发言权(因为有大多数支持)，这样就开始讲话，算法中的下一步处理将可能进行；如果回复诺言的节点数量没有达到大多数，也就是共识没有达成，这样这个节点提交的建议将退出，客户端要求的写操作失败。&lt;/p&gt;

&lt;p&gt;　　为了决定一个建议是否已经有足够的回复诺言promises, 提交建议者只是统计一下它接受到的 promise 消息数量，然后和整个系统中节点服务器数量比较一下，“足够”意味着大多数(N/2 + 1)个进程服务器在某段时间内都回复了诺言promises。如果超过一半的进程服务器没有返回诺言，这意味着这个建议没有被大多数通过，那么在前面描述的读算法中就不能满足大多数的要求，也就不能达成共识，这个建议就退出。其他包括网络分区错误也可能会阻止大多数达成共识，&lt;/p&gt;

&lt;p&gt;第二阶段：Paoxs接纳Acceptance
　　当完成prepare/promise阶段，进入了 propose/accept阶段。&lt;/p&gt;

&lt;p&gt;　　一旦建议提交者已经从大多数其他进程服务器获得了诺言，它会要求许诺的进程服务器接收它们之前承诺接受的新值数据，这是一个“确认commit”阶段，如果没有冲突建议 失败或分区错误，那么这个新建议将被所有其他节点接受，那么Paxos过程就完成了。&lt;/p&gt;

&lt;p&gt;　　你能看到右边的演示，注意这个演示比上面promise在最后多了一个动作，也就是提交建议者将新值发给那些许诺言的进程服务器，让它们接受了这个新值。&lt;/p&gt;

&lt;p&gt;　　接受的过程也许可能会发生失败，在回复了诺言消息以后，在接受到Accept消息之前，如果有足够多的服务器正好在这个时间段失败，那么接受行为只能是少数服务器，那么Paxos进入了厄运状态：一些进程服务器接受了新值，而不是全部的，这种不一致已经在前面读操作中描述：一个客户端试图从系统中大多数节点服务器读取它们同意接受的值，它发现一些节点服务器报告有不同的值数据，这会引起读失败，但是Paxos还保持一致性，不允许在没有达成共识情况下任何写操作发生，这种坏的情况在实践中经常通过重复接受阶段来让大多数节点最终接受。&lt;/p&gt;

&lt;p&gt;总结
　　Paxos算法是保证在分布式系统中写操作能够顺利进行，保证系统中大多数状态是一致的，没有机会看到不一致，因此，Paxos算法的特点是一致性&amp;gt;可用性。&lt;/p&gt;

&lt;p&gt;　　vector clock向量时钟是另外一种保证复制的算法，其特点是可用性&amp;gt;一致性，但是，一旦发生冲突，不像Paxos能自行解决，需要人工干预编写代码解决。&lt;/p&gt;

&lt;p&gt;　　Paxos算法和Vector Clock都是由Leslie Lamport提出。
ZooKeeper在服务发现中应用
1.CAP原理
要想数据高可用，就得写多份数据&lt;/p&gt;

&lt;p&gt;写多分数据就会导致数据一致性问题&lt;/p&gt;

&lt;p&gt;数据一致性问题会引起性能问题&lt;/p&gt;

&lt;p&gt;2.一致性模型
弱一致性&lt;/p&gt;

&lt;p&gt;最终一致性（一段时间达到一致性）&lt;/p&gt;

&lt;p&gt;强一致&lt;/p&gt;

&lt;p&gt;1、2 异步冗余；3是同步冗余&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;扩展服务的方案
数据分区： uid % 16&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;数据镜像：让多有的服务器都有相同的数据，提供相当的服务（冗余存储，一般3份为好）&lt;/p&gt;

&lt;p&gt;4.两种方案的事务问题
A向B汇钱，两个用户不在一个服务器上&lt;/p&gt;

&lt;p&gt;镜像：在不同的服务器上对同一数据的写操作如何保证一致性。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;解决一致性事务问题的技术&lt;/li&gt;
  &lt;li&gt;Master -Slave
读写请求由Master负责&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;写请求写到Master后，由Master同步到Slave上&lt;/p&gt;

&lt;p&gt;由Master push or Slave pull&lt;/p&gt;

&lt;p&gt;通常是由Slave 周期性来pull，所以是最终一致性&lt;/p&gt;

&lt;p&gt;问题： 若在 pull 周期内（不是期间？），master挂掉，那么会导致这个时间片内的数据丢失&lt;/p&gt;

&lt;p&gt;若不想让数据丢掉，Slave 只能成为 ReadOnly方式等Master恢复&lt;/p&gt;

&lt;p&gt;若容忍数据丢失，可以让 Slave代替Master工作&lt;/p&gt;

&lt;p&gt;如何保证强一致性？&lt;/p&gt;

&lt;p&gt;Master 写操作，写完成功后，再写 Slave，两者成功后返回成功。若 Slave失败，两种方法&lt;/p&gt;

&lt;p&gt;标记 Slave 不可用报错，并继续服务（等恢复后，再同步Master的数据，多个Slave少了一个而已）&lt;/p&gt;

&lt;p&gt;回滚自己并返回失败&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Master-Master
数据同步一般是通过 Master 间的异步完成，所以是最终一致&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;好处： 一台Master挂掉，另外一台照样可以提供读写服务。当数据没有被赋值到别的Master上时，数据会丢失。&lt;/p&gt;

&lt;p&gt;对同一数据的处理问题：Dynamo的Vector Clock的设计（记录数据的版本号和修改者），当数据发生冲突时，要开发者自己来处理&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;3.两阶段提交  Two  Phase Commit   _ 2PC
第一阶段：针对准备工作&lt;/p&gt;

&lt;p&gt;协调者问所有节点是否可以执行提交&lt;/p&gt;

&lt;p&gt;参与者开始事务，执行准备工作：锁定资源（获取锁操作）&lt;/p&gt;

&lt;p&gt;参与者响应协调者，如果事务的准备工作成功，则回应”可以提交”，否则，拒绝提交&lt;/p&gt;

&lt;p&gt;第二阶段：&lt;/p&gt;

&lt;p&gt;若都响应可以提交，则协调者项多有参与者发送正式提交的命令（更新值），参与者完成正式提交，释放资源，回应完成。协调者收到所有节点的完成响应后结束这个全局事务.。若参与者回应拒绝提交，则协调者向所有的参与者发送回滚操作，并释放资源，当收到全部节点的回滚回应后，取消全局事务&lt;/p&gt;

&lt;p&gt;存在的问题：若一个没提交，就会进行回滚&lt;/p&gt;

&lt;p&gt;第一阶段：若消息的传递未接收到，则需要协调者作超时处理，要么当做失败，要么重载&lt;/p&gt;

&lt;p&gt;第二阶段：若参与者的回应超时，要么重试，要么把那个参与者即为问题节点，提出整个集群&lt;/p&gt;

&lt;p&gt;在第二阶段中，参与者未收到协调者的指示（也许协调者挂掉），则所有参与者会进入“不知所措” 的状态（但是已经锁定了资源），所以引入了三段提交&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;三段提交：把二段提交的第一阶段 break 成了两段
询问&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;锁定资源（获取锁）&lt;/p&gt;

&lt;p&gt;提交&lt;/p&gt;

&lt;p&gt;核心理念：在询问的时候并不锁定资源，除非所有人都同意了，才开始锁定&lt;/p&gt;

&lt;p&gt;好处：当发生了失败或超时时，三段提交可以继续把状态变为Commit 状态，而二段提交则不知所措？&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Raxos 算法（少数服从多数）
解决的问题：在一个可能发生异常的分布式系统中如何就某个值达成一致，让整个集群的节点对某个值的变更达成一致&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;任何一个节点都可以提出要修改某个数据的提案,是否通过这个提案取决于这个集群中是否有超过半数的节点同意（所以节点数总是单数）—— 版本标记。虽然一致性，但是只能对一个操作进行操作啊？？&lt;/p&gt;

&lt;p&gt;当一个Server接收到比当前版本号小的提案时，则拒绝。当收到比当前大的版本号的提案时，则锁定资源，进行修改，返回OK.   也就是说收到超过一半的最大版本的提案才算成功。&lt;/p&gt;

&lt;p&gt;核心思想：&lt;/p&gt;

&lt;p&gt;在抢占式访问权的基础上引入多个acceptor，也就是说当一个版本号更大的提案可以剥夺版本号已经获取的锁。&lt;/p&gt;

&lt;p&gt;后者认同前者的原则：&lt;/p&gt;

&lt;p&gt;在肯定旧epoch 无法生成确定性取值时，新的 epoch 会提交自己的valu&lt;/p&gt;

&lt;p&gt;一旦 旧epoch形成确定性取值，新的 epoch肯定可以获取到此取值，并且会认同此取值，不会被破坏。&lt;/p&gt;

&lt;p&gt;步骤&lt;/p&gt;

&lt;p&gt;P1 请求Acceptor的 #1,Acceptor 这时并没有其他线程获取到锁，所以把锁交给 P1，并返回这时 #1 的值为null&lt;/p&gt;

&lt;p&gt;然后 P1 向 第一个 Acceptor 提交 #1 的值，Acceptor 接受并返回 OK&lt;/p&gt;

&lt;p&gt;这个时候，P2向Acceptor请求#1上的锁，因为版本号更大，所以直接抢占了 P1 的锁。这时 Acceptor 返回了 OK并且返回了 #1 的值&lt;/p&gt;

&lt;p&gt;这时 P1 P向 后面两个 Acceptor 提交 #1 的值，但是由于中间的那个Acceptor 版本号已经更改为 2 了，所以拒绝P1。第三个 Acceptor 接受了，并且返回了 OK&lt;/p&gt;

&lt;p&gt;由于后者认同前者的原则，这时 P1 已经形成确定性取值了 V1 了，这时新的 P2 会认同此取值，而不是提交自己的取值。所以，P2会选择最新的那个取值 也就是V1 进行提交。这时Acceptor 返回 OK&lt;/p&gt;

&lt;p&gt;6.ZAB 协议 ( Zookeeper Atomic  Broadcast) 原子广播协议：保证了发给各副本的消息顺序相同
定义：原子广播协议 ZAB 是一致性协议，Zookeeper 把其作为数据一致性的算法。ZAB 是在 Paxos 算法基础上进行扩展而来的。Zookeeper 使用单一主进程 Leader用于处理客户端所有事务请求，采用 ZAB 协议将服务器状态以事务形式广播到所有 Follower 上，由于事务间可能存在着依赖关系，ZAB协议保证 Leader 广播的变更序列被顺序的处理，一个状态被处理那么它所依赖的状态也已经提前被处理&lt;/p&gt;

&lt;p&gt;核心思想：保证任意时刻只有一个节点是Leader，所有更新事务由Leader发起去更新所有副本 Follower，更新时用的是 两段提交协议，只要多数节点 prepare 成功，就通知他们commit。各个follower 要按当初 leader 让他们 prepare 的顺序来 apply 事务&lt;/p&gt;

&lt;p&gt;协议状态&lt;/p&gt;

&lt;p&gt;Looking:系统刚启动时 或者 Leader 崩溃后正处于选举状态&lt;/p&gt;

&lt;p&gt;Following：Follower 节点所处的状态，Follower与 Leader处于数据同步状态&lt;/p&gt;

&lt;p&gt;Leading：Leader 所处状态，当前集群中有一个 Leader 为主进程&lt;/p&gt;

&lt;p&gt;ZooKeeper启动时所有节点初始状态为Looking，这时集群会尝试选举出一个Leader节点，选举出的Leader节点切换为Leading状态；当节点发现集群中已经选举出Leader则该节点会切换到Following状态，然后和Leader节点保持同步；当Follower节点与Leader失去联系时Follower节点则会切换到Looking状态，开始新一轮选举；在ZooKeeper的整个生命周期中每个节点都会在Looking、Following、Leading状态间不断转换。&lt;/p&gt;

&lt;p&gt;选举出Leader节点后 ZAB 进入原子广播阶段，这时Leader为和自己同步每个节点 Follower 创建一个操作序列，一个时期一个 Follower 只能和一个Leader保持同步&lt;/p&gt;

&lt;p&gt;阶段&lt;/p&gt;

&lt;p&gt;Election： 在 Looking状态中选举出 Leader节点，Leader的LastZXID总是最新的（只有lastZXID的节点才有资格成为Leade,这种情况下选举出来的Leader总有最新的事务日志）。在选举的过程中会对每个Follower节点的ZXID进行对比只有highestZXID的Follower才可能当选Leader&lt;/p&gt;

&lt;p&gt;每个Follower都向其他节点发送选自身为Leader的Vote投票请求，等待回复；&lt;/p&gt;

&lt;p&gt;Follower接受到的Vote如果比自身的大（ZXID更新）时则投票，并更新自身的Vote，否则拒绝投票；&lt;/p&gt;

&lt;p&gt;每个Follower中维护着一个投票记录表，当某个节点收到过半的投票时，结束投票并把该Follower选为Leader，投票结束；&lt;/p&gt;

&lt;p&gt;Discovery:Follower 节点向准 Leader推送 FollwerInfo,该信息包含了上一周期的epoch，接受准 Leader 的 NEWLEADER 指令&lt;/p&gt;

&lt;p&gt;Sync：将 Follower 与 Leader的数据进行同步，由Leader发起同步指令，最终保持数据的一致性&lt;/p&gt;

&lt;p&gt;Broadcast：Leader广播 Proposal 与 Commit，Follower 接受 Proposal 与 commit。因为一个时刻只有一个Leader节点，若是更新请求，只能由Leader节点执行（若连到的是 Follower 节点，则需转发到Leader节点执行；读请求可以从Follower 上读取，若是要最新的数据，则还是需要在 Leader上读取）&lt;/p&gt;

&lt;p&gt;消息广播使用了TCP协议进行通讯所有保证了接受和发送事务的顺序性。广播消息时Leader节点为每个事务Proposal分配一个全局递增的ZXID（事务ID），每个事务Proposal都按照ZXID顺序来处理（Paxos 保证不了）&lt;/p&gt;

&lt;p&gt;Leader节点为每一个Follower节点分配一个队列按事务ZXID顺序放入到队列中，且根据队列的规则FIFO来进行事务的发送。&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;Recovery ：根据Leader的事务日志对Follower 节点数据进行同步更新&lt;/p&gt;

&lt;p&gt;同步策略：&lt;/p&gt;

&lt;p&gt;SNAP ：如果Follower数据太老，Leader将发送快照SNAP指令给Follower同步数据；&lt;/p&gt;

&lt;p&gt;DIFF ：Leader发送从Follolwer.lastZXID到Leader.lastZXID议案的DIFF指令给Follower同步数据；&lt;/p&gt;

&lt;p&gt;TRUNC ：当Follower.lastZXID比Leader.lastZXID大时，Leader发送从Leader.lastZXID到Follower.lastZXID的TRUNC指令让Follower丢弃该段数据；（当老Leader在Commit前挂掉，但是已提交到本地）&lt;/p&gt;

&lt;p&gt;Follower将所有事务都同步完成后Leader会把该节点添加到可用Follower列表中；&lt;/p&gt;

&lt;p&gt;Follower接收Leader的NEWLEADER指令，如果该指令中epoch比当前Follower的epoch小那么Follower转到Election阶段&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Raft 算法
Raft 算法也是一种少数服从多数的算法，在任何时候一个服务器可以扮演以下角色之一：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Leader：负责 Client 交互 和 log 复制，同一时刻系统中最多存在一个&lt;/p&gt;

&lt;p&gt;Follower：被动响应请求 RPC，从不主动发起请求 RPC&lt;/p&gt;

&lt;p&gt;Candidate : 由Follower 向Leader转换的中间状态&lt;/p&gt;

&lt;p&gt;在选举Leader的过程中，是有时间限制的，raft 将时间分为一个个 Term，可以认为是“逻辑时间”：&lt;/p&gt;

&lt;p&gt;每个 Term中至多存在1个 Leader&lt;/p&gt;

&lt;p&gt;某些 Term由于不止一个得到的票数一样，就会选举失败，不存在Leader。则会出现 Split Vote  ，再由候选者发出邀票&lt;/p&gt;

&lt;p&gt;每个 Server 本地维护 currentTerm&lt;/p&gt;

&lt;p&gt;选举过程：&lt;/p&gt;

&lt;p&gt;自增 CurrentTerm，由Follower 转换为 Candidate，设置 votedFor 为自身，并行发起 RequestVote RPC,不断重试，直至满足下列条件之一为止：&lt;/p&gt;

&lt;p&gt;获得超过半数的Server的投票，转换为 Leader，广播 HeatBeat&lt;/p&gt;

&lt;p&gt;接收到 合法 Leader 的 AppendEnties RPC，转换为Follower&lt;/p&gt;

&lt;p&gt;选举超时，没有 Server选举成功，自增 currentTerm ,重新选举&lt;/p&gt;

&lt;p&gt;当Candidate 在等待投票结果的过程中，可能会接收到来自其他Leader的 AppendEntries RPC ,如果该 Leader 的 Term 不小于本地的 Current Term，则认可该Leader身份的合法性，主动降级为Follower，反之，则维持 candida 身份继续等待投票结果&lt;/p&gt;

&lt;p&gt;Candidate 既没有选举成功，也没有收到其他 Leader 的 RPC (多个节点同时发起选举，最终每个 Candidate都将超时)，为了减少冲突，采取随机退让策略，每个 Candidate 重启选举定时器&lt;/p&gt;

&lt;p&gt;日志更新问题：&lt;/p&gt;

&lt;p&gt;如果在日志复制过程中，发生了网络分区或者网络通信故障，使得Leader不能访问大多数Follwers了，那么Leader只能正常更新它能访问的那些Follower服务器，而大多数的服务器Follower因为没有了Leader，他们重新选举一个候选者作为Leader，然后这个Leader作为代表于外界打交道，如果外界要求其添加新的日志，这个新的Leader就按上述步骤通知大多数Followers，如果这时网络故障修复了，那么原先的Leader就变成Follower，在失联阶段这个老Leader的任何更新都不能算commit，都回滚，接受新的Leader的新的更新。&lt;/p&gt;

&lt;p&gt;流程：&lt;/p&gt;

&lt;p&gt;Client 发送command 命令给 Leader&lt;/p&gt;

&lt;p&gt;Leader追加日志项，等待 commit 更新本地状态机，最终响应 Client&lt;/p&gt;

&lt;p&gt;若 Client超时，则不断重试，直到收到响应为止（重发 command，可能被执行多次，在被执行但是由于网络通信问题未收到响应）&lt;/p&gt;

&lt;p&gt;解决办法：Client 赋予每个 Command唯一标识，Leader在接收 command 之前首先检查本地log&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;paxos 算法与 raft 算法的差异
raft强调是唯一leader的协议，此leader至高无上&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;raft：新选举出来的leader拥有全部提交的日志，而 paxos 需要额外的流程从其他节点获取已经被提交的日志，它允许日志有空洞&lt;/p&gt;

&lt;p&gt;相同点：得到大多数的赞成，这个 entries 就会定下来，最终所有节点都会赞成&lt;/p&gt;

&lt;p&gt;NWR模型
N： N个备份&lt;/p&gt;

&lt;p&gt;W：要写入至少 w 份才认为成功&lt;/p&gt;

&lt;p&gt;R : 至少读取 R 个备份&lt;/p&gt;

&lt;p&gt;W+ R &amp;gt; N    ——&amp;gt;    R &amp;gt; N - W(未更新成功的) ，代表每次读取，都至少读取到一个最新的版本（更新成功的），从而不会读到一份旧数据&lt;/p&gt;

&lt;p&gt;问题：并非强一致性，会出现一些节点上的数据并不是最新版本，但却进行了最新的操作&lt;/p&gt;

&lt;p&gt;版本冲突问题：矢量钟 Vector Clock ： 谁更新的我，我的版本号是什么（对于同一个操作者的同一操作，版本号递增）&lt;/p&gt;

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;http://www.tuicool.com/articles/IfQR3u3&lt;/p&gt;

&lt;p&gt;http://blog.csdn.net/chen77716/article/details/7309915&lt;/p&gt;

&lt;p&gt;http://www.infoq.com/cn/articles/distributed-system-transaction-processing/&lt;/p&gt;

&lt;p&gt;http://www.jdon.com/artichect/raft.html&lt;/p&gt;

&lt;p&gt;http://blog.csdn.net/cszhouwei/article/details/38374603&lt;/p&gt;

</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/jekyll/2018/01/04/raft.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/jekyll/2018/01/04/raft.html</guid>
        
        
        <category>jekyll</category>
        
      </item>
    
      <item>
        <title>比较raft ，basic paxos以及multi-paxos</title>
        <description>&lt;p&gt;深入浅出的来讲清楚这些协议的两个问题：&lt;/p&gt;

&lt;p&gt;这些协议是用来干什么的
这些协议有什么区别，为何有这些区别&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;首先来说说Paxos
作为basic paxos和multi-paxos，绕不过去的概念就是先得说说paxos，先来理理基本概念：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;paxos并不指代一个协议，而是一类协议的统称，比较常见的paxos类协议有：basic paxos ， 以及 multi-paxos
paxos的目的是为了多个参与者达成一致观点
paxos基于一个原则，参与者不能阳奉阴违，一致的观点不会在传递过程中反转
paxos协议有一堆角色和限制，因为经统计87%的读者都会在阅读paxos的限制时就点X，这篇文章会直接跳过paxos的其他内容，因为这些内容并不有助于理解multi paxos和basic paxos&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;basic paxos vs multi-paxos
先说结论，basic paxos 的协议更复杂，且相对效率较低。所以现在所有的和paxos有关的协议，一定是基于multi-paxos来实现的。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2.1 从basic paxos的两阶段协议开始：
basic paxos的角色中，谨记一个原则：每个acceptor维护了自己知道的最大的协议号 vx，且每个acceptor不会同意任意一项小于等于vx的协议的prepare请求。&lt;/p&gt;

&lt;p&gt;basic paxos是由client发起的同步过程，在两阶段返回前，client不能得到成功的返回。&lt;/p&gt;

&lt;p&gt;第一阶段a（发送prepare），proposer向acceptor提出一个协议，这里的协议可以理解为client发送过来期望多节点一起保存的一致性内容，举例：一句日志，某个配置等
第一阶段b（计算协议vn），根据半数以上acceptor的返回，选择 max{va,vb,vc} = vn，这里的vx理解为这个acceptor已知的最大协议号，acceptor一旦返回了vx后，则表明：
acceptor在接下来的prepare请求中，会返回的vx自增1
acceptor不会accept任何小于vx的协议请求，只会accept大于vx的协议请求
第二阶段a（发送决议好的vn），把vn发送给acceptor
第二阶段b，在半数acceptor返回了成功后，再返回client成功通过协议
引用wiki上的流程图：&lt;/p&gt;

&lt;p&gt;Client   Proposer      Acceptor     Learner
   |         |          |  |  |       |  |
   X——–&amp;gt;|          |  |  |       |  |  Request
   |         X———&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Prepare(1)
   |         |&amp;lt;———X–X–X       |  |  Promise(1,{Va,Vb,Vc})
   |         X———&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Accept!(1,Vn)
   |         |&amp;lt;———X–X–X——&amp;gt;|-&amp;gt;|  Accepted(1,Vn)
   |&amp;lt;———————————X–X  Response
   |         |          |  |  |       |  |
2.2 有助于理解basic paxos 协议的小细节
proposer是可以有多个，所以他叫做proposer，而不叫leader
proposer并不强制要发送propose到全部acceptor，也可以发送70%的acceptor，只要通过的有半数以上，就认为协议是成功的
容易看出，第二阶段的时候client仍需要等着，只有在第二阶段大半acceptor返回accepted后，client才能得到成功的信息
有一些错误场景中，proposer会互相锁住对方的递交，详细可以看wiki，这里不多阐述：Paxos (computer science)
2.3 Multi-paxos 如何让自己看起来像一阶段
multi-paxos 在basic paxos的二阶段上引入了一个机制，让自己看起来像一阶段一样，是如何做到的？让我们回顾basic paxos协议的关键内容：&lt;/p&gt;

&lt;p&gt;多个proposer
max{va,vb,vc} = vn
这两个关键点导致的后果就是，在一阶段得到vn后，proposer并不可以肯定，accept(vn) 命令会在多数acceptor中成功执行，一个极端的例子如下：&lt;/p&gt;

&lt;p&gt;Client   Proposer        Acceptor     Learner
   |      |             |  |  |       |  |
   X—–&amp;gt;|             |  |  |       |  |  Request
   |      X————&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Prepare(1)
   |      |&amp;lt;————X–X–X       |  |  Promise(1,{null,null,null})
   |      !             |  |  |       |  |  !! LEADER FAILS
   |         |          |  |  |       |  |  !! NEW LEADER (knows last number was 1)
   |         X———&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Prepare(2)
   |         |&amp;lt;———X–X–X       |  |  Promise(2,{null,null,null})
   |      |  |          |  |  |       |  |  !! OLD LEADER recovers
   |      |  |          |  |  |       |  |  !! OLD LEADER tries 2, denied
   |      X————&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Prepare(2)
   |      |&amp;lt;————X–X–X       |  |  Nack(2)
   |      |  |          |  |  |       |  |  !! OLD LEADER tries 3
   |      X————&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Prepare(3)
   |      |&amp;lt;————X–X–X       |  |  Promise(3,{null,null,null})
   |      |  |          |  |  |       |  |  !! NEW LEADER proposes, denied
   |      |  X———&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Accept!(2,Va)
   |      |  |&amp;lt;———X–X–X       |  |  Nack(3)
   |      |  |          |  |  |       |  |  !! NEW LEADER tries 4
   |      |  X———&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Prepare(4)
   |      |  |&amp;lt;———X–X–X       |  |  Promise(4,{null,null,null})
   |      |  |          |  |  |       |  |  !! OLD LEADER proposes, denied
   |      X————&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Accept!(3,Vb)
   |      |&amp;lt;————X–X–X       |  |  Nack(4)
   |      |  |          |  |  |       |  |  … and so on …
因为其他proposer的存在，大家交替propose的结果就是所有的prepare计算得到的vn，全部中途作废，accept的动作一个都没正常执行。
显然，这样的决议过程是正确且低效的。&lt;/p&gt;

&lt;p&gt;如何让basic paxos得以进行一阶段的递交，最最重点的地方聚焦在了一个点：假如只允许有一个proposer&lt;/p&gt;

&lt;p&gt;multi-paxos将集群状态分成了两种：&lt;/p&gt;

&lt;p&gt;选主状态，由集群中的任意节点拉票发起选主，拉票中带上自己的vx，通过收集集群中半数以上的vx，来更新自己的vx值，得到目前集群通过的最大vx = vn
强leader状态，leader对vn的演变了如指掌，每次把vn的值直接在一阶段中发送给acceptor，和basic paxos的区别：basic paxos一阶段的时候，proposer对vn的值一无所知，要依赖一阶段的结果来算vn
一些有趣的细节：为何选主的时候，半数的vx就可以确定集群最大vn？&lt;/p&gt;

&lt;p&gt;因为在vn生成过程中，必须达成通知到半数节点的目的，vn才能成功生成，所以一定有半数以上的节点知道vn
选主的时候的限制恰好对应vn生成时的限制，这是一个环环相扣的证明
multi-paxos强leader状态的流程图：&lt;/p&gt;

&lt;p&gt;Client     Leader       Acceptor     Learner
   |         |          |  |  |       |  |  — Following Requests —
   X——–&amp;gt;|          |  |  |       |  |  Request
   |         X———&amp;gt;|-&amp;gt;|-&amp;gt;|       |  |  Accept!(N,I+1,W)(prepare）
   |         |&amp;lt;———X–X–X——&amp;gt;|-&amp;gt;|  Accepted(N,I+1,W)(prepared)
   |&amp;lt;———————————X–X  Response
   |         |          |  |  |       |  |
2.4 multi-paxos 一些有趣的小细节：
流程图中没有了basic paxos的两阶段，变成了一个一阶段的递交协议：
一阶段a：发起者（leader）直接告诉Acceptor，准备递交协议号为I+1的协议
一阶段b：收到了大部分acceptor的回复后（图中是全部），acceptor就直接回复client协议成功写入
wiki中写的Accept方法，我更愿意把它当做prepare，因为如果没有半数返回，该协议在超时后会返回失败，这种情况下，I+1这个协议号并没有通过，在下个请求是仍是使用I+1这个协议号
看过上一篇：raft 经典场景分析 - 知乎专栏 的同学已经发现了，multi-paxos在选定了leder后的行为和raft一模一样，那么multi-paxos和raft有什么区别呢，且看下一节分解。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;raft vs multi-paxos
看到这里的，说明对分布式一致性的学习绝对是真爱。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在我的上一篇文章：raft 经典场景分析 - 知乎专栏 中，已经详细讨论了raft的 选主和日志复制，而在上一节中，我们惊喜的发现multi-paxos和raft，在选定了leader状态下的行为模式一模一样，在这一节中，主要将比较它们在选主和日志复制上的行为。&lt;/p&gt;

&lt;p&gt;注意：raft的日志复制，等同我们在上文讨论的协议，为叙述方便，现在这一节，我们把multi-paxos的一阶段协议递交成为日志复制。&lt;/p&gt;

&lt;p&gt;结论也同样隐藏在我上一篇文章中：&lt;/p&gt;

&lt;p&gt;raft仅允许日志最多的节点当选为leader，而multi-paxos则相反，任意节点都可以当选leader
raft不允许日志的空洞，这也是为了比较方便和拉平两个节点的日志方便，而multi-paxos则允许日志出现空洞
这两个不同的设定反应到日志中的区别可以看如下图：&lt;/p&gt;

&lt;p&gt;如图，multi-paxos日志中间允许出现空洞，而multi-paxos的leader节点也会异步的查询其他节点来填补自身日志空洞&lt;/p&gt;

&lt;p&gt;如图：raft的典型日志，可以看出这个raft集群中，8节点的日志仍未完成且返回，只有1，3，5这三个节点有机会被重新选为leader
深化一下刚才的理解，那就是：multi-paxos的leader只是知道最大的log index是多少，但是空洞部分，可以异步填充，而raft的leader，不但知道最大的log index是多少，也必须拥有从0到log index直接的之间日志，在被选择为leader后，raft的leader可以单向的向其他节点同步落后的日志。&lt;/p&gt;

&lt;p&gt;多谢 @LoopJump 指出，raft这个协议方便的地方在于：”raft不允许日志的空洞，这也是为了比较方便和拉平两个节点的日志方便 “ Raft连续确认更大的一个优势是新主上任过程简单了。Multi Paxos在上任过程很复杂，不只是补全过程。我在raft经典场景这篇文章中也指出了这一点：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在一些一致性算法中，例如：Viewstamped Replication，
即使一开始没有包含全部已提交的条目也可以被选为领导人。
这些算法都有一些另外的机制来保证找到丢失的条目并将它们传输给新的领导人，
这个过程要么在选举过程中完成，要么在选举之后立即开始。
不幸的是，这种方式大大增加了复杂性。&lt;/li&gt;
  &lt;li&gt;Raft 使用了一种更简单的方式来保证：也就是日志落后的候选人，
无法被选中为Leader，这其实大大减小了选举和日志复制的相关性，
简化了raft的算法理解难度。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;作者：张帅
链接：https://www.zhihu.com/question/57321934/answer/152640954
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。&lt;/p&gt;

&lt;p&gt;multi-paxos在basic-paxos的模型基础之上，增加了“连续递增”value的元素；换言之，basic-paxos至始至终都是说单个value达成一致的过程，而multi-paxos说的是多个value达成一致，并且这些value还有个特点：value有id标记，而且id连续递增。用数据库redo 日志的模型来描述这个模型就是：数据库不停写入多个事务，每个事务insert一条记录，每个insert都会在数据库引擎内部产生一条redo日志，这个redo日志就是一个value，redo日志会被顺序标记递增的log_id。介绍完背景，现在可以来看看正确性和可优化的地方：先说协议正确性容易被忽视的地方：0. 有些值必须持久化存储这个似乎是废话，需要持久化哪些值呢？prepare 记录 ，accept记录。。。1.全局唯一的proposal_idBasic-paxos要求proposal_id全局唯一（否则会有不一致风险），如何保证唯一呢， ip+本地时间戳是个可选方案；2.prepare请求应该包含哪些信息？multi-paxos的prepare请求，可以理解为basic-paxos对于一批value发送prepare请求，那这个“一批”具体如何设定呢，答案是[start_id, 无穷大)这个连续区间。Start_id指的是什么呢？就是当前proposer 中value序列连续形成多数派的最大值，举个栗子，比如本地形成多数派的是{123,5}，未形成多数派的是{4， 7}，那么此时start_id就是3。是否这一区间的每一个value都需要指定一个proposal_id呢？当然不是，共享一个相同的proposal_id，这里是multi-paxos的关键。3. acceptor 如何响应收到的一个prepare请求？如何响应prepare请求，basic-paxos已经有详细说明，不再赘述。现在只说multi-paoxs的情况下，acceptor响应的注意点。acceptor收到prepare请求后，首先当然要比较proposal_id，通过检查后，下面就是关键操作了。这个关键操作其实在basic-paxos中有清楚的说明，就是返回给存在的已经accepted的最大的proposal_id的value，如果不存在，则返回NULL。当有basic到multi后，返回结果则变为一个集合了。如果集合元素太多怎么办？此处有个优化手段是，acceptor返回一个max_log_id，此id的含义是本地存在的accept记录的最大值。proposer拿到这个值后，就知道哪些value需要找acceptor去要，哪些可以自己随便生成了。  下面说实现上的优化问题：滑动窗口Multi-paxos大部分应用场景就是数据流，既然是数据流传输，其实相关优化都可以在tcp滑动窗口上找到相应点，滑动窗口中存储的就是value：1.批量并发网络发送：proposal可以同时发送多个value的accept请求；2.批量发送accept ack3.proposal超时重推（push），acceptor超时拉取（pull）已经从滑动窗口出去的value，是明确已经形成多数派不会被改写的value；滑动窗口中的，可能是空洞或者还有可能被改写的value，滑动窗口外是还未收到或者还未产生的value；commit消息另外一个重要的优化是commit 消息，当某个value确认形成多数派后，proposer可以发送commit消息给acceptor，便于acceptor将其清除出buff，并且加速此acceptor转换到proposal并且能够正常提供写入的恢复时间。为什么能加速呢？这要联系前面讲的proposal 发送prepare请求的过程，proposal在发送prepare前，要获取start_id，如果没有commit消息，start_id的确定就非常困难了。Group commit这个不是multi-paxos特有的优化，但凡是传输速度有差异的介质转换都有这个优化存在的空间，比如内存到磁盘（无论是机械还是SSD）。基本原因是，磁盘的I/O ps是有上限的，机械盘200左右，SSD 几千左右，比起内存和网络太低了。怎么办呢？把几百几千几万个value放在一个I/O buff里面刷到磁盘去，I/Ops成倍提升。成员组变更成员组指的是本来acceptor是{A BC}，我想安全的变成{D EF},怎么实现呢？raft给出了完美的实现方式，但multi-paxos在实现上不能照搬，有两点需要注意：1.成员变更的信息也是value序列中的一个，只不过内容比较特殊，描述的是成员组变更信息，简称做change_value。Change_value需要是一个barrier，就是说，change_value被accept之前，需要之前的value都accept，这个水很深，可以再开一个问题讨论；2.raft中成员变更是两阶段的，其实没必要的，想实现一阶段安全的成员变更，只需要限制变更成员的数量就可以，即{ABC}可以变为{ABD}，但不能变为{ADE}。连续变多次，就可以从{ABC}到{DEF}了。&lt;/p&gt;

&lt;p&gt;谨记paxos协议，对空洞日志的重确认，要用新的proposal id与raft不同，multi-paxos对选主没有特别要求，谁当都可以，甚至可以外部指定，过一轮完整paxos就行使用multi-paxos的上层应用要支持乱序提交和乱序回放，否则难以发挥性能优势redolog本地是乱序存储的，因此你可能需要配合一个的日志内容索引文件，以提供方便的旁路日志导出由于乱序确认的机制，网络层面在主备之间建立多条TCP链接更能发挥优势主切换为备，仔细考虑回放起点和未决事务如何处理；同样备切换为主时，也要仔细考虑重确认起点和未决事务尽管leader有效期内只需要accept过程即可，仍然要记得备机要严格按照P2b的约束进行检查，避免不小心接受了上一任leader的消息；同样的，主机也要有机制避免接受过期的备机应答消息&lt;/p&gt;

&lt;p&gt;作者：朱一聪
链接：https://www.zhihu.com/question/36648084/answer/82332860
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。&lt;/p&gt;

&lt;p&gt;Raft协议比paxos的优点是 容易理解，容易实现。它强化了leader的地位，把整个协议可以清楚的分割成两个部分，并利用日志的连续性做了一些简化：
（1）Leader在时。由Leader向Follower同步日志
（2）Leader挂掉了，选一个新Leader，Leader选举算法。      但是本质上来说，它容易的地方在于流程清晰，描述更清晰，关键之处都给出了伪代码级别的描述，可以直接用于实现，而paxos最初的描述是针对非常理论的一致性问题，真正能应用于工程实现的mulit-paxos，Lamport老爷爷就提了个大概，之后也有人尝试对multi-paxos做出更为完整详细的描述，但是每个人描述的都不大一样。      Zookeeper的ZAB，Viewstamped Replication（VR），raft，multi-paxos，这些都可以被称之为Leader-based一致性协议。不同的是，multi-paxos leader是作为对经典paxos的优化而提出，通过选择一个proposer作为leader降低多个proposer引起冲突的频率，合并阶段一从而将一次决议的平均消息代价缩小到最优的两次，实际上就算有多个leader存在，算法还是安全的，只是退化为经典的paxos算法。而经典的paxos，从一个提案被提出到被接受分为两个阶段，第一个阶段去询问值，第二阶段根据询问的结果提出值。这两个阶段是无法分割的，两个阶段的每个细节都是精心设计的，相互关联，共同保障了协议的一致性。而VR,ZAB,Raft这些强调合法leader的唯一性协议，它们直接从leader的角度描述协议的流程，也从leader的角度出发论证正确性。但是实际上它们使用了和Paxos完全一样的原理来保证协议的安全性，当同时存在多个节点同时尝试成为leader或者不知一个节点认为自己时leader时，本质上它们和经典Paxos中多个proposer并存的情形没什么不同。    Paxos和raft都是一旦一个entries（raft协议叫日志，paxos叫提案，叫法而已）得到多数派的赞成，这个entries就会定下来，不丢失，值不更改，最终所有节点都会赞成它。Paxos中称为提案被决定，Raft,ZAB,VR称为日志被提交，这只是说法问题。一个日志一旦被提交(或者决定），就不会丢失，也不可能更改，这一点这4个协议都是一致的。Multi-paxos和Raft都用一个数字来标识leader的合法性，multi-paxos中叫proposer-id，Raft叫term，意义是一样的，multi-paxos proposer-id最大的Leader提出的决议才是有效的，raft协议中term最大的leader才是合法的。实际上raft协议在leader选举阶段，由于老leader可能也还存活，也会存在不只一个leader的情形，只是不存在term一样的两个leader，因为选举算法要求leader得到同一个term的多数派的同意，同时赞同的成员会承诺不接受term更小的任何消息。这样可以根据term大小来区分谁是合法的leader。Multi-paxos的区分leader的合法性策略其实是一样的，谁的proproser-id大谁合法，而proposer-id是唯一的。因此它们其实在同一个时刻，都只会存在一个合法的leader。同时raft协议的Leader选举算法，新选举出的Leader已经拥有全部的可以被提交的日志，而multi-paxos择不需要保证这一点，这也意味multi-paxos需要额外的流程从其它节点获取已经被提交的日志。因此raft协议日志可以简单的只从leader流向follower在raft协议中，而multi-paxos则需要额外的流程补全已提交的日志。需要注意的是日志可以被提交和日志已经被提交是两个概念，它们的区别就像是我前方有块石头和我得知我前方有块石头。但是实际上，Raft和multi-Paxos一旦日志可以被提交，就能会保证不丢失，multi-paxos天然保证了这一点，这也是为什么新leader对于尚未被确认已经提交的日志需要重新执行经典paxos的阶段一，来补全可能缺失的已经被提交的日志，Raft协议通过强制新Leader首先提交一个本term的no-op 日志，配合前面提到的Leader选举算法所保证的性质，确保了这一点。一条日志一旦被多数派的节点写入本地日志文件中，就可以被提交，但是leader只有得知这一点后，才会真正commit这条日志，此时日志才是已经被提交的。       Raft协议强调日志的连续性，multi-paxos则允许日志有空洞。日志的连续性蕴含了这样一条性质：如果两个不同节点上相同序号的日志，只要term相同，那么这两条日志必然相同，且这和这之前的日志必然也相同的，这使得leader想follower同步日志时，比对日志非常的快速和方便；同时Raft协议中日志的commit（提交）也是连续的，一条日志被提交，代表这条日志之前所有的日志都已被提交，一条日志可以被提交，代表之前所有的日志都可以被提交。日志的连续性使得Raft协议中，知道一个节点的日志情况非常简单，只需要获取它最后一条日志的序号和term。可以举个列子，A,B,C三台机器，C是Leader，term是3，A告诉C它们最后一个日志的序列号都是4，term都是3，那么C就知道A肯定有序列号为1,2,3,4的日志，而且和C中的序列号为1,2,3,4的日志一样，这是raft协议日志的连续性所强调的，好了那么Leader知道日志1，2，3，4已经被多数派（A,C)拥有了，可以提交了。同时，这也保证raft协议在leader选举的时候，一个多数派里必然存在一个节点拥有全部的已提交的日志，这是由于最后一条被commit的日志，至少被多数派记录，而由于日志的连续性，拥有最后一条commit的日志也就意味着拥有全部的commit日志，即至少有一个多数派拥有所有已commit的日志。并且只需要从一个多数集中选择最后出最后一条日志term最大且序号最大的节点作为leader，新leader必定是拥有全部已commit的日志(关于这一点的论证，可以通过反证法思考一下，多数集中节点A拥有最后一条已commit的日志，但是B没有，而B当选leader。根据选主的法则只能有两种可能(1)当选而A最后一条日志的term小于B；(2)A最后一条日志的term等于B，但是A的日志少于B。1,2可能嘛？）而对于multi-paxos来说，日志是有空洞的，每个日志需要单独被确认是否可以commit，也可以单独commit。因此当新leader产生后，它只好重新对每个未提交的日志进行确认，已确定它们是否可以被commit，甚至于新leader可能缺失可以被提交的日志，需要通过Paxos阶段一向其它节点学习到缺失的可以被提交的日志，当然这都可以通过向一个多数派询问完成（这个流程存在着很大的优化空间，例如可以将这个流程合并到leader选举阶段，可以将所有日志的确认和学习合并到一轮消息中，减少消息数目等）。但是无论是Raft还是multi-paxos，新leader对于那些未提交的日志，都需要重新提交，不能随意覆写，因为两者都无法判定这些未提交的日志是否已经被之前的leader提交了。所以本质上，两者是一样的。一个日志被多数派拥有，那么它就可以被提交，但是Leader需要通过某种方式得知这一点，同时为了已经被提交的日志不被新leader覆写，新leader需要拥有所有已经被提交的日志（或者说可以被提交，因为有时候并没有办法得知一条可以被提交的日志是否已经被提交，例如当只有老leader提交了该日志，并返回客户端成功，然而老leader挂了），之后才能正常工作，并且需要重新提交所有未commit的日志。两者的区别在于Leader确认提交和获取所有可以被提交日志的方式上，而方式上的区别又是由于是日志是否连续造成的，Raft协议利用日志连续性，简化了这个过程。     在Raft和multi-paxos协议确保安全性的原理上，更进一步的说，所有的凡是 满足 集群中存活的节点数还能构成一个多数派，一致性就能满足的算法，raft协议，paxos，zab，viewstamp都是利用了同一个性质：两个多数派集合之间存在一个公共成员。对于一致性协议来说，一旦一个变量的值被确定，那么这个变量的值应该是唯一的，不再更改的。Raft,paoxos等协议，对于一个变量v来说，一个由节点n1提出的值a只有被一个多数集q1认可并记录后，才会正式令v=a，如果另一个节点n2想要修改变量v的值为b，也需要一个多数集q2的认可，而q1和q2必然至少有一个共同的成员p,节点p已经记录了v=a。因此只需要通过增加一些约束，让p能够告诉节点n2这个事实：v=a，使得n2放弃自己的提议，或者让节点p拒绝节点n2想要赋予v的值为b这个行为，都可以确保变量v的一致性不被破坏。这个思想对于这个四个协议来说都是一样的，4个协议都使用一个唯一的整数作为标识符来标明leader的合法性，paxos叫做proposer-id，ZAB叫epoch，VR叫view，raft叫term。把leader看做是想要赋予变量v某个值的节点n1,n2，上面提到的情形中，如果n2是目前的合法leader，那么n2需要知道v=a这个事实，对于raft来说就是选择n2是已经记录了v=a的节点，对于multi-paxos来说，就是重新确认下v的值。如果n1是目前的合法leader，n2是老的leader，p就会根据leader的标识符拒绝掉n2的提案，n2的提案会由于得不到一个多数派的接受而失效。最直接的从理论上阐明这个原理的是经典的paxos算法，关于这个原理更具体的阐述可以看看我在如何浅显易懂地解说 Paxos 的算法？下的回答。所以确实在一定程度上可以视raft,ZAB,VR都是paxos算法的一种改进，一种简化，一种优化，一种具象化。Lamport老人家还是叼叼叼。。。。。。。不过值得一提的是，ZAB和raft作者确实是受了paxos很多启发，VR是几乎同时独立于paxos提出的。       Raft容易实现在于它的描述是非常规范的，包括了所有的实现细节。如上面的人说的有如伪代码。而paxos的描述侧重于理论，工程实现按照谷歌chubby论文中的说话，大家从paxos出现，写着写着，处理了n多实际中的细节之后，已经变成另外一个算法了，这时候正确性已经无法得到理论的保证。所以它的实现非常难，因为一致性协议实非常精妙的。小细节上没考虑好，整个协议的一致性就崩溃了，而发现并更正细节上的错误在没有详尽的现成的参考的情况下是困难的，这需要对协议很深的理解。而且在Raft协议的博士论文CONSENSUS: BRIDGING THEORY AND PRACTICE，两位作者手把手事无巨细的教你如何用raft协议构建一个复制状态机。我表示智商正常的大学生，都能看懂。我相信在未来一致性现在被提起来，肯定不是现在这样，大部分人觉得好难啊，实现更难。。。。应该会成为一种常用技术。&lt;/p&gt;

&lt;p&gt;作者：郁白
链接：https://www.zhihu.com/question/36648084/answer/81413837
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。&lt;/p&gt;

&lt;p&gt;raft利用日志连续性对Paxos做了大量很好的简化，但是其中有一点有很强的误导性，就是新任leader对于旧日志的处理，他的原文描述是“Raft uses a simpler approach where
it guarantees that all the committed entries from previous terms are present on each new leader from the moment of
its election, without the need to transfer those entries to
the leader.”raft利用日志连续确认的特性，只要选举termID最大的做leader，就可以保证leader当选以后，不需要对本地旧日志重新投票，而是请follower过来直接抓就行了。即使对哪些尚未形成多数派的entry，也是一样的处理方式，并未进行重新投票，在他的协议里这个做法是没有问题的。但是multi-paxos原本的意思是，即使形成了多数派，仍然需要使用新的proposalID走一遍prepare-accept过程，工程上做了优化之后，对于明确标记了已形成多数派的entry可以不重新投票，但是未确认是否形成多数派的entry，则要求新任leader使用新的proposalID重新投票。因为multi-paxos并不假设日志间有连续确认的关系，每条日志之间相互独立，没有关系，1号日志尚未确认时，2号日志就可以确认形成多数派备份。&lt;/p&gt;

&lt;p&gt;作者：朱一聪
链接：https://www.zhihu.com/question/19787937/answer/82340987
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。&lt;/p&gt;

&lt;p&gt;Paxos是个精巧，又强大的协议，仅从过程的复杂度来说，确实如作者本人一再说的那样是个“简单的协议”，但是可以从非常多的角度来理解它为何正确，而原本的流程也并不适合直接工程化，这也是大概为什么工程上它存在如此多的变体。希望这个回答的让人更快的感受paxos的魅力，建立一个初步印象的同时不给人以误导。最后依然推荐larmport自己写的和paxos相关的三篇论文：« The Part-Time Parliament»、«Paxos made simple»、«Fast Paxos»前面关于Paxos的论述。2016/12/28上周和一个有真正paxos工程经验的人讨论一下paxos，paxos现在大多是应用于replication的一致性，用来实现一个 多节点一致的日志，和他 的讨论让我觉得要想真正的精确掌握paxos和它对应的强一致性领域，也许只有真正的在工程中实现过才行。这个回答只能当做是想要了解原理的入门吧，甚至可能有些微妙的地方还会产生误导。它介绍了paxos面向的问题，以及为何它的流程要这么设计，但还是希望对有兴趣阅读这个问题的有所帮助。2016 10/30现在看开头这段话是在是觉得有点羞耻，遂改之。我了解paxos是从找工作开始，比较详细的了解则是在毕设，自己动手了写了个类似Zookeeper的系统，paxos本身并不复杂，在«paxos made simple» Lamport用两段话就描述清楚了它的流程，他老人家也说paxos其实是个简单的算法。但是是我在工程领域见过最为精妙的算法。我想论述Paxos为什么难以理解会比描述Paxos的流程长的多的多。我最初学习Paxos是从《从Paxos到Zookeeper:分布式一致性原理与实践》，现在看来并不是个很好选择，作者讲解的方式是直接翻译论文，论述ZAB和paxos关系的部分我也觉得不是很严谨。如果真心觉得Paxos的原文不知所云，也不知道能拿来干嘛，可以从阅读Raft的论文开始，如果真的有兴趣，强推Raft作者Diego Ongaro那篇300页的博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》，不只是讲解了Raft协议，而且系统的论述Paxos类似的一致性协议，不仅从原理，也从工程角度出发，涉及方方面面，在写毕设中了就是写不动就翻翻的良作。我个人觉得阅读任何号称浅显易懂的解说Paxos算法的描述（比如下文=/=），最多只是让你更好的入门，要更深的理解Paxos以及所有等同于它的一致性协议，ZAB,Raft,ViewStamp，直接阅读相关论文，理解证明，理解它们哪里不同，为何本质上相同，与人探讨，在工程上自己实现，或者阅读开源实现的源代码才是最好的方式。分布式一致性是个有趣的领域，而Paxos和类似的协议对这个问题的重要性不喻，在过去的十年，Paxos几乎等价于分布式一致性。2016 6/20之前的答案最大的不严谨之处在于两个事件“先后”这种时序关系的处理上。paxos是个分布式一致性协议，它的事件需要多个节点共同参与，一个事件完成是指多个节点上均完成了自身负责的单机子事件(就让我门把这样的事件称为”分布式事件”)，这样的分布式事件可以看作是多个单机子事件的复合，但是即不能从两个分布式事件的先后推导出某个节点上它们的单机子事件的先后，也不能根据某个节点上两个单机子事件的先后断言它们对应的分布式事件的先后。举一个简单的例子，两个节点P1，P2;分布式事件a1设置每节点的本地变量v=1,分布式式事件a2设置每个节点本地变量v=2，如果我们称a1先于a2完成，那么对于节点P1而言，v=1发生在v=2之前还是之后都有可能;反之如果P1上v=1发生在v=2之前，a1和a2哪个县完成也都有可能。原来的回答有些地方论述 分布式事件a1在a2之后(先)时，默认了单节点上，a1会比a2先达成状态，或者反之。实际上为了论证paxos的正确性，并不需要借助于分布式事件的时序(起码无用太在意物理时序），对于paxos流程中的分布式事件，例如提案被通过，值被决定，让我们忘记它们之间物理时间上的先后关系吧。下面就开始假装推导出paxos，作为一种理解协议的流程和协议如何保证正确性的方式。这样的推导的过程远比我想象的冗长；相比之下，论文中Lamport自己推导出Paxos的过程简洁、巧妙、漂亮，但是更抽象。在末尾用自己的语言描述了这种方式，作为补充。补充的版本基本思路来自«Paxos made simple»，和原文略有不同；总共不到1500字，却既说明了Paxos是如何得到的，又严谨的论证了Paxos的正确性。首先我们简单介绍paxos所保证的一致性的具体含义；达成一致的条件(何时达成一致）；基于的一个基本的数学原理；以及它需要满足的假设。什么是一致性?实际上这个词在不同地方语义并不那么一致，Paxos面向的是一个理论的一致问题，这个问题的通俗描述是：有一个变量v，分布在N个进程中，每个进程都尝试修改自身v的值，它们的企图可能各不相同，例如进程A尝试另v=a,进程B尝试另v=b，但最终所有的进程会对v就某个值达成一致,即上述例子中如果v=a是v达成一致时的值，那么B上，最终v也会为a。需要注意的是某个时刻达成一致并不等价于该时刻所有进程的本地的v值都相同，有一个原因是进程可能挂掉，你不能要求挂掉的进程任何事；更像是最终所有存活的进程本地v的值都会相同。这个一致性需要满足三个要求：1.v达成一致时的值是由某个进程提出的。这是为了防止像这样的作弊方式：无论如何，最终都令每个进程的v为同一个预先设置好的值，例如都令v=2，那么这样的一致也太容易了，也没有任何实际意义。2.一旦v就某个值达成了一致，那么v不能对另一个值再次达成一致。这个要求称为安全性。3.一致总是能够达成，即v总会被决定为某个值。这是因为不想无休止的等待，这个要求也称为活性。Paxos中变量v达成一致的条件：  N个进程中大多数（超过一半） 进程都认为v是同一个值，例如c，那么我们称v被决定为c。这样即使少数进程挂掉了，也不会使得一致无法达成。Paxos保证的一致性如下：不存在这样的情形，某个时刻v被决定为c，而另一个时刻v又决定为另一个值d。由这个定义我们也看到，当v的值被决定后，Paxos保证了它就像是个单机的不可变变量，不再更改。也因此，对于一个客户端可以多次改写值的可读写变量在不同的节点上的一致性问题，Paxos并不能直接解决，它需要和状态机复制结合。Paxos基于的数学原理：  我们称大多数进程组成的集合为法定集合，两个法定集合必然存在非空交集，即至少有一个公共进程，称为法定集合性质。 例如A,B,C,D,F进程组成的全集，法定集合Q1包括进程A,B,C，Q2包括进程B,C,D，那么Q1和Q2的交集必然不在空，C就是Q1，Q2的公共进程。如果要说Paxos最根本的原理是什么，那么就是这个简单性质。同时，可以注意到，这个性质和达成一致的定义相呼应。Paxos中进程之间是平等的，即不存在一个特殊的进程，这是由于如果协议依赖于某个特殊的进程，那么这个进程挂掉势必会影响协议;而对于分布式环境，无法保证单个进程必然必活，能够容忍一定数量的进程挂掉，是分布式协议的必然要求。这是推导过程所要遵循的一个原则，就称为平等性原则好了。消息是进程间通信的唯一手段，对于分布式环境来说，这是显然的。Paxos要求满足的前置假设只有一个：消息内容不会被篡改；更正式的说是无拜占庭将军问题。假装的推导总是先从一些具体的场景开始，既然Paxos的假设仅仅只是消息不会被篡改,保证了这点任意场景下都能保证一致性，那么对于举例的场景它必然是能够保证一致性的；因此不妨先使得协议流程能在当前场景下能保证一致性，然后再举出另一个场景，当前的协议流程无法再该场景下满足一致性，接着再丰富协议流程，满足该场景，如此往复，最终得到完整的paxos协议，最后再不失一般性的论证协议对任意场景都能保证一致性。进程的平等性假设会带来如下的问题，考虑如下的场景1：三个进程的场景P1,P2P3（n个进程的场景类似），P1尝试令v的值被决定为a,P2尝试令v被决定为b。假设它们都先改写了自身的v值，然后发送消息尝试改修P3的v值。显然如果P3收到两个消息后都满足了它们的企图，那么v就会两次被决定为不同的值，这破坏了之前的定义。因此P3必须得拒绝掉其中一个进程的请求，如何拒绝也是我们最先考虑的问题。一个最简单的拒绝策略是先来后到，P3只会接受收到的第一个消息，拒绝之后的消息，即只会改写v一次。按照这个策略，如果P1发送的消息首先到达P3，那么P3接受该消息令v=a,拒绝掉后到的来自P2的消息。但是这个策略会引入一个另外的问题；在场景1的基础上考虑这样的场景1’，P3也尝试决定v的值，P3尝试令v被决定为c，那么P1，P2，P3都尝试修改v的值，首先P1令v=a,P2令v=b,P3令v=c(相当于自己给自己发消息），按照之前的策略，每个进程只会改写v的值一次，那么将永远不会出现两个进程的v值相等的情况，即v永远无法被决定。更正式的说，这样的协议不满足活性，活性要求协议总能达成一致。由此我们也得到第一个结论:进程必须能够多次改写v的值。同时我们也要意识到：当进程收到第一个消息时，进程是没有任何理由拒绝这个消息的请求的。拒绝策略总是需要有一个依据，之前我们的依据是消息到达的先后，只接受第一个到达的消息，但这导致不满足活性。现在我们需要另一个拒绝策略，也就是需要另一个依据，这个依据至少能够区分两个消息。为此我们引入一个ID来描述这个消息，这样就可以根据ID的大小来作为拒绝或接受的依据；选择ID更大的消息接受和选择ID更小的消息接受是两种完全对称的策略，不妨选择前者。这个策略不会导致明显的活性问题，ID更大的消息总是能被接受，一个节点可以多次更改v的值。例如在场景1’中，只要P1的消息ID比P3发送给自己的消息ID更大，P3就会接受P1的消息，令v=a，从而令v的值被决定为a。再来考虑最初的场景1，不妨假设P1的消息ID大于P2的消息ID，根据P3收到消息的先后可以分为两种情况：1. P3先收到P1的消息，记做场景1-2。由于P1的消息是P3收到的第一个消息，P3接受该请求，令v=a；同时为了能对之后收到的消息作出是否接受的判断，P3需要记录该消息的ID作为判断的依据。之后P3又收到P2的消息，该消息的ID小于P3记录的ID(即P1的消息ID)，因此P3拒绝该消息，这样我们的目的就达到。2. P3先收到P2的消息，记作场景1-3。同样P3接受该消息，令v=b,记录该消息的ID。之后P3收到P1的消息，由于P1的消息ID大于P3记录的ID，因此P3无法拒绝该消息，之前的问题依旧存在。尽管对于场景1-3，目前的策略依旧无法保证一致性，但是起码我们缩小协议不适用的范围。先总结下我们目前的策略，并定义一些称谓以方便后面的论述。我们称呼进程P发送的尝试修改另一个进程中变量v的值的消息称之为提案，记作Proposal；提案的ID记作proposal_id；提案中会附带一个值，如果一个进程接受一个提案，则修改自身的v值为该提案中的值。如果一个提案被大多数进程所接受，那么称提案被通过，此时显然v被决定为提案中的值。进程P记录的接受的提案ID记做a_proposal_id。之前我们尚未清晰定义a_proposal_id，实际上我们也就并未清晰的定义我们的拒绝策略，当P收到一个提案Proposal-i时，可能已经收到过多个提案，Proposal-i.proposal_id该和其中哪个提案的proposal_id比较，我们并未定义。我们定义为其中的最大者，这样实际上进程P只需维护一个a_proposal_id即可，当收到一个Proposal时，更新a_proposal_id = Max(Proposal.proposal_id，a_proposal_id）。同时在之前的描述中我们应当注意到实际上一个进程存在两个功能：1. 进程主动尝试令v的值被决定为某个值，向进程集合广播提案。2. 进程被动收到来自其它进程的提案，判断是否要接受它。因此可以把一个进程分为两个角色，称负责功能1的角色是提议者，记作Proposer，负责功能2的角色是接受者，记作Acceptor。由于两者完全没有耦合，所以并不一定需要在同个进程，但是为了方面描述，我们假定一个进程同时担任两种角色，而实际的工程实现也往往如此。接着我们尝试解决场景1-3,这看起来很难。P3作为接受者，收到P2的提案之前未收到任何消息，只能接受该提案，而由于P1的提案proposal_id大于P2的提案，我们的拒绝策略也无法让P3拒绝P2。我们先不急着推导具体可行的策略，先考虑下解决1-3场景可能的角度，有如下三种角度可以入手：1. P3能够拒绝掉P2的提案。2. P3能够拒绝掉P1的提案。3. 限制P1提出的提案中的值，如果P1的提案中的值与P2的提案一致，那么接受P1也不会破坏一致性。接着我们分析三个角度的可行性：角度1需要P3有做出拒绝的依据，由于消息是进程间通信唯一手段，这要求P3在收到P2的提案之前必须先收到过其它消息。对于场景1-3，只有P1，P2是主动发送消息的进程，P2当然不可能额外还发送一个消息试图令P3拒绝自己随后的提案。那么唯一的可能是P1在正式发送提案前，还发送了一个消息给P3，这个消息先于P2的提案到达，给了P3拒绝P2提案的理由。如果沿用目前的拒绝策略，那么P1只需先发送随后提案的proposal_id给P3，P3更新a_proposal_id 为 该消息附带的proposal_id，这样a_proposal_id将大于P2的提案的proposal_id，而导致P2的提案被拒绝，似乎这是一个可行的角度。对于角度2，我们目前的策略无法做到这一点，因此除了proposal_id外，我们还得给提案附加上额外的信息作为另外的拒绝策略的依据。提案由进程提出，也许我们可以附加进程的信息，但是就算P3得知P1的提案由P1提出，P3又凭什么歧视P1，这违反进程的平等性假设。似乎这个角度不是一个好角度。最后我们分析一下角度3，角度3提供了与1,2截然不同的思路，它不再是考虑如何拒绝，而把注意力放在如何对提案的值做出恰当的限制上。对于场景1-3而言，从这个角度，由于P3无法拒绝P1和P2的提案中的任何一个，因此P1的提案的值就必须与P2的提案一致；这也意味着了P1在正式提出提案前，需要有途径能获悉P2的提案的值。如我们上面一直强调的，消息是唯一的通信手段，P1必须收到来自其它节点消息才有可能得知P2提出的提案的值。P1可以被动的等待收到消息，也可以主动的去询问其它节点等待回复。后者显然是更好的策略，没有收到想要的消息就一直等待未免也太消极了，这种等待也可能一直持续下去从而导致活性问题。经过上面的分析，我们暂时排除了从角度2入手（实际上后面也不会再考虑，因为从1,3入手已经足以解决问题）。下面将沿着角度1,3进行更深入的分析，我们先尝试从角度1出发，毕竟考虑如何拒绝已经有了经验。先来总结下我们在分析角度1时引入的额外的流程:进程P在发送提案前，先广播一轮消息，消息附带着接下来要发送的提案的proposal_id。由于该消息和接下来发送的提案相关，且在提案被提出之前发送，不妨称这个消息为预提案，记作PreProposal，预提案中附带着接下来的提案的proposal_id。当作为接受者的进程Pj收到预提案后，更新Pj. a_proposal_id。还记得我们之前的拒绝策略中a_proposal_id的更新策略嘛：a_proposal_id = max(proposal_id,a_proposal_id)，a_proposal_id是递增的。因此如果预提案的proposal_id小于Pj.a_proposal_id，Pj完全可以忽略这个预提案，因为这代表了该预提案对应的提案的proposal_id小于Pj.a_proposal_id，必然会被拒绝。我们沿用之前拒绝策略中a_proposal_id的更新策略。这样当收到预提案或者提案后，a_proposal_id的值均更新为 max(a_proposal_id,proposal_id)。接着我们来看看引入了预提案后，能否真正解决场景1-3。根据P1和P2的预提案到达P3的先后也存在两种场景：1.场景1-3-1：P1的预提案先到达，P3更新a_proposal_id 为该提案的proposal_id，这导致随后到达的P2的提案的proposal_id小于a_proposal_id，被拒绝。满足一致性2.场景1-3-2：P2的提案先到达，P3接受P2的提案，此时和原始的场景1-3存在同样的问题。归根结底，预提案阶段能否使得P3拒绝该拒绝的，也依赖消息到达的顺序，和提案阶段的拒绝策略存在相同的问题，但至少又缩小了不能保证安全性的场景范围。幸好我们还有角度3可以着手考虑，所以仍有希望完全解决场景1-3。在深入角度3之前，先总结下协议目前为止的流程，现在协议的流程已经分为了两个阶段：预提案阶段和提案阶段，两种消息：预提案 和提案，两种角色：接受者 和 提议者，流程如下：阶段一 提议者Proposer：向接受者Acceptor广播预提案，附带接下来提案Proposal的proposal_id 接受者Acceptor：收到预提案后更新a_proposal_id = max(proposal_id,a_proposal_id) 阶段二       提议者Proposer：向接受者Acceptor广播提案，和之前的预提案共享同一个proposal_id 接受者Acceptor：如果收到的提案的proposal_id&amp;gt;= a.proposal_id，那么接受这个提案，更新a_proposal_id = max(proposal_id,a_proposal_id)为了更形象，之前的讨论是基于三个进程的场景,实际上对于N进程的场景也是一样的。N个进程时，与之前场景1对应的场景是：N个进程，存在两个进程Pi,Pj，Pi尝试另v被决定为a,Pj尝试另v被决定为b，Pi提出的预提案记作PreProposal-i，提案记作Proposal-i；Pj的预提案PreProsal-j，提案Proposal-j。之拒绝策略的讨论都是基于一个关键的进程P3，只要P3最终能拒绝Proposal-i和Proposal-j中的一个，两个提案就不会都被通过，那么一致性就不会被破坏。Pi的提案被通过代表了存在一个法定集合Q-i，Q-i中的进程都接受了Proposal-i，Pj同理，存在一个Q-j，Q-j中的进程都接受了Proposal-j。由于法定集合的性质，两个多数集Q-i和Q-j中必然存在一个公共进程Pk。Pk即相当于场景1中的P3，只要Pk能够拒绝Proposal-i和Proposal-j中的一个，协议依旧是安全的。为了不失一般性，下面我们都以N个进程的场景作为讨论的基础，称为场景2，由于场景1和场景2可以一一对应，所以接下来顺着限制提案的值的角度，我们直接针对场景2-3-2，之前的场景和场景1一样，我们的拒绝策略已经足以应付。v的值被决定代表有一个提案，它被法定数目的集合接受，我们称这为提案被通过。首先我们看下场景2-3-2，Pi对应场景1-3-2中的P1，Pj对应P2，Pk对应P3。Pj的提案Proposal-j最终会被法定集合Q-j接受，即v的值被决定为b，且Proposal-i.proposal-id &amp;gt; Proposal-j.proposal_id。我们需要限制Pi的提案的值，不能让Pi自由的给Proposal-i中的v赋值。在2-3-2中，由于拒绝策略失效，所以只能令Proposal-i.v = Proposal-j.v=b。要做到这一点，正如前面的分析所言，Pi需要先主动询问进程集合，来得知Proposal-j.v =b这一事实。显然Pi是没有先验信息来得知Proposal-j由哪个进程提出，也不知道Q-i和Q-j的公共节点Pk是谁，因此Pi只能广播它的查询。由于我们需要允许少数进程失败，Pi可能只能得到大多数进程的回复，而这之中可能不包括Pj。我们称这些回复Pi的查询的进程的集合为Q-i-2,为了描述更简单，无妨假设Q-i-2=Q-i。尽管Pi的查询可能得不到Pj的回复，好在作为将会被通过的提案，Proposal-j将会被Q-j内所有进程接受，因此如果进程作为接受者在接受提案时，顺便记录该提案，那么Q-j内所有进程都将得知Proposal-j.v=b。由于Pk属于Q-i和Q-j的交集，所以Pk即收到了Pi的查询，又接受了提案Proposal-j。之前我们已经引入了预提案阶段，显然我们可以为预提案附带上查询的意图，即Pk作为接受者收到Pi的预提案后，会回复它记录的接受过的提案。有一个问题是Pk此时是否已经记录了Proposal-j呢?很巧的是在场景2-3-2中，Pj的提案Proposal-j是先于Pi的预提案PreProposal-i先到达，所以Pk已经记录了Proposal-j.v = b，Pj收到的来自Pk的回复中包含了提案Proposal-j，而2-3-2之外的场景，拒绝策略已经足以应付。这里依旧还有两个问题，先讨论第一个：实际上除了Pi和Pj外可能还会有多个进程发起预提案和提案，所以收到 PreProposal-i时Pk可能已经接受过多个提案，并非只有Proposal-j，那么Pk应该回复PreProposal-i其中哪个提案，或者都回复？Pk并不知道Proposal-j会被通过，它只知道自己接受了该提案。都回复是个效率很低但是稳妥，可以保证Pk不会遗漏Proposal-j，Pk已经回复了它所能知道的全部，我们也无法要求更多。需要注意到的是进程是平等的，所以Q-i中所有进程都和Pk一样回复了它接受过的所有提案。当Pi收到所有来自Q-i的回复时，随之而来的是第二个问题：Pi收到了多个Proposal作为一个Acceptor组成的法定集合Q-i对PreProposal-i的回复，记这些Proposal组成的集合记坐K-i，那么它应当选择K-i中哪个一个提案的值作为它接下来的提案Proposal-i的v值？记最终选择的这个提案为Proposal-m。在场景2-3-2中，我们第一直觉是希望选择的Proposal-m 即是 Proposal-j，但是实际上，我们只要保证Proposal-m .v = Proposal-j.v即可。从另一个角度 ，K-i中很可能存在这样的提案Proposal-f,Proposal-f.v!=Proposal-j.v，我们要做的是避免选择到这类提案。我们可以根据一些依据瞎选碰碰运气，但是这并明智。我们不妨假设存在一个策略cf，cf满足需求，使得选择出提案Proposal-m满足Proposal-m.v= Proposal-j.v。然后让我们来分析一下此时Proposal-f有什么特征。Proposal-f能够被提出，代表存在一个多数集合Q-f，Q-f中每个进程都接受了PreProposal-f，同时假设是进程P-f提出了PreProposal-f和Proposal-f。Q-f和Q-j必然存在一个公共节点，记做Ps，Ps即接受了PreProposal-f又接受了Proposal-j。Ps收到PreProposal-f和Proposal-j的顺序只有两种可能：1.Ps先收到PreProposal-f2.Ps先收到Proposal-jPreProposal-f.proposa-id和Proposal-j. proposal_id的大小也只有两种可能，不妨先假设PreProposal-f.proposal_id &amp;gt; Proposal-j.proposal_id。对于情形1，Ps先收到PreProposal-f，接受它，更新Ps.a_proposal_id = PreProposal-f.proposal_id &amp;gt; Proposal-j.proposal_id，同时之前的a_proposal_id的更新策略又使得Ps.a_proposal_id是递增的，于是导致收到Proposal-j时,Proposal-j.proposal_id小于Ps.a_proposal_id,被拒绝，而这于Ps的定义矛盾。对于情形2，Ps将把提案Proposal-j回复给PreProposal-f。由于我们假设了策略cl的存在，于是P-f在收到所有Q-f对PreProposal-f的回复后，将令Proposal-f.v=Proposal-j.v，cl就是干这个的。因此由于Proposal-f.v!=Proposal-j.v矛盾。于是当假设PreProposal-f.proposal_id &amp;gt; Proposal-j.proposal_id 时，情形1,2我们都得出了矛盾，同时两者的proposal_id又不相等(最初的假设），所以必然PreProposal-f.proposal_id &amp;lt; Proposal-j.proposal_id，即Propsoal-f.proposal_id &amp;lt; Proposal-j.proposal_id。于是我们得到的结论是：如果策略cl存在，提案Proposal-j最终会被通过，任意一个proposal_id更大的预提案PreProposal-i，对于它得到的Q-i的回复K-i中的Proposal-f，只要Proposal-f.v!= Proposal-j.v，那么必然 Proposal-f.proposal_id &amp;lt; Proposal-j.proposal_id。既然K-i中所有v值不等于Proposal-j.v的提案，proposal_id都比Proposal-j更小，那代表所有proposal_id比Proposal-j更大的提案，v值都等于Proposal-j.v，因此选择K-i中proprosal_id最大的提案，就能保证Proposal-i.v = Proposal-j.v。于是我们得到了策略cl的具体形式。我们得到了具体可行的策略cl是建立在策略cl存在这一前提之上，因此反过来，对于这个具体的选值策略cl，结合之前我们得到了协议流程，它是否能保证如下的性质CP1，依旧需要进一步的论证 ：如果一个提案Proposal-j最终会被通过，那么对于任意的一个提案Proposal-i,如果Proposal-i.proposal_id &amp;gt; Proposal-j.proposal_id，那么Proposal-i.v = Proposal-j.v。我们先总结下目前得到的协议流程：阶段一 预提案阶段 提议者Proposer：向接受者Acceptor广播预提案，附带接下来提案Proposal的proposal_id 接受者Acceptor：收到预提案后更新a_proposal_id = max(proposal_id,a_proposal_id)，如果预提案的proposal_id大于a_proposal_id，那么回复该预提案的提议者改接受者接受过的所有提案。阶段二 提案阶段 提议者Proposer：等待直到收到大多数接受者对预提案的回复，从所有回复的提案组合成的集合K中挑选proposal_id最大的提案，以该提案的值作为本次提案的值。如果K是空集，那么可以给提案任意赋值。 向接受者Acceptor广播提案，和之前的预提案共享同一个proposal_id 接受者Acceptor：如果收到的提案的proposal_id&amp;gt;= a.proposal_id，那么接受这个提案，更新a_proposal_id = max(proposal_id,a_proposal_id)这些流程是为了解决举例的场景而不断丰富的，接着就让我们论证下协议流程是否总是可以确保CP1。首先假设Proposal-i.v != Proposal-j.v，如果得出矛盾即可证明CP1。在尝试推出矛盾前，我们先做一些定义，以便后续的推导。记大多数接受者组成的法定集合为Q，K是提议者在提案阶段收到的所有Q回复的提案组成的集合，如果K不为空，记K中proposal_id最大的提案是MaxProposal(K)，本次提案的值即是MaxProposal(K).v；如果K是空集，那么MaxProposal(K).v = null。特别的，对于提案Proposal-i,回复它预提案接受者的集合为Q-i,回复的提案组成的集合为K-i，Proposal-i.v = MaxProposal(K-i)，Proposal-i.v=null代表可以随意赋值。为了描述方便，我们令Proposal-i的proposal_id为i，即Proposal-i代表了proposal_id=i的提案，Proposal-j意味着Proposal-j.proposal_id =j。论证过程如下：(1) Proposal-i.v!=Proposal-j.v，即MaxProposal(K-i) .v！= Proposal-j.v，即MaxProposal(K-i)!=Proposal-j(2) Proposal-j最终会被通过，代表最终会存在一个多数集合Q-j，Q-j中每个接受者都接受了Proposal-j。(3) 两个多数集必然存在公共成员，故Q-j和Q-i必然存在一个公共的进程Pk，Pk即收到了PreProposal-i又收到了Proposal-j，且都接受了它们；Pk收到消息的先后关系只存在如下两种可能：1.Pk先收到了PreProposal-i2.Pk先收到了Proposal-j(4) 情形1中Pk先收到了PreProposal-i，那么Pk收到Proposal-j时，Pk.a_proposal &amp;gt;= PreProposal-i.proposal_id ＞Proposal-j.proposal_id，Pk会拒绝Proposal-j，与(3)矛盾，因此情况1不可能，Pk必然是先收到Proposal-j。(5) 情形2中Pk收到PreProposal-i时，已经接受了Proposal-j，因此Pk回复PreProposal-i的提案中包含了Proposal-j，因此K-i中必然包含了Proposal-j。(6) 由(1)已知MaxProposal(K-i) != Proposal-j，即存在另一个提案Proposal-m = MaxProposal(K-i),而Proposal-j属于K-i,因此Proposal-m.proposal_id &amp;gt; Proposal-j.proposal_id，且Proposal-m.v != Proposal-j.v。(7)由预提案阶段，接受者回复预提案的条件可知：Proposal-i.proposal_id大于集合K-i中任意一个提案的Proposal-id，故Proposal-i.proposal_id&amp;gt;Proposal-m.proposal_id。(8) 目前我们已经论证如下一点：在Proposal-j最终会被通过的前提下，如果存在一个提案Proposal-i.v!=Proposal-j.v，且Proposal-i.proposal_id &amp;gt;Proposal-j.proposal_id,我们一个数学符号来带表示这个情况，记CF(j,i)；那么 必然存在一个提案Proposal-m, Proposal-m！=Proposal-j.v,且Proposal-m.proposal_id &amp;gt; Proposal-j.proposal_id，同样的我们可以记做CF(j,m)。并且Proposal-m.proposal_id &amp;lt; Proposal-i.proposal_id，m &amp;lt; i。即如果CF(i,j)成立，那么必然CF(m,j)成立，且i&amp;gt;m，即 CF(i,j) —&amp;gt; CF(m,j)。这个过程可以继续往下递归，但由于区间[j,i]范围是有限的，因此一定会递归到一个CF(j,e)，此时不可能存在一个提案，它的proposal_id在区间(j,e)内，无法往下递归，这与(8)矛盾。这也就意味着CF(e,j)不成立，而如果CF(i,j)成立，那么CF(e,j)成立,因此CF(i,j)不成立，故假设不成立，即Proposal-i.v 必然等于Proposal-j.v，即证CP1。通过归约的方式得出矛盾的方式依旧有些抽象，我们可以通过更好的定义假设来更容易得到的矛盾：我们加强对Proposal-i的约束;先假设存在一个提案的非空集合KD，KD中的任意一个提案Proposal-k,Proposal-k.v!=Proposal-j.v，且Proposal-k.proposal_id &amp;gt; Proposal-j.v;再假设Proposal-i是KD中proposal_id最小的提案；由于KD是非空集合，故Proposal-i必然存在。我们依旧可以从Proposal-i出发，(1)~(7)与上面相同，同理得到：存在一个提案Proposal-m, Proposal-m！=Proposal-v,且Proposal-m.proposal_id &amp;gt; Proposal-j.proposal_id，且Proposal-m.proposal_id &amp;lt; Proposal-i.proposal_id。显然Proposal-m满足集合KD对提案的要求，故Proposal-m属于KD，又Proposal-m.proposal_id&amp;lt;Proposal-i.proposal_id，这和Proposal-i是KD中proposal_id最小的提案的定义矛盾。因此不存在这样的非空集合KD，即不存在一个提案Proposal-k,Proposal-k.v!=Proposal-j.v且Proposal-k.proposal_id&amp;gt;Proposal-j.proposal_id，即如果一个提案Proposal-j最终会被通过，对于任意的一个提案Proposal-i,如果Proposal-i.proposal_id &amp;gt; Proposal-j.proposal_id，那么必定Proposal-i.v = Proposal-j.v，即CP1。CP1约束了proposal_id大于Proposal-j的提案的值，保证了如果一个提案Proposal-j最终会被通过，不会存在另一个proposal-id更大且值不同的提案被通过，因为这些提案的值都和Proposal-j相同。那么对于proposal_id更小的提案呢？ 我们假设存在一个提案Proposal-o,Proposal-o.proposal_id &amp;lt; Proposal-j.proposal_id，且Proposal-o.v!=Proposal-j.v,Proposal-o最终会被通过，将CP1应用于Proposal-o,则可知Proposal-j不存在，这矛盾，故Proposal-o不存在。故由CP1我们可知：如果一个提案Proposal-j最终会被通过，那么不存在另一个提案，它最终会被通过，且它的值与Proposal-j不同。由此协议必然是安全的。虽然我们得到了一个安全的一致性协议，基本上它就是Paxos，但是真正的Paxos要比我们假装推导出的协议更简单一点。回过头来看下我们的阶段1中接受者Acceptor的行为，它要回复所有的它接受过的提案，从实践的角度，不论是在本地保存所有它接受过的提案还是通过网络将它们传输给提议者，开销都太大且不可控。再看下阶段二中，提议者的选值策略，它只是选择了收到的多数集接受者回复的提案中proposal_id最大的那一个，因此接受者实际上只需要回复它接受过的proposal_id最大的提案即可，因为其它提案根本不可能会被选值策略选中。因此最终的协议如下，它就是Paxos:阶段一 预提案阶段： 提议者Proposer：向接受者Acceptor广播预提案，附带接下来提案Proposal的proposal_id 接受者Acceptor：收到预提案后更新a_proposal_id = max(proposal_id,a_proposal_id)，如果预提案的proposal_id&amp;gt;a_proposal_id，Acceptor回复记录的接受过的proposal_id最大的提案。 阶段二 提案阶段： 提议者Proposer：等待直到收到大多数接受者对预提案的回复，从所有回复的提案组成的法定数目的提案集合K中挑选proposal_id最大的提案，以该提案的值作为本次提案的值。如果K是空集，那么可以给提案任意赋值。然后把该提案广播给接受者们，提案和预提案共享同一个proposal_id。 接受者Acceptor：如果收到的提案的proposal_id&amp;gt;= a.proposal_id，那么接受这个提案，更新a_proposal_id = max(proposal_id,a_proposal_id)，更新记录的提案。补充部分：上面的过程从具体的场景开始推导Paxos，虽然直观但是繁琐，如果从抽象的概念和分析入手，那么过程将会相当简洁和漂亮，这也是Lamport的原始论文中的方式。这种方式理解起来更困难的地方在于：1.没有任何具体的认知下，直接抽象的讨论容易让人摸不着头脑。2.大神总是在一些地方觉得显然而不加以展开论述，而普通读者如我的内心OS:显然你mei!但是原文引出Paxos算法的过程实在是简洁、漂亮；而经过上面的轮述，相信有了直观的印象后，再来看抽象的方式也不那么困难，所以补充下。回顾下定理CP1：如果一个提案Proposal-j最终会被通过，那么对于任意的一个提案Proposal-i,如果Proposal-i.proposal_id &amp;gt; Proposal-j.proposal_id，那么必定Proposal-i.v = Proposal-j.v。上面我们已经论证了只要协议能够保证CP1就能够保证一致性。但是CP1依旧过于宽泛，从CP1引出具体的协议流程依然是一头雾水，那么我们是否可以得到一个更加具体的定理CP2，保证CP2即可保证CP1，同时从CP2出发更容易引出协议的具体流程。为了描述方便，我们令Proposal-i的proposal_id为i，即Proposal-i代表了proposal_id=i的提案。要导出CP2不妨先考虑下如何证明CP1，利用归纳法，只要如能证明如下性质成立，即可证明CP1：如果proposal_id在区间[j,i）内任意的提案，提案的值均为Proposal-j.v，那么必定Proposal-i.v=v;这个定理记做CP1_2。现在我们用高中时简单而效果神奇的归纳法，利用CP1_2证明下CP1:假设propsal_id小于i的提案中最大的提案是Proposal-(i-1)。1.如果对于[j,i-1)内的任意提案，值均为Proposal-j.v，那么由CP1_2可知Proposal-i.v = Proposal-j.v。2.由1可知如果对于[j,i-1)内的任意提案，值均为Proposal-j.v，[j,i)内的任意提案，值均为Proposal-j.v3.假设Proposal-(j+1)是proposal-id大于j的最小提案，由CP1_2可知Proposal-(j+1).v = Proposal-j.v4.由3,2归纳可知[j,  )内任意提案Proposal-i，Proposal-i.v = Proposal-j.v，即CP1来看下CP1_2，相比CP1，它结论不变，但是多了一个前置条件：proposal_id在区间[j,i）内任意的提案值均为Proposal-j.v；这是一个重大的进步。CP1_2相比CP1看起来容易保证 很多，但是它们却是等价的。考虑CP1_2的三个前置条件：1.i &amp;gt; j2.提案Proposal-j最终会被通过。因此由提案被通过的定义可知必然存在一个法定集合Q-j，Q-j中任意一个接受者最终都接受了Proposal-j3.proposal_id在区间[j,i)内的提案的值均为Proposal-j.v对于任意的一个法定集合Q，考虑Q最终（包括过去和未来的所有时空）会接受的所有proposal_id小于i的提案组成的集合K。根据法定集合性质，Q和Q-j必然存在一个公共的节点，即Q中必然存在一个节点，该节点最终会接受Proposal-j，因此集合K包含Proposal-j。由K包含Proposal-j可知K中最大的提案proposal_id &amp;gt;= j；由CP1_2的前置条件3和K的定义可知如果K中存在proposal-id大于j的提案，那么该提案的值等于Proposal-j.v，因此K中proposal_id最大的提案的值等于Proposal-j.v。综上所述由CP1_2的前置条件可知：对于任意的一个法定集合Q，Q最终会接受的proposal_id小于i的提案组成的集合K，K中proposal_id最大的提案的值必定为Proposal-j.v。如果我们能证明该条件下，Proposal-i.v = Proposal-j.v，即可证明CP1_2。将CP1_2的前置条件替换为该条件，我们可以得到一个如下的性质CP2，保证CP2即可保证CP1_2：对于任意的一个法定集合Q，Q最终会接受的所有proposal_id小于i的提案组成的集合K，如果K中proposal_id最大的提案的值为Proposal-j.v;那么Proposal-i.v = Proposal-j.v。而引出满足CP2的流程就比较容易了，由于集合K中proposal_id最大的提案的值等于Proposal-j.v，看起来只要令Proposal-i的值为K中proposal-id最大提案的值就可以保证CP2。由于Q是任意一个法定集合，因此获取K似乎在实现上也不难，提出Proposal-i的提议者只要向Q中所有接受者询问即可。然后： CP2 —&amp;gt; CP1_2—&amp;gt; CP1 —&amp;gt;一致性但是实际上获取K没有那么简单，K包含的是Q所有最终接受的proposal-id小于i的的提案，不仅包含已经接受过的提案，还包括未来会接受的提案。获取已经接受过的提案是容易的，Q中的接受者只需记录它所有接受过的提案，当收到提出Proposal-i的提议者询问时，回复当中proposal_id小于i的提案即可;但是如何知晓未来？我们可以换个思路，既然无法知晓未来，那么我们约束未来，收到询问后，令Q中的接受者都承诺不再接受任何proposal_id小于i的提案，即接受者未来将不接受任何proposal_id小于i的提案;既然未来已不存在，那么Proposal-i的提议者根据Q的回复获能得到完整的K。于是协议的流程如下：对于提议者，在正式提案前，先向任意的法定集合Q发送一个消息，这个消息即是预提案，消息中要附带提案的proposal-id，作为接受者承诺和回复的依据。接受者收到预提案后，承诺：不再接受比预提案中附带的proposal-id更小的提案；并回复:已经接受的proposal-id比于提案的proposal-id更小的提案，如之前所论述的，回复的所有满足条件的提案可以优化为只回复一个比预提案proposal_id更小的提案中proposal_id最大的那个提案。提议者收到所有Q中接受者回复的提案后，挑选其中proposal_id最大的提案的值作为本次提案的值。这样我们就得到了Paxos中最为关键的几步，阅读了之前冗长的假装推导，相信读者很容易就能补全它得到完整的Paxos。相比于之前近万字的假装推导，这个推导过程才1500字左右，但是即说清了Paxos是如何得出的，又论证Paxos为何正确，简洁却更有力。所以最后还是建议真有兴趣的话去看下原文，在我看来它无疑是计算机领域那数不尽的论文中最值得阅读的那一类。末尾我所描述的版本思路来自«Paxos made simple»，基本一致但也并不完全相同；而« The Part-Time Parliament»则别有一番风味。最后需要注意的是Paxos并不完全满足开头解决一致性问题需要满足的三个条件中的3。理论上，Paxos存在永远无法达成一致的可能，哪怕是在所有进程都存活的情况下。想象一下这样的场景，一个提案Proposal-j被提出时，恰好一个proposal-id更大的预提案Proposal-i被提出，导致Proposal-j无法被通过，而Proposal-i同样的 又因为一个proposal_id更大的其它预提案被提出，导致无法被通过。这种情况理论上存在无限递归的可能，这个问题也称为活锁;FLP早就证明了就算是容忍一个进程的失败，异步环境下任何一致性算法都存在永不终止的可能。但是实际的工程中，很多手段可以来减小两个提案的冲突概率，使得v被决定的均摊开销是一个提案，多个提案还无法决定v值的情形是极小概率事件，且概率随着提案个数增加越来越小。另外的一点，通常认为Paxos可以容忍少数进程挂掉 ，但这只是为了保证它的活性，对于安全性，实际上Paxos永远满足1,2，哪怕进程都挂掉了，此时只是显然一致无法达成而已。&lt;/p&gt;

&lt;p&gt;作者：朱一聪
链接：https://www.zhihu.com/question/19787937/answer/82340987
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。&lt;/p&gt;

&lt;p&gt;Paxos是个精巧，又强大的协议，仅从过程的复杂度来说，确实如作者本人一再说的那样是个“简单的协议”，但是可以从非常多的角度来理解它为何正确，而原本的流程也并不适合直接工程化，这也是大概为什么工程上它存在如此多的变体。希望这个回答的让人更快的感受paxos的魅力，建立一个初步印象的同时不给人以误导。最后依然推荐larmport自己写的和paxos相关的三篇论文：« The Part-Time Parliament»、«Paxos made simple»、«Fast Paxos»前面关于Paxos的论述。2016/12/28上周和一个有真正paxos工程经验的人讨论一下paxos，paxos现在大多是应用于replication的一致性，用来实现一个 多节点一致的日志，和他 的讨论让我觉得要想真正的精确掌握paxos和它对应的强一致性领域，也许只有真正的在工程中实现过才行。这个回答只能当做是想要了解原理的入门吧，甚至可能有些微妙的地方还会产生误导。它介绍了paxos面向的问题，以及为何它的流程要这么设计，但还是希望对有兴趣阅读这个问题的有所帮助。2016 10/30现在看开头这段话是在是觉得有点羞耻，遂改之。我了解paxos是从找工作开始，比较详细的了解则是在毕设，自己动手了写了个类似Zookeeper的系统，paxos本身并不复杂，在«paxos made simple» Lamport用两段话就描述清楚了它的流程，他老人家也说paxos其实是个简单的算法。但是是我在工程领域见过最为精妙的算法。我想论述Paxos为什么难以理解会比描述Paxos的流程长的多的多。我最初学习Paxos是从《从Paxos到Zookeeper:分布式一致性原理与实践》，现在看来并不是个很好选择，作者讲解的方式是直接翻译论文，论述ZAB和paxos关系的部分我也觉得不是很严谨。如果真心觉得Paxos的原文不知所云，也不知道能拿来干嘛，可以从阅读Raft的论文开始，如果真的有兴趣，强推Raft作者Diego Ongaro那篇300页的博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》，不只是讲解了Raft协议，而且系统的论述Paxos类似的一致性协议，不仅从原理，也从工程角度出发，涉及方方面面，在写毕设中了就是写不动就翻翻的良作。我个人觉得阅读任何号称浅显易懂的解说Paxos算法的描述（比如下文=/=），最多只是让你更好的入门，要更深的理解Paxos以及所有等同于它的一致性协议，ZAB,Raft,ViewStamp，直接阅读相关论文，理解证明，理解它们哪里不同，为何本质上相同，与人探讨，在工程上自己实现，或者阅读开源实现的源代码才是最好的方式。分布式一致性是个有趣的领域，而Paxos和类似的协议对这个问题的重要性不喻，在过去的十年，Paxos几乎等价于分布式一致性。2016 6/20之前的答案最大的不严谨之处在于两个事件“先后”这种时序关系的处理上。paxos是个分布式一致性协议，它的事件需要多个节点共同参与，一个事件完成是指多个节点上均完成了自身负责的单机子事件(就让我门把这样的事件称为”分布式事件”)，这样的分布式事件可以看作是多个单机子事件的复合，但是即不能从两个分布式事件的先后推导出某个节点上它们的单机子事件的先后，也不能根据某个节点上两个单机子事件的先后断言它们对应的分布式事件的先后。举一个简单的例子，两个节点P1，P2;分布式事件a1设置每节点的本地变量v=1,分布式式事件a2设置每个节点本地变量v=2，如果我们称a1先于a2完成，那么对于节点P1而言，v=1发生在v=2之前还是之后都有可能;反之如果P1上v=1发生在v=2之前，a1和a2哪个县完成也都有可能。原来的回答有些地方论述 分布式事件a1在a2之后(先)时，默认了单节点上，a1会比a2先达成状态，或者反之。实际上为了论证paxos的正确性，并不需要借助于分布式事件的时序(起码无用太在意物理时序），对于paxos流程中的分布式事件，例如提案被通过，值被决定，让我们忘记它们之间物理时间上的先后关系吧。下面就开始假装推导出paxos，作为一种理解协议的流程和协议如何保证正确性的方式。这样的推导的过程远比我想象的冗长；相比之下，论文中Lamport自己推导出Paxos的过程简洁、巧妙、漂亮，但是更抽象。在末尾用自己的语言描述了这种方式，作为补充。补充的版本基本思路来自«Paxos made simple»，和原文略有不同；总共不到1500字，却既说明了Paxos是如何得到的，又严谨的论证了Paxos的正确性。首先我们简单介绍paxos所保证的一致性的具体含义；达成一致的条件(何时达成一致）；基于的一个基本的数学原理；以及它需要满足的假设。什么是一致性?实际上这个词在不同地方语义并不那么一致，Paxos面向的是一个理论的一致问题，这个问题的通俗描述是：有一个变量v，分布在N个进程中，每个进程都尝试修改自身v的值，它们的企图可能各不相同，例如进程A尝试另v=a,进程B尝试另v=b，但最终所有的进程会对v就某个值达成一致,即上述例子中如果v=a是v达成一致时的值，那么B上，最终v也会为a。需要注意的是某个时刻达成一致并不等价于该时刻所有进程的本地的v值都相同，有一个原因是进程可能挂掉，你不能要求挂掉的进程任何事；更像是最终所有存活的进程本地v的值都会相同。这个一致性需要满足三个要求：1.v达成一致时的值是由某个进程提出的。这是为了防止像这样的作弊方式：无论如何，最终都令每个进程的v为同一个预先设置好的值，例如都令v=2，那么这样的一致也太容易了，也没有任何实际意义。2.一旦v就某个值达成了一致，那么v不能对另一个值再次达成一致。这个要求称为安全性。3.一致总是能够达成，即v总会被决定为某个值。这是因为不想无休止的等待，这个要求也称为活性。Paxos中变量v达成一致的条件：  N个进程中大多数（超过一半） 进程都认为v是同一个值，例如c，那么我们称v被决定为c。这样即使少数进程挂掉了，也不会使得一致无法达成。Paxos保证的一致性如下：不存在这样的情形，某个时刻v被决定为c，而另一个时刻v又决定为另一个值d。由这个定义我们也看到，当v的值被决定后，Paxos保证了它就像是个单机的不可变变量，不再更改。也因此，对于一个客户端可以多次改写值的可读写变量在不同的节点上的一致性问题，Paxos并不能直接解决，它需要和状态机复制结合。Paxos基于的数学原理：  我们称大多数进程组成的集合为法定集合，两个法定集合必然存在非空交集，即至少有一个公共进程，称为法定集合性质。 例如A,B,C,D,F进程组成的全集，法定集合Q1包括进程A,B,C，Q2包括进程B,C,D，那么Q1和Q2的交集必然不在空，C就是Q1，Q2的公共进程。如果要说Paxos最根本的原理是什么，那么就是这个简单性质。同时，可以注意到，这个性质和达成一致的定义相呼应。Paxos中进程之间是平等的，即不存在一个特殊的进程，这是由于如果协议依赖于某个特殊的进程，那么这个进程挂掉势必会影响协议;而对于分布式环境，无法保证单个进程必然必活，能够容忍一定数量的进程挂掉，是分布式协议的必然要求。这是推导过程所要遵循的一个原则，就称为平等性原则好了。消息是进程间通信的唯一手段，对于分布式环境来说，这是显然的。Paxos要求满足的前置假设只有一个：消息内容不会被篡改；更正式的说是无拜占庭将军问题。假装的推导总是先从一些具体的场景开始，既然Paxos的假设仅仅只是消息不会被篡改,保证了这点任意场景下都能保证一致性，那么对于举例的场景它必然是能够保证一致性的；因此不妨先使得协议流程能在当前场景下能保证一致性，然后再举出另一个场景，当前的协议流程无法再该场景下满足一致性，接着再丰富协议流程，满足该场景，如此往复，最终得到完整的paxos协议，最后再不失一般性的论证协议对任意场景都能保证一致性。进程的平等性假设会带来如下的问题，考虑如下的场景1：三个进程的场景P1,P2P3（n个进程的场景类似），P1尝试令v的值被决定为a,P2尝试令v被决定为b。假设它们都先改写了自身的v值，然后发送消息尝试改修P3的v值。显然如果P3收到两个消息后都满足了它们的企图，那么v就会两次被决定为不同的值，这破坏了之前的定义。因此P3必须得拒绝掉其中一个进程的请求，如何拒绝也是我们最先考虑的问题。一个最简单的拒绝策略是先来后到，P3只会接受收到的第一个消息，拒绝之后的消息，即只会改写v一次。按照这个策略，如果P1发送的消息首先到达P3，那么P3接受该消息令v=a,拒绝掉后到的来自P2的消息。但是这个策略会引入一个另外的问题；在场景1的基础上考虑这样的场景1’，P3也尝试决定v的值，P3尝试令v被决定为c，那么P1，P2，P3都尝试修改v的值，首先P1令v=a,P2令v=b,P3令v=c(相当于自己给自己发消息），按照之前的策略，每个进程只会改写v的值一次，那么将永远不会出现两个进程的v值相等的情况，即v永远无法被决定。更正式的说，这样的协议不满足活性，活性要求协议总能达成一致。由此我们也得到第一个结论:进程必须能够多次改写v的值。同时我们也要意识到：当进程收到第一个消息时，进程是没有任何理由拒绝这个消息的请求的。拒绝策略总是需要有一个依据，之前我们的依据是消息到达的先后，只接受第一个到达的消息，但这导致不满足活性。现在我们需要另一个拒绝策略，也就是需要另一个依据，这个依据至少能够区分两个消息。为此我们引入一个ID来描述这个消息，这样就可以根据ID的大小来作为拒绝或接受的依据；选择ID更大的消息接受和选择ID更小的消息接受是两种完全对称的策略，不妨选择前者。这个策略不会导致明显的活性问题，ID更大的消息总是能被接受，一个节点可以多次更改v的值。例如在场景1’中，只要P1的消息ID比P3发送给自己的消息ID更大，P3就会接受P1的消息，令v=a，从而令v的值被决定为a。再来考虑最初的场景1，不妨假设P1的消息ID大于P2的消息ID，根据P3收到消息的先后可以分为两种情况：1. P3先收到P1的消息，记做场景1-2。由于P1的消息是P3收到的第一个消息，P3接受该请求，令v=a；同时为了能对之后收到的消息作出是否接受的判断，P3需要记录该消息的ID作为判断的依据。之后P3又收到P2的消息，该消息的ID小于P3记录的ID(即P1的消息ID)，因此P3拒绝该消息，这样我们的目的就达到。2. P3先收到P2的消息，记作场景1-3。同样P3接受该消息，令v=b,记录该消息的ID。之后P3收到P1的消息，由于P1的消息ID大于P3记录的ID，因此P3无法拒绝该消息，之前的问题依旧存在。尽管对于场景1-3，目前的策略依旧无法保证一致性，但是起码我们缩小协议不适用的范围。先总结下我们目前的策略，并定义一些称谓以方便后面的论述。我们称呼进程P发送的尝试修改另一个进程中变量v的值的消息称之为提案，记作Proposal；提案的ID记作proposal_id；提案中会附带一个值，如果一个进程接受一个提案，则修改自身的v值为该提案中的值。如果一个提案被大多数进程所接受，那么称提案被通过，此时显然v被决定为提案中的值。进程P记录的接受的提案ID记做a_proposal_id。之前我们尚未清晰定义a_proposal_id，实际上我们也就并未清晰的定义我们的拒绝策略，当P收到一个提案Proposal-i时，可能已经收到过多个提案，Proposal-i.proposal_id该和其中哪个提案的proposal_id比较，我们并未定义。我们定义为其中的最大者，这样实际上进程P只需维护一个a_proposal_id即可，当收到一个Proposal时，更新a_proposal_id = Max(Proposal.proposal_id，a_proposal_id）。同时在之前的描述中我们应当注意到实际上一个进程存在两个功能：1. 进程主动尝试令v的值被决定为某个值，向进程集合广播提案。2. 进程被动收到来自其它进程的提案，判断是否要接受它。因此可以把一个进程分为两个角色，称负责功能1的角色是提议者，记作Proposer，负责功能2的角色是接受者，记作Acceptor。由于两者完全没有耦合，所以并不一定需要在同个进程，但是为了方面描述，我们假定一个进程同时担任两种角色，而实际的工程实现也往往如此。接着我们尝试解决场景1-3,这看起来很难。P3作为接受者，收到P2的提案之前未收到任何消息，只能接受该提案，而由于P1的提案proposal_id大于P2的提案，我们的拒绝策略也无法让P3拒绝P2。我们先不急着推导具体可行的策略，先考虑下解决1-3场景可能的角度，有如下三种角度可以入手：1. P3能够拒绝掉P2的提案。2. P3能够拒绝掉P1的提案。3. 限制P1提出的提案中的值，如果P1的提案中的值与P2的提案一致，那么接受P1也不会破坏一致性。接着我们分析三个角度的可行性：角度1需要P3有做出拒绝的依据，由于消息是进程间通信唯一手段，这要求P3在收到P2的提案之前必须先收到过其它消息。对于场景1-3，只有P1，P2是主动发送消息的进程，P2当然不可能额外还发送一个消息试图令P3拒绝自己随后的提案。那么唯一的可能是P1在正式发送提案前，还发送了一个消息给P3，这个消息先于P2的提案到达，给了P3拒绝P2提案的理由。如果沿用目前的拒绝策略，那么P1只需先发送随后提案的proposal_id给P3，P3更新a_proposal_id 为 该消息附带的proposal_id，这样a_proposal_id将大于P2的提案的proposal_id，而导致P2的提案被拒绝，似乎这是一个可行的角度。对于角度2，我们目前的策略无法做到这一点，因此除了proposal_id外，我们还得给提案附加上额外的信息作为另外的拒绝策略的依据。提案由进程提出，也许我们可以附加进程的信息，但是就算P3得知P1的提案由P1提出，P3又凭什么歧视P1，这违反进程的平等性假设。似乎这个角度不是一个好角度。最后我们分析一下角度3，角度3提供了与1,2截然不同的思路，它不再是考虑如何拒绝，而把注意力放在如何对提案的值做出恰当的限制上。对于场景1-3而言，从这个角度，由于P3无法拒绝P1和P2的提案中的任何一个，因此P1的提案的值就必须与P2的提案一致；这也意味着了P1在正式提出提案前，需要有途径能获悉P2的提案的值。如我们上面一直强调的，消息是唯一的通信手段，P1必须收到来自其它节点消息才有可能得知P2提出的提案的值。P1可以被动的等待收到消息，也可以主动的去询问其它节点等待回复。后者显然是更好的策略，没有收到想要的消息就一直等待未免也太消极了，这种等待也可能一直持续下去从而导致活性问题。经过上面的分析，我们暂时排除了从角度2入手（实际上后面也不会再考虑，因为从1,3入手已经足以解决问题）。下面将沿着角度1,3进行更深入的分析，我们先尝试从角度1出发，毕竟考虑如何拒绝已经有了经验。先来总结下我们在分析角度1时引入的额外的流程:进程P在发送提案前，先广播一轮消息，消息附带着接下来要发送的提案的proposal_id。由于该消息和接下来发送的提案相关，且在提案被提出之前发送，不妨称这个消息为预提案，记作PreProposal，预提案中附带着接下来的提案的proposal_id。当作为接受者的进程Pj收到预提案后，更新Pj. a_proposal_id。还记得我们之前的拒绝策略中a_proposal_id的更新策略嘛：a_proposal_id = max(proposal_id,a_proposal_id)，a_proposal_id是递增的。因此如果预提案的proposal_id小于Pj.a_proposal_id，Pj完全可以忽略这个预提案，因为这代表了该预提案对应的提案的proposal_id小于Pj.a_proposal_id，必然会被拒绝。我们沿用之前拒绝策略中a_proposal_id的更新策略。这样当收到预提案或者提案后，a_proposal_id的值均更新为 max(a_proposal_id,proposal_id)。接着我们来看看引入了预提案后，能否真正解决场景1-3。根据P1和P2的预提案到达P3的先后也存在两种场景：1.场景1-3-1：P1的预提案先到达，P3更新a_proposal_id 为该提案的proposal_id，这导致随后到达的P2的提案的proposal_id小于a_proposal_id，被拒绝。满足一致性2.场景1-3-2：P2的提案先到达，P3接受P2的提案，此时和原始的场景1-3存在同样的问题。归根结底，预提案阶段能否使得P3拒绝该拒绝的，也依赖消息到达的顺序，和提案阶段的拒绝策略存在相同的问题，但至少又缩小了不能保证安全性的场景范围。幸好我们还有角度3可以着手考虑，所以仍有希望完全解决场景1-3。在深入角度3之前，先总结下协议目前为止的流程，现在协议的流程已经分为了两个阶段：预提案阶段和提案阶段，两种消息：预提案 和提案，两种角色：接受者 和 提议者，流程如下：阶段一 提议者Proposer：向接受者Acceptor广播预提案，附带接下来提案Proposal的proposal_id 接受者Acceptor：收到预提案后更新a_proposal_id = max(proposal_id,a_proposal_id) 阶段二       提议者Proposer：向接受者Acceptor广播提案，和之前的预提案共享同一个proposal_id 接受者Acceptor：如果收到的提案的proposal_id&amp;gt;= a.proposal_id，那么接受这个提案，更新a_proposal_id = max(proposal_id,a_proposal_id)为了更形象，之前的讨论是基于三个进程的场景,实际上对于N进程的场景也是一样的。N个进程时，与之前场景1对应的场景是：N个进程，存在两个进程Pi,Pj，Pi尝试另v被决定为a,Pj尝试另v被决定为b，Pi提出的预提案记作PreProposal-i，提案记作Proposal-i；Pj的预提案PreProsal-j，提案Proposal-j。之拒绝策略的讨论都是基于一个关键的进程P3，只要P3最终能拒绝Proposal-i和Proposal-j中的一个，两个提案就不会都被通过，那么一致性就不会被破坏。Pi的提案被通过代表了存在一个法定集合Q-i，Q-i中的进程都接受了Proposal-i，Pj同理，存在一个Q-j，Q-j中的进程都接受了Proposal-j。由于法定集合的性质，两个多数集Q-i和Q-j中必然存在一个公共进程Pk。Pk即相当于场景1中的P3，只要Pk能够拒绝Proposal-i和Proposal-j中的一个，协议依旧是安全的。为了不失一般性，下面我们都以N个进程的场景作为讨论的基础，称为场景2，由于场景1和场景2可以一一对应，所以接下来顺着限制提案的值的角度，我们直接针对场景2-3-2，之前的场景和场景1一样，我们的拒绝策略已经足以应付。v的值被决定代表有一个提案，它被法定数目的集合接受，我们称这为提案被通过。首先我们看下场景2-3-2，Pi对应场景1-3-2中的P1，Pj对应P2，Pk对应P3。Pj的提案Proposal-j最终会被法定集合Q-j接受，即v的值被决定为b，且Proposal-i.proposal-id &amp;gt; Proposal-j.proposal_id。我们需要限制Pi的提案的值，不能让Pi自由的给Proposal-i中的v赋值。在2-3-2中，由于拒绝策略失效，所以只能令Proposal-i.v = Proposal-j.v=b。要做到这一点，正如前面的分析所言，Pi需要先主动询问进程集合，来得知Proposal-j.v =b这一事实。显然Pi是没有先验信息来得知Proposal-j由哪个进程提出，也不知道Q-i和Q-j的公共节点Pk是谁，因此Pi只能广播它的查询。由于我们需要允许少数进程失败，Pi可能只能得到大多数进程的回复，而这之中可能不包括Pj。我们称这些回复Pi的查询的进程的集合为Q-i-2,为了描述更简单，无妨假设Q-i-2=Q-i。尽管Pi的查询可能得不到Pj的回复，好在作为将会被通过的提案，Proposal-j将会被Q-j内所有进程接受，因此如果进程作为接受者在接受提案时，顺便记录该提案，那么Q-j内所有进程都将得知Proposal-j.v=b。由于Pk属于Q-i和Q-j的交集，所以Pk即收到了Pi的查询，又接受了提案Proposal-j。之前我们已经引入了预提案阶段，显然我们可以为预提案附带上查询的意图，即Pk作为接受者收到Pi的预提案后，会回复它记录的接受过的提案。有一个问题是Pk此时是否已经记录了Proposal-j呢?很巧的是在场景2-3-2中，Pj的提案Proposal-j是先于Pi的预提案PreProposal-i先到达，所以Pk已经记录了Proposal-j.v = b，Pj收到的来自Pk的回复中包含了提案Proposal-j，而2-3-2之外的场景，拒绝策略已经足以应付。这里依旧还有两个问题，先讨论第一个：实际上除了Pi和Pj外可能还会有多个进程发起预提案和提案，所以收到 PreProposal-i时Pk可能已经接受过多个提案，并非只有Proposal-j，那么Pk应该回复PreProposal-i其中哪个提案，或者都回复？Pk并不知道Proposal-j会被通过，它只知道自己接受了该提案。都回复是个效率很低但是稳妥，可以保证Pk不会遗漏Proposal-j，Pk已经回复了它所能知道的全部，我们也无法要求更多。需要注意到的是进程是平等的，所以Q-i中所有进程都和Pk一样回复了它接受过的所有提案。当Pi收到所有来自Q-i的回复时，随之而来的是第二个问题：Pi收到了多个Proposal作为一个Acceptor组成的法定集合Q-i对PreProposal-i的回复，记这些Proposal组成的集合记坐K-i，那么它应当选择K-i中哪个一个提案的值作为它接下来的提案Proposal-i的v值？记最终选择的这个提案为Proposal-m。在场景2-3-2中，我们第一直觉是希望选择的Proposal-m 即是 Proposal-j，但是实际上，我们只要保证Proposal-m .v = Proposal-j.v即可。从另一个角度 ，K-i中很可能存在这样的提案Proposal-f,Proposal-f.v!=Proposal-j.v，我们要做的是避免选择到这类提案。我们可以根据一些依据瞎选碰碰运气，但是这并明智。我们不妨假设存在一个策略cf，cf满足需求，使得选择出提案Proposal-m满足Proposal-m.v= Proposal-j.v。然后让我们来分析一下此时Proposal-f有什么特征。Proposal-f能够被提出，代表存在一个多数集合Q-f，Q-f中每个进程都接受了PreProposal-f，同时假设是进程P-f提出了PreProposal-f和Proposal-f。Q-f和Q-j必然存在一个公共节点，记做Ps，Ps即接受了PreProposal-f又接受了Proposal-j。Ps收到PreProposal-f和Proposal-j的顺序只有两种可能：1.Ps先收到PreProposal-f2.Ps先收到Proposal-jPreProposal-f.proposa-id和Proposal-j. proposal_id的大小也只有两种可能，不妨先假设PreProposal-f.proposal_id &amp;gt; Proposal-j.proposal_id。对于情形1，Ps先收到PreProposal-f，接受它，更新Ps.a_proposal_id = PreProposal-f.proposal_id &amp;gt; Proposal-j.proposal_id，同时之前的a_proposal_id的更新策略又使得Ps.a_proposal_id是递增的，于是导致收到Proposal-j时,Proposal-j.proposal_id小于Ps.a_proposal_id,被拒绝，而这于Ps的定义矛盾。对于情形2，Ps将把提案Proposal-j回复给PreProposal-f。由于我们假设了策略cl的存在，于是P-f在收到所有Q-f对PreProposal-f的回复后，将令Proposal-f.v=Proposal-j.v，cl就是干这个的。因此由于Proposal-f.v!=Proposal-j.v矛盾。于是当假设PreProposal-f.proposal_id &amp;gt; Proposal-j.proposal_id 时，情形1,2我们都得出了矛盾，同时两者的proposal_id又不相等(最初的假设），所以必然PreProposal-f.proposal_id &amp;lt; Proposal-j.proposal_id，即Propsoal-f.proposal_id &amp;lt; Proposal-j.proposal_id。于是我们得到的结论是：如果策略cl存在，提案Proposal-j最终会被通过，任意一个proposal_id更大的预提案PreProposal-i，对于它得到的Q-i的回复K-i中的Proposal-f，只要Proposal-f.v!= Proposal-j.v，那么必然 Proposal-f.proposal_id &amp;lt; Proposal-j.proposal_id。既然K-i中所有v值不等于Proposal-j.v的提案，proposal_id都比Proposal-j更小，那代表所有proposal_id比Proposal-j更大的提案，v值都等于Proposal-j.v，因此选择K-i中proprosal_id最大的提案，就能保证Proposal-i.v = Proposal-j.v。于是我们得到了策略cl的具体形式。我们得到了具体可行的策略cl是建立在策略cl存在这一前提之上，因此反过来，对于这个具体的选值策略cl，结合之前我们得到了协议流程，它是否能保证如下的性质CP1，依旧需要进一步的论证 ：如果一个提案Proposal-j最终会被通过，那么对于任意的一个提案Proposal-i,如果Proposal-i.proposal_id &amp;gt; Proposal-j.proposal_id，那么Proposal-i.v = Proposal-j.v。我们先总结下目前得到的协议流程：阶段一 预提案阶段 提议者Proposer：向接受者Acceptor广播预提案，附带接下来提案Proposal的proposal_id 接受者Acceptor：收到预提案后更新a_proposal_id = max(proposal_id,a_proposal_id)，如果预提案的proposal_id大于a_proposal_id，那么回复该预提案的提议者改接受者接受过的所有提案。阶段二 提案阶段 提议者Proposer：等待直到收到大多数接受者对预提案的回复，从所有回复的提案组合成的集合K中挑选proposal_id最大的提案，以该提案的值作为本次提案的值。如果K是空集，那么可以给提案任意赋值。 向接受者Acceptor广播提案，和之前的预提案共享同一个proposal_id 接受者Acceptor：如果收到的提案的proposal_id&amp;gt;= a.proposal_id，那么接受这个提案，更新a_proposal_id = max(proposal_id,a_proposal_id)这些流程是为了解决举例的场景而不断丰富的，接着就让我们论证下协议流程是否总是可以确保CP1。首先假设Proposal-i.v != Proposal-j.v，如果得出矛盾即可证明CP1。在尝试推出矛盾前，我们先做一些定义，以便后续的推导。记大多数接受者组成的法定集合为Q，K是提议者在提案阶段收到的所有Q回复的提案组成的集合，如果K不为空，记K中proposal_id最大的提案是MaxProposal(K)，本次提案的值即是MaxProposal(K).v；如果K是空集，那么MaxProposal(K).v = null。特别的，对于提案Proposal-i,回复它预提案接受者的集合为Q-i,回复的提案组成的集合为K-i，Proposal-i.v = MaxProposal(K-i)，Proposal-i.v=null代表可以随意赋值。为了描述方便，我们令Proposal-i的proposal_id为i，即Proposal-i代表了proposal_id=i的提案，Proposal-j意味着Proposal-j.proposal_id =j。论证过程如下：(1) Proposal-i.v!=Proposal-j.v，即MaxProposal(K-i) .v！= Proposal-j.v，即MaxProposal(K-i)!=Proposal-j(2) Proposal-j最终会被通过，代表最终会存在一个多数集合Q-j，Q-j中每个接受者都接受了Proposal-j。(3) 两个多数集必然存在公共成员，故Q-j和Q-i必然存在一个公共的进程Pk，Pk即收到了PreProposal-i又收到了Proposal-j，且都接受了它们；Pk收到消息的先后关系只存在如下两种可能：1.Pk先收到了PreProposal-i2.Pk先收到了Proposal-j(4) 情形1中Pk先收到了PreProposal-i，那么Pk收到Proposal-j时，Pk.a_proposal &amp;gt;= PreProposal-i.proposal_id ＞Proposal-j.proposal_id，Pk会拒绝Proposal-j，与(3)矛盾，因此情况1不可能，Pk必然是先收到Proposal-j。(5) 情形2中Pk收到PreProposal-i时，已经接受了Proposal-j，因此Pk回复PreProposal-i的提案中包含了Proposal-j，因此K-i中必然包含了Proposal-j。(6) 由(1)已知MaxProposal(K-i) != Proposal-j，即存在另一个提案Proposal-m = MaxProposal(K-i),而Proposal-j属于K-i,因此Proposal-m.proposal_id &amp;gt; Proposal-j.proposal_id，且Proposal-m.v != Proposal-j.v。(7)由预提案阶段，接受者回复预提案的条件可知：Proposal-i.proposal_id大于集合K-i中任意一个提案的Proposal-id，故Proposal-i.proposal_id&amp;gt;Proposal-m.proposal_id。(8) 目前我们已经论证如下一点：在Proposal-j最终会被通过的前提下，如果存在一个提案Proposal-i.v!=Proposal-j.v，且Proposal-i.proposal_id &amp;gt;Proposal-j.proposal_id,我们一个数学符号来带表示这个情况，记CF(j,i)；那么 必然存在一个提案Proposal-m, Proposal-m！=Proposal-j.v,且Proposal-m.proposal_id &amp;gt; Proposal-j.proposal_id，同样的我们可以记做CF(j,m)。并且Proposal-m.proposal_id &amp;lt; Proposal-i.proposal_id，m &amp;lt; i。即如果CF(i,j)成立，那么必然CF(m,j)成立，且i&amp;gt;m，即 CF(i,j) —&amp;gt; CF(m,j)。这个过程可以继续往下递归，但由于区间[j,i]范围是有限的，因此一定会递归到一个CF(j,e)，此时不可能存在一个提案，它的proposal_id在区间(j,e)内，无法往下递归，这与(8)矛盾。这也就意味着CF(e,j)不成立，而如果CF(i,j)成立，那么CF(e,j)成立,因此CF(i,j)不成立，故假设不成立，即Proposal-i.v 必然等于Proposal-j.v，即证CP1。通过归约的方式得出矛盾的方式依旧有些抽象，我们可以通过更好的定义假设来更容易得到的矛盾：我们加强对Proposal-i的约束;先假设存在一个提案的非空集合KD，KD中的任意一个提案Proposal-k,Proposal-k.v!=Proposal-j.v，且Proposal-k.proposal_id &amp;gt; Proposal-j.v;再假设Proposal-i是KD中proposal_id最小的提案；由于KD是非空集合，故Proposal-i必然存在。我们依旧可以从Proposal-i出发，(1)~(7)与上面相同，同理得到：存在一个提案Proposal-m, Proposal-m！=Proposal-v,且Proposal-m.proposal_id &amp;gt; Proposal-j.proposal_id，且Proposal-m.proposal_id &amp;lt; Proposal-i.proposal_id。显然Proposal-m满足集合KD对提案的要求，故Proposal-m属于KD，又Proposal-m.proposal_id&amp;lt;Proposal-i.proposal_id，这和Proposal-i是KD中proposal_id最小的提案的定义矛盾。因此不存在这样的非空集合KD，即不存在一个提案Proposal-k,Proposal-k.v!=Proposal-j.v且Proposal-k.proposal_id&amp;gt;Proposal-j.proposal_id，即如果一个提案Proposal-j最终会被通过，对于任意的一个提案Proposal-i,如果Proposal-i.proposal_id &amp;gt; Proposal-j.proposal_id，那么必定Proposal-i.v = Proposal-j.v，即CP1。CP1约束了proposal_id大于Proposal-j的提案的值，保证了如果一个提案Proposal-j最终会被通过，不会存在另一个proposal-id更大且值不同的提案被通过，因为这些提案的值都和Proposal-j相同。那么对于proposal_id更小的提案呢？ 我们假设存在一个提案Proposal-o,Proposal-o.proposal_id &amp;lt; Proposal-j.proposal_id，且Proposal-o.v!=Proposal-j.v,Proposal-o最终会被通过，将CP1应用于Proposal-o,则可知Proposal-j不存在，这矛盾，故Proposal-o不存在。故由CP1我们可知：如果一个提案Proposal-j最终会被通过，那么不存在另一个提案，它最终会被通过，且它的值与Proposal-j不同。由此协议必然是安全的。虽然我们得到了一个安全的一致性协议，基本上它就是Paxos，但是真正的Paxos要比我们假装推导出的协议更简单一点。回过头来看下我们的阶段1中接受者Acceptor的行为，它要回复所有的它接受过的提案，从实践的角度，不论是在本地保存所有它接受过的提案还是通过网络将它们传输给提议者，开销都太大且不可控。再看下阶段二中，提议者的选值策略，它只是选择了收到的多数集接受者回复的提案中proposal_id最大的那一个，因此接受者实际上只需要回复它接受过的proposal_id最大的提案即可，因为其它提案根本不可能会被选值策略选中。因此最终的协议如下，它就是Paxos:阶段一 预提案阶段： 提议者Proposer：向接受者Acceptor广播预提案，附带接下来提案Proposal的proposal_id 接受者Acceptor：收到预提案后更新a_proposal_id = max(proposal_id,a_proposal_id)，如果预提案的proposal_id&amp;gt;a_proposal_id，Acceptor回复记录的接受过的proposal_id最大的提案。 阶段二 提案阶段： 提议者Proposer：等待直到收到大多数接受者对预提案的回复，从所有回复的提案组成的法定数目的提案集合K中挑选proposal_id最大的提案，以该提案的值作为本次提案的值。如果K是空集，那么可以给提案任意赋值。然后把该提案广播给接受者们，提案和预提案共享同一个proposal_id。 接受者Acceptor：如果收到的提案的proposal_id&amp;gt;= a.proposal_id，那么接受这个提案，更新a_proposal_id = max(proposal_id,a_proposal_id)，更新记录的提案。补充部分：上面的过程从具体的场景开始推导Paxos，虽然直观但是繁琐，如果从抽象的概念和分析入手，那么过程将会相当简洁和漂亮，这也是Lamport的原始论文中的方式。这种方式理解起来更困难的地方在于：1.没有任何具体的认知下，直接抽象的讨论容易让人摸不着头脑。2.大神总是在一些地方觉得显然而不加以展开论述，而普通读者如我的内心OS:显然你mei!但是原文引出Paxos算法的过程实在是简洁、漂亮；而经过上面的轮述，相信有了直观的印象后，再来看抽象的方式也不那么困难，所以补充下。回顾下定理CP1：如果一个提案Proposal-j最终会被通过，那么对于任意的一个提案Proposal-i,如果Proposal-i.proposal_id &amp;gt; Proposal-j.proposal_id，那么必定Proposal-i.v = Proposal-j.v。上面我们已经论证了只要协议能够保证CP1就能够保证一致性。但是CP1依旧过于宽泛，从CP1引出具体的协议流程依然是一头雾水，那么我们是否可以得到一个更加具体的定理CP2，保证CP2即可保证CP1，同时从CP2出发更容易引出协议的具体流程。为了描述方便，我们令Proposal-i的proposal_id为i，即Proposal-i代表了proposal_id=i的提案。要导出CP2不妨先考虑下如何证明CP1，利用归纳法，只要如能证明如下性质成立，即可证明CP1：如果proposal_id在区间[j,i）内任意的提案，提案的值均为Proposal-j.v，那么必定Proposal-i.v=v;这个定理记做CP1_2。现在我们用高中时简单而效果神奇的归纳法，利用CP1_2证明下CP1:假设propsal_id小于i的提案中最大的提案是Proposal-(i-1)。1.如果对于[j,i-1)内的任意提案，值均为Proposal-j.v，那么由CP1_2可知Proposal-i.v = Proposal-j.v。2.由1可知如果对于[j,i-1)内的任意提案，值均为Proposal-j.v，[j,i)内的任意提案，值均为Proposal-j.v3.假设Proposal-(j+1)是proposal-id大于j的最小提案，由CP1_2可知Proposal-(j+1).v = Proposal-j.v4.由3,2归纳可知[j,  )内任意提案Proposal-i，Proposal-i.v = Proposal-j.v，即CP1来看下CP1_2，相比CP1，它结论不变，但是多了一个前置条件：proposal_id在区间[j,i）内任意的提案值均为Proposal-j.v；这是一个重大的进步。CP1_2相比CP1看起来容易保证 很多，但是它们却是等价的。考虑CP1_2的三个前置条件：1.i &amp;gt; j2.提案Proposal-j最终会被通过。因此由提案被通过的定义可知必然存在一个法定集合Q-j，Q-j中任意一个接受者最终都接受了Proposal-j3.proposal_id在区间[j,i)内的提案的值均为Proposal-j.v对于任意的一个法定集合Q，考虑Q最终（包括过去和未来的所有时空）会接受的所有proposal_id小于i的提案组成的集合K。根据法定集合性质，Q和Q-j必然存在一个公共的节点，即Q中必然存在一个节点，该节点最终会接受Proposal-j，因此集合K包含Proposal-j。由K包含Proposal-j可知K中最大的提案proposal_id &amp;gt;= j；由CP1_2的前置条件3和K的定义可知如果K中存在proposal-id大于j的提案，那么该提案的值等于Proposal-j.v，因此K中proposal_id最大的提案的值等于Proposal-j.v。综上所述由CP1_2的前置条件可知：对于任意的一个法定集合Q，Q最终会接受的proposal_id小于i的提案组成的集合K，K中proposal_id最大的提案的值必定为Proposal-j.v。如果我们能证明该条件下，Proposal-i.v = Proposal-j.v，即可证明CP1_2。将CP1_2的前置条件替换为该条件，我们可以得到一个如下的性质CP2，保证CP2即可保证CP1_2：对于任意的一个法定集合Q，Q最终会接受的所有proposal_id小于i的提案组成的集合K，如果K中proposal_id最大的提案的值为Proposal-j.v;那么Proposal-i.v = Proposal-j.v。而引出满足CP2的流程就比较容易了，由于集合K中proposal_id最大的提案的值等于Proposal-j.v，看起来只要令Proposal-i的值为K中proposal-id最大提案的值就可以保证CP2。由于Q是任意一个法定集合，因此获取K似乎在实现上也不难，提出Proposal-i的提议者只要向Q中所有接受者询问即可。然后： CP2 —&amp;gt; CP1_2—&amp;gt; CP1 —&amp;gt;一致性但是实际上获取K没有那么简单，K包含的是Q所有最终接受的proposal-id小于i的的提案，不仅包含已经接受过的提案，还包括未来会接受的提案。获取已经接受过的提案是容易的，Q中的接受者只需记录它所有接受过的提案，当收到提出Proposal-i的提议者询问时，回复当中proposal_id小于i的提案即可;但是如何知晓未来？我们可以换个思路，既然无法知晓未来，那么我们约束未来，收到询问后，令Q中的接受者都承诺不再接受任何proposal_id小于i的提案，即接受者未来将不接受任何proposal_id小于i的提案;既然未来已不存在，那么Proposal-i的提议者根据Q的回复获能得到完整的K。于是协议的流程如下：对于提议者，在正式提案前，先向任意的法定集合Q发送一个消息，这个消息即是预提案，消息中要附带提案的proposal-id，作为接受者承诺和回复的依据。接受者收到预提案后，承诺：不再接受比预提案中附带的proposal-id更小的提案；并回复:已经接受的proposal-id比于提案的proposal-id更小的提案，如之前所论述的，回复的所有满足条件的提案可以优化为只回复一个比预提案proposal_id更小的提案中proposal_id最大的那个提案。提议者收到所有Q中接受者回复的提案后，挑选其中proposal_id最大的提案的值作为本次提案的值。这样我们就得到了Paxos中最为关键的几步，阅读了之前冗长的假装推导，相信读者很容易就能补全它得到完整的Paxos。相比于之前近万字的假装推导，这个推导过程才1500字左右，但是即说清了Paxos是如何得出的，又论证Paxos为何正确，简洁却更有力。所以最后还是建议真有兴趣的话去看下原文，在我看来它无疑是计算机领域那数不尽的论文中最值得阅读的那一类。末尾我所描述的版本思路来自«Paxos made simple»，基本一致但也并不完全相同；而« The Part-Time Parliament»则别有一番风味。最后需要注意的是Paxos并不完全满足开头解决一致性问题需要满足的三个条件中的3。理论上，Paxos存在永远无法达成一致的可能，哪怕是在所有进程都存活的情况下。想象一下这样的场景，一个提案Proposal-j被提出时，恰好一个proposal-id更大的预提案Proposal-i被提出，导致Proposal-j无法被通过，而Proposal-i同样的 又因为一个proposal_id更大的其它预提案被提出，导致无法被通过。这种情况理论上存在无限递归的可能，这个问题也称为活锁;FLP早就证明了就算是容忍一个进程的失败，异步环境下任何一致性算法都存在永不终止的可能。但是实际的工程中，很多手段可以来减小两个提案的冲突概率，使得v被决定的均摊开销是一个提案，多个提案还无法决定v值的情形是极小概率事件，且概率随着提案个数增加越来越小。另外的一点，通常认为Paxos可以容忍少数进程挂掉 ，但这只是为了保证它的活性，对于安全性，实际上Paxos永远满足1,2，哪怕进程都挂掉了，此时只是显然一致无法达成而已。&lt;/p&gt;

&lt;p&gt;数据库高可用性难题
数据库的数据一致和持续可用对电子商务和互联网金融的意义不言而喻，而这些业务在使用数据库时，无论 MySQL 还是 Oracle，都会面临一个艰难的取舍，就是如何处理主备库之间的数据同步。对于传统的主备模式或者一主多备模式，我们都需要考虑的问题，就是与备机保持强同步还是异步复制。&lt;/p&gt;

&lt;p&gt;对于强同步模式，要求主机必须把 Redolog 同步到备机之后，才能应答客户端，一旦主备之间出现网络抖动，或者备机宕机，则主机无法继续提供服务，这种模式实现了数据的强一致，但是牺牲了服务的可用性，且由于跨机房同步延迟过大使得跨机房的主备模式也变得不实用。&lt;/p&gt;

&lt;p&gt;而对于异步复制模式，主机写本地成功后，就可以立即应答客户端，无需等待备机应答，这样一旦主机宕机无法启动，少量不同步的日志将丢失，这种模式实现了服务持续可用，但是牺牲了数据一致性。这两种方式对应的就是 Oracle 的 Max Protection 和 Max Performance 模式，而 Oracle 另一个最常用的 Max Availability 模式，则是一个折中，在备机无应答时退化为 Max Performance 模式，我认为本质上还是异步复制。&lt;/p&gt;

&lt;p&gt;主备模式还有一个无法绕过的问题，就是选主，最简单山寨的办法，搞一个单点，定时 Select 一下主机和各个备机，貌似 MHA 就是这个原理，具体实现细节我就不太清楚了。一个改进的方案是使用类似 ZooKeeper 的多点服务替代单点，各个数据库机器上使用一个 Agent 与单点保持 Lease，主机 Lease 过期后，立即置为只读。改进的方案基本可以保证不会出现双主，而缺点是 ZooKeeper 的可维护性问题，以及多级 Lease 的恢复时长问题（这个本次就不展开讲了，感兴趣的同学请参考这篇文章 Http://oceanbase.org.cn/&lt;/p&gt;

&lt;p&gt;Paxos 协议简单回顾
主备方式处理数据库高可用问题有上述诸多缺陷，要改进这种数据同步方式，我们先来梳理下数据库高可用的几个基本需求：&lt;/p&gt;

&lt;p&gt;数据不丢失&lt;/p&gt;

&lt;p&gt;服务持续可用&lt;/p&gt;

&lt;p&gt;自动的主备切换&lt;/p&gt;

&lt;p&gt;使用Paxos协议的日志同步可以实现这三个需求，而 Paxos 协议需要依赖一个基本假设，主备之间有多数派机器（N / 2 + 1）存活并且他们之间的网络通信正常，如果不满足这个条件，则无法启动服务，数据也无法写入和读取。&lt;/p&gt;

&lt;p&gt;我们先来简单回顾一下 Paxos 协议的内容，首先，Paxos 协议是一个解决分布式系统中，多个节点之间就某个值（提案）达成一致（决议）的通信协议。它能够处理在少数派离线的情况下，剩余的多数派节点仍然能够达成一致。然后，再来看一下协议内容，它是一个两阶段的通信协议，推导过程我就不写了（中文资料请参考这篇 Http://t.cn/R40lGrp ），直接看最终协议内容：&lt;/p&gt;

&lt;p&gt;1、第一阶段 Prepare&lt;/p&gt;

&lt;p&gt;P1a：Proposer 发送 Prepare&lt;/p&gt;

&lt;p&gt;Proposer 生成全局唯一且递增的提案 ID（Proposalid，以高位时间戳 + 低位机器 IP 可以保证唯一性和递增性），向 Paxos 集群的所有机器发送 PrepareRequest，这里无需携带提案内容，只携带 Proposalid 即可。&lt;/p&gt;

&lt;p&gt;P1b：Acceptor 应答 Prepare
Acceptor 收到 PrepareRequest 后，做出“两个承诺，一个应答”。&lt;/p&gt;

&lt;p&gt;两个承诺：&lt;/p&gt;

&lt;p&gt;第一，不再应答 Proposalid 小于等于（注意：这里是 &amp;lt;= ）当前请求的 PrepareRequest；&lt;/p&gt;

&lt;p&gt;第二，不再应答 Proposalid 小于（注意：这里是 &amp;lt; ）当前请求的 AcceptRequest&lt;/p&gt;

&lt;p&gt;一个应答：&lt;/p&gt;

&lt;p&gt;返回自己已经 Accept 过的提案中 ProposalID 最大的那个提案的内容，如果没有则返回空值;&lt;/p&gt;

&lt;p&gt;注意：这“两个承诺”中，蕴含两个要点：&lt;/p&gt;

&lt;p&gt;就是应答当前请求前，也要按照“两个承诺”检查是否会违背之前处理 PrepareRequest 时做出的承诺；&lt;/p&gt;

&lt;p&gt;应答前要在本地持久化当前 Propsalid。&lt;/p&gt;

&lt;p&gt;2、第二阶段 Accept&lt;/p&gt;

&lt;p&gt;P2a：Proposer 发送 Accept
“提案生成规则”：Proposer 收集到多数派应答的 PrepareResponse 后，从中选择proposalid最大的提案内容，作为要发起 Accept 的提案，如果这个提案为空值，则可以自己随意决定提案内容。然后携带上当前 Proposalid，向 Paxos 集群的所有机器发送 AccpetRequest。&lt;/p&gt;

&lt;p&gt;P2b：Acceptor 应答 Accept&lt;/p&gt;

&lt;p&gt;Accpetor 收到 AccpetRequest 后，检查不违背自己之前作出的“两个承诺”情况下，持久化当前 Proposalid 和提案内容。最后 Proposer 收集到多数派应答的 AcceptResponse 后，形成决议。&lt;/p&gt;

&lt;p&gt;这里的“两个承诺”很重要，后面也会提及，请大家细细品味。&lt;/p&gt;

&lt;p&gt;Basic Paxos 同步日志的理论模型
上面是 Lamport 提出的算法理论，那么 Paxos 协议如何具体应用在 Redolog 同步上呢，我们先来看最简单的理论模型，就是在 N 个 Server的机群上，持久化数据库或者文件系统的操作日志，并且为每条日志分配连续递增的 LogID，允许多个客户端并发的向机群内的任意机器发送日志同步请求。在这个场景下，不同 Logid 标识的日志都是一个个相互独立的 Paxos Instance，每条日志独立执行完整的 Paxos 两阶段协议。&lt;/p&gt;

&lt;p&gt;因此在执行 Paxos 之前，需要先确定当前日志的 Logid，理论上对每条日志都可以从 1 开始尝试，直到成功持久化当前日志，但是为了降低失败概率，可以先向集群内的 Acceptor 查询他们 PrepareResponse 过的最大 Logid，从多数派的应答结果中选择最大的 Logi-d，加 1 后，作为本条日志的 Logid。然后以当前 Logid 标识 Paxos Instance，开始执行Paxos两阶段协议。可能出现的情况是，并发情况下，当前 Logid 被其他日志使用，那么在 P2a 阶段确定的提案内容可能就不是自己本次要同步的日志内容，这种情况下，就要重新决定logid，然后重新开始执行 Paxos 协议。&lt;/p&gt;

&lt;p&gt;考虑几种异常情况，Proposer 在 P1b 或 P2b 阶段没有收到多数派应答，可能是受到了其他 Logid 相同而 Proposalid 更大的 Proposer 干扰，或者是网络、机器等问题，这种情况下则要使用相同的 Logid，和新生成的 Proposalid 来重新执行 Paxos 协议。恢复时，按照 Logid 递增的顺序，针对每条日志执行完整 Paxos 协议成功后，形成决议的日志才可以进行回放。那么问题来了：比如 A/B/C 三个 Server，一条日志在 A/B 上持久化成功，已经形成多数派，然后B宕机；另一种情况，A/B/C 三个 Server，一条日志只在A 上持久化成功，超时未形成多数派，然后B宕机。上述两种情况，最终的状态都是 A 上有这条日志，C 上没有，那么应该怎么处理呢？&lt;/p&gt;

&lt;p&gt;这里提一个名词：“最大 Commit 原则”，这个阳振坤博士给我讲授 Paxos 时提出的名词，我觉得它是 Paxos 协议的最重要隐含规则之一，一条超时未形成多数派应答的提案，我们即不能认为它已形成决议，也不能认为它未形成决议，跟“薛定谔的猫”差不多，这条日志是“又死又活”的，只有当你观察它（执行 Paxos 协议）的时候，你才能得到确定的结果。因此对于上面的问题，答案就是无论如何都对这条日志重新执行 Paxos。这也是为什么在恢复的时候，我们要对每条日志都执行 Paxos 的原因。&lt;/p&gt;

&lt;p&gt;Multi Paxos 的实际应用
上述 Basic-Paxos 只是理论模型，在实际工程场景下，比如数据库同步 Redolog，还是需要集群内有一个 leader，作为数据库主机，和多个备机联合组成一个 Paoxs 集群，对 Redolog 进行持久化。此外持久化和回放时每条日志都执行完整 Paxos 协议（3 次网络交互，2 次本地持久化），代价过大，需要优化处理。因此使用 Multi-Paxos 协议，要实现如下几个重要功能：&lt;/p&gt;

&lt;p&gt;自动选主&lt;/p&gt;

&lt;p&gt;简化同步逻辑&lt;/p&gt;

&lt;p&gt;简化回放逻辑&lt;/p&gt;

&lt;p&gt;我在刚刚学习 Paxos 的时候，曾经认为选主就是跑一轮 Paxos 来形成“谁是 leader”的决议，其实并没有这么简单，因为 Paxos 协议的基本保证就是一旦形成决议，就不能更改，那么再次选新主就没办法处理了。因此对“选主”，需要变通一下思路，还是执行 Paxos 协议，但是我们并不关心决议内容，而是关心“谁成功得到了多数派的 AcceptResponse”，这个 Server 就是选主产生的 Leader。而多轮选主，就是针对同一个 Paxos Instance 反复执行，最后赢得多数派 Accept 的 Server 就是“当选 Leader”。&lt;/p&gt;

&lt;p&gt;不幸的是执行 Paxos 胜出的“当选 Leader”还不能算是真正的 Leader，只能算是“当选 Leader”，就像美国总统一样，“当选总统”是赢得选举的总统，但是任期还未开始他还不是真正的总统。在 Multi-Paxos 中因为可能存在多个 Server 先后赢得了选主，因此新的“当选leader”还要立即写出一条日志，以确认自己的 Leader 身份。这里就顺势引出日志同步逻辑的简化，我们将 Leader 选主看作 Paxos 的 Prepare 阶段，这个 Prepare 操作在逻辑上一次性的将后续所有即将产生的日志都执行 Prepare，因此在 Leader任期内的日志同步，都使用同一个 Proposalid，只执行 Accept 阶段即可。那么问题来了，各个备机在执行 Accept 的时候，需要注意什么？&lt;/p&gt;

&lt;p&gt;答案是上面提到过的“两个承诺”，因为我们已经把选主的那轮 Paxos 看做 Prepare 操作了，所以对于后续要 Accept 的日志，要遵守“两个承诺”。所以，对于先后胜出选主的多个“当选 Leader”，他们同步日志时携带的 Proposalid 的大小是不同的，只有最大的 Pro-posalid 能够同步日志成功，成为正式的 Leader。&lt;/p&gt;

&lt;p&gt;再进一步简化，选主 Leader 后，“当选 Leader”既然必先写一条日志来确认自己的 Leader身份，而协议允许多个“当选 Leader”产生，那么选主过程的本质其实就是为了拿到各个备机的“两个承诺”而已，选主过程本身产生的决议内容并没有实际意义，所以可以进一步简化为只执行 Prepare 阶段，而无需执行 Accept。&lt;/p&gt;

&lt;p&gt;再进一步优化，与 Raft 协议不同，Multi-Paxos 并不要求新任 Leader 本地拥有全部日志，因此新任 Leader 本地可能与其他 Server 相差了一些日志，它需要知道自己要补全哪些日志，因此它要向多数派查询各个机器上的 MaxLogD，以确定补全日志的结束 LogID。这个操作成为 GetMaxLogID，我们可以将这个操作与选主的 Prepare 操作搭车一起发出。这个优化并非 Multi-Paxos 的一部分，只是一个工程上比较有效的实现。&lt;/p&gt;

&lt;p&gt;回放逻辑的简化就比较好理解了，Leader 对每条形成多数派的日志，异步的写出一条“确认日志”即可，回放时如果一条日志拥有对应的“确认日志”，则不需要重新执行 Paoxs，直接回放即可。对于没有“确认日志”的，则需要重新执行 Paxos。工程上为了避免“确认日志”与对应的 Redolog 距离过大而带来回放的复杂度，往往使用滑动窗口机制来控制他们的距离。同时“确认日志”也用来提示备机可以回放收到的日志了。与 Raft 协议不同，由于 Multi-Paxos 允许日志不连续的确认（请思考：不连续确认的优势是什么？），以及允许任何成员都可以当选 Leader，因此新任 leader 需要补全自己本地缺失的日志，以及对未“确认”的日志重新执行 Paxos。我把这个过程叫做日志的“重确认”，本质上就是按照“最大commit原则”，使用当前最新的 Proposalid，逐条的对这些日志重新执行 Paxos，成功后再补上对应的“确认日志”。&lt;/p&gt;

&lt;p&gt;相对于 Raft 连续确认的特性，使用 Multi-Paxos 同步日志，由于多条日志间允许乱序确认，理论上会出现一种被称我们团队同学戏称为“幽灵复现”的诡异现象，如下图所示（图片引用自我的博客）&lt;/p&gt;

&lt;p&gt;第一轮中A被选为 Leader，写下了 1-10 号日志，其中 1-5 号日志形成了多数派，并且已给客户端应答，而对于 6-10 号日志，客户端超时未能得到应答。&lt;/p&gt;

&lt;p&gt;第二轮，A 宕机，B 被选为 Leader，由于 B 和 C 的最大的 LogID 都是 5，因此 B 不会去重确认 6 - 10 号日志，而是从 6 开始写新的日志，此时如果客户端来查询的话，是查询不到上一轮 6 - 10 号 日志内容的，此后第二轮又写入了 6 - 20 号日志，但是只有 6 号和 20 号日志在多数派。&lt;/p&gt;

&lt;p&gt;第三轮，A 又被选为 Leader，从多数派中可以得到最大 LogID 为 20，因此要将 7 - 20 号日志执行重确认，其中就包括了 A 上的 7-10 号日志，之后客户端再来查询的话，会发现上次查询不到的 7 - 10 号日志又像幽灵一样重新出现了。&lt;/p&gt;

&lt;p&gt;处理“幽灵复现”问题，需要依赖新任 Leader 在完成日志重确认，开始写入新的 Redolog 之前，写出一条被称为 StartWorking 的日志，这条日志的内容中记录了当前 Leader 的 EpochID（ 可以使用 Proposalid 的值），并且 Leader 每写一条日志都在日志内容中携带现任 Leader 的 EpochID。回放时，经过了一条 StartWorking 日志之后，再遇到 EpochID 比它小的日志，就直接忽略掉，比如按照上面例子画出的这张图，7 - 19 号日志要在回放时被忽略掉。&lt;/p&gt;

&lt;p&gt;依赖时钟误差的变种 Paxos 选主协议简单分析
阿里的阳振坤老师根据 Paxos 协议设计了一个简化版本的选主协议，相对 MultiPaxos 和 Raft 协议的优势在于，它不需要持久化任何数据，引入选主窗口的概念，使得大部分场景下集群内的所有机器能够几乎同时发起选主请求，便于投票时比对预定的优先级。下面的图引用自 OB 团队在公开场合分享 PPT 中的图片。&lt;/p&gt;

&lt;p&gt;如图所示，选主协议规定选主窗口开启是当前时间对一个T取余为0的时间，即只能在第 0，T，2T，3T…N*T 的时间点上开启选主窗口，协议将一次选主划分为三个阶段&lt;/p&gt;

&lt;p&gt;T1 预投票开始即由各个选举组成员向集群里的其他机器发送拉票请求；&lt;/p&gt;

&lt;p&gt;一段时间后进入 T2 预投票开始，选举组各个成员根据接受到的拉票请，从中选出优先级最高的，给它投票应答；&lt;/p&gt;

&lt;p&gt;一段时间后进入 T3 计票阶段，收到多数派投票的成员成为 leader，并向投票组其他成员发送自己上任的消息。&lt;/p&gt;

&lt;p&gt;假设时钟误差最大为 Tdiff，网络网路传输单程最长耗时为 Tst&lt;/p&gt;

&lt;p&gt;收到预投票消息的时间区间 [T1 - Tdiff × 2，T1 + Tdiff × 2 + Tst = T2]&lt;/p&gt;

&lt;p&gt;收到投票消息的时间区间 [T2 - Tdiff×2，T2 + Tdiff × 2 + Tst = T3]&lt;/p&gt;

&lt;p&gt;收到广播消息的时间区间 [T3 - Tdiff×2，T3 + Tdiff × 2 + Tst = T4]&lt;/p&gt;

&lt;p&gt;选主耗时 Telect = T4-T1 = Tdiff × 6 + Tst × 3&lt;/p&gt;

&lt;p&gt;因此最差情况下，选主开始后，经过 Tdiff × 6 + Tst × 3 的 d 时间，就可以选出 Leader 各个成员投出选票后，就从自己的 T1 时刻开始计时，认为 leader 持续 lease 时间内有效，在 Lease 有效期内，Leader 每隔 Telect 的时间就向其他成员发出续约请求，将 Lease 时间顺延一个 Telect，如果 Lease 过期后 Leader 没有续约，则各个成员等待下一个选主窗口到来后发起选主。因此最差情况下的无主时间是：Lease 时间 + Telect + 选主窗口间隔时间 T。&lt;/p&gt;

&lt;p&gt;这个选主算法相对 Paxos 和 Raft 更加简单，但是对时钟误差有比较强的依赖，时钟误差过大的情况下，会造成投票分裂无法选出主，甚至可能出现双主（不过话说任何保持 Leader 身份的 Lease 机制都得依赖时钟…），因此可能仅仅适合 BAT 这种配备了原子钟和 GPS 校准时钟，能够控制时钟误差在 100ms 以内的土豪机房。2015 年闰秒时，这个选主算法已经上线至支付宝，当时测试了几个月吧，1 秒的跳变已经太大，当时测试了几个月，修改 ntp 配置缓慢校准，最后平稳渡过。&lt;/p&gt;

&lt;p&gt;Q &amp;amp; A
1、ZooKeeper 所使用的 zad 协议与 Paxos 协议有什么区别？&lt;/p&gt;

&lt;p&gt;Zab 用的是Epoch 和 Count 的组合来唯一表示一个值, 而Raft 用的是 Term和 Index.&lt;/p&gt;

&lt;p&gt;Zab 的 Follower 在投票给一个 Leader 之前必须和 Leader 的日志达成一致,而 Raft的 Follower 则简单地说是谁的 Term 高就投票给谁。&lt;/p&gt;

&lt;p&gt;Raft 协 议的心跳是从 Leader 到 Follower, 而 zab 协议则相反。&lt;/p&gt;

&lt;p&gt;Raft 协议数据只有单向地从 Leader 到 Follower (成为 Leader 的条件之一就是拥有最新的 Log), 而 Zab 协议在 Discovery 阶段, 一个 Prospective Leader 需要将自己的Log 更新为 Quorum 里面最新的 Log,然后才好在 Synchronization 阶段将 Quorum 里的其他机器的 Log 都同步到一致。&lt;/p&gt;

&lt;p&gt;2、Paxos 能完成在全球同步的业务吗？理论上支持多少机器同步?
Paxos 成员组横跨全球的案例我还没有见过 Paper，我个人认为它并不适合全球不同，原因是延迟太大，但是 Google 的 Spanner 和 Amazon 的 Aurora 都实现了横跨北美多 IDC 的同步；理论上多少都行，你能接受延迟就可以。&lt;/p&gt;

&lt;p&gt;3、问个问题，能否简单说说 Raft 算法和 Paxos 算法的异同？应用场的异同？&lt;/p&gt;

&lt;p&gt;Raft 可以认为是一种简化的 Multi-Paxos 实现，他的最大简化之处在于备机接受 Leader 日志的前提是收到 LogID 连续的日志，在这个假设前提下，没有我文中提到的“幽灵复现”和“重确认”问题。简化带来的代价是对网络抖动的容忍度稍低一些，考虑这样的场景 ABC 三台机器，C 临时下线一会错过一些日志，然后 C上 线了，但是在 C 补全日志之前，AB 如果再宕机一台的话，服务就停了。&lt;/p&gt;

&lt;p&gt;4、Paxos 实现是独立的库或服务还是和具体的业务逻辑绑定，上线前如何验证 Paxos 算法实现的正确性？&lt;/p&gt;

&lt;p&gt;OB 实现的 Paxos 是和事务 Redolog 库比较紧耦合的，没有独立的库；测试方案一个是 Monkey tests，随机模拟各种异常环境，包括断网、网络延迟、机器宕机、包重复到达等情况保持压力和异常；另外一个是做了一个简易的虚拟机，来解释测试 Case，通过人工构造多种极端的场景，来是系统立即进入一个“梦境”。&lt;/p&gt;

&lt;p&gt;5、Logid 和 proposalid都应该是不能重复的，这个是如何保证的？原子钟的精确性仅仅是为了选主吗？&lt;/p&gt;

&lt;p&gt;首先，Leader 任期内，Logid 只由 Leader 产生，没有重复性的问题；&lt;/p&gt;

&lt;p&gt;第二，Leader 产生后，会执行 GetMaxLogID，从集群多数派拿到最大的 Logid，加以后作为本届任期内的 Logid 起点，这也可以保证有效日志 logid 不重复。Proposalid，高位使用 64 位时间戳，低位使用 IP 地址，可以保证唯一性和递增性。&lt;/p&gt;

&lt;p&gt;6、在用 Paxos 协议做 Master 和 Slave 一致性保证时，Paxos 日志回放应该怎样去做？&lt;/p&gt;

&lt;p&gt;Master 形成多数派确认后，异步的写出“确认日志”，Slave 回放到确认日志之后，才能去回放收到的正常日志。因此一般情况下，备机总是要落后主机一点点的。&lt;/p&gt;

&lt;p&gt;paxos是在多个成员之间对某个值(提议)达成一致的一致性协议。这个值可以是任何东西。比如多个成员之间进行选主，那么这个值就是主的身份。在把multi-paxos协议应用在日志同步中时，这个值就是一条日志。网上讲paxos的文章已经很多了，这里简要说明一下。&lt;/p&gt;

&lt;p&gt;paxos分为prepare和accept两个阶段。协议中有两个主要的角色，proposer和acceptor。&lt;/p&gt;

&lt;p&gt;value被majority accept之前，每个acceptor可以accept多个不同的值。但是，一旦一个value被majority accept(即value达成一致)，那么这个value就不会变了。因为prepare阶段会将该value给找出来，随后accept阶段会使用这个value，后续的所有的提案都会选择这个value。&lt;/p&gt;

&lt;p&gt;需要注意的是，每个阶段都是收到majority的响应后即开始处理。并且由于机器会宕机，acceptor需要对acceptedProposalID, acceptedValue和minProposal进行持久化。&lt;/p&gt;

&lt;p&gt;从流程中可以看出prepare有两个作用:&lt;/p&gt;

&lt;p&gt;大的proposal id会block未完成的小的proposal id达成一致的过程，所以为了减少无效的prepare请求，每次都选择比自己以往见过的proposal id更大的id。
一旦某个value达成一致，那么后续的prepare都会找到这个value作为accept阶段的值
可以看出，一次paxos达成一致至少需要两次网络交互。&lt;/p&gt;

&lt;p&gt;multi-paxos
paxos是对一个值达成一致，multi-paxos是运行多个paxos instance来对多个值达成一致，每个paxos instance对一个值达成一致。在一个支持多写并且强一致性的系统中，每个节点都可以接收客户端的写请求，生成redo日志然后开始一个paxos instance的prepare和accept过程。这里的问题是每条redo日志到底对应哪个paxos instance。&lt;/p&gt;

&lt;p&gt;在日志同步应用中，用log id来区分不同的paxos instance。每条日志都由一个id唯一标示，这个log id标识一个paxos instance，这个paxos instance达成一致，即对应的日志内容达成一致，即majority的成员accept了这个日志内容。在一个由N个机器(每个机器既承担proposer也承担acceptor角色)组成的集群(通常叫做paxos group)中，每个proposer都可以产生redo日志并且进行paxos instance，那么每条redo日志到底使用哪个log id? 显然，每个proposer都会选择自己知道的还没有达成一致的最小的log id来作为这次日志的log id，然后运行paxos协议，显然，多个proposer可能会选择同一个log id(最典型的场景就是空集群启动的情况下),最终，只有一个proposer能够成功，那么其他的proposer就需要选择更大的未达成一致的log id来运行paxos。显然，这种冲突是非常严重的，会有很多的proposer成功不了进而选择更大的log id来运行paxos。&lt;/p&gt;

&lt;p&gt;在真实的系统中，比如chubby, spanner，都会在paxos group中选择一个成员作为leader，只有leader能够propose日志，这样，prepare阶段就不会存在冲突，相当于对整个log文件做了一次prepare，后面这些日志都可以选用同一个proposal id。这样的话，每条日志只需要一次网络交互就能达成一致。回顾一下文章开头提到paxos中需要每个成员需要记录3个值，minProposal，acceptedProposal,acceptedValue，其中后面两个值可以直接记录在log中，而第一个值minProposal可以单独存在一个文件中。由于这里后面的日志都可以选用同一个proposal id，显然，在大部分时间内，minProposal都不需要改变。这正是multi-paxos的精髓&lt;/p&gt;

&lt;p&gt;选主
对于paxos来说，主的身份无所谓，主不需要像raft那样拥有最全的已经commit的日志。所以选主算法无所谓，比如大家都给机器ip最大的机器投票，或者给日志最多的投票，或者干脆直接运行一次paxos，值的内容就是主的身份。显然，由于对新主的身份无限制，那么，新主很有可能没有某些已经达成一致的日志，这个时候，就需要将这些已达成一致的日志拉过来，另外，新主也有可能没有某些还未达成一致的日志。如下图所示：&lt;/p&gt;

&lt;p&gt;图中，恢复之前，log id等于3的日志C已经在多数派上达成了一致，但是在新主上没有。比如log id等于4的日志D在多数派上没有达成一致，在新主上也没有。&lt;/p&gt;

&lt;p&gt;恢复
新主向所有成员发送查询最大log id的请求，收到majority的响应后，选择最大的log id作为日志恢复的结束点。图中，如果收到的majority不包括2号成员，那么log id=6为恢复结束点。如果收到的majority包括2号成员，那么log id=7为恢复结束点。这里取majority的意义在于恢复结束点包含任何的majority达成一致的日志。拿到log id后，从头开始扫描日志，对于每条日志都运行paxos协议确认一次：如果日志之前已经达成一致了，比如日志A，B，C，E，F，那么再次运行paxos的prepare阶段会把日志内容找出来作为accept阶段的值，不影响结果。如果日志之前并没有达成一致，比如日志D，那么当返回的majority中包含3号成员时，D会被选出来当作accept阶段的值，当返回的majority中不包含3号成员时，那么D实际上不会被选出，这时主可以选择一个dummy日志作为accept阶段的值。&lt;/p&gt;

&lt;p&gt;恢复优化
可以看出，如果日志非常多，每次重启后都要对每条日志做一次paxos，那么恢复时间可想而知。在上面的例子中，A,B,E已经达成一致，做了无用功。paxos协议中，只有主即proposer知道哪些日志达成了一致，acceptor不知道，那么很容易想到的一个优化就是proposer将已经达成一致的日志id告诉其他acceptor,acceptor写一条确认日志到日志文件中。后续重启的时候，扫描本地日志只要发现对应的确认日志就知道这条日志已经达成多数派，不需要重新使用paxos进行确认了。这种做法有一个问题，考虑如下场景:&lt;/p&gt;

&lt;p&gt;旧主成功的给自己和2号成员发送了确认日志，但是没有给3号成员发送成功旧挂了，然后2号成员被选为新主，那么新主不会对log id=3的日志重新运行paxos，因为本机已经存在确认日志。这样的话，3号成员就回放log id=3的日志到上层了。解决这个问题的做法就是followers需要主动的向主询问日志到底有没有达成一致，如果有，则自己补充确认日志。&lt;/p&gt;

&lt;p&gt;回放
宕机重启后，对未达成一致的日志重新运行paxos时，如log id=4的日志，如果返回的majority中不包含3号成员，那么日志D不会被找出来，这样就需要将3号成员的log id=4日志置未一条无操作的日志记作NOP日志，D最终也就不会形成多数派。由于multi-paxos允许日志乱序接收，并且日志的长度几乎都不一样，所以在磁盘上log id是乱序的，所以从物理上说，每个成员的日志不是一模一样的。那么要把log id=4的日志覆盖写成NOP日志也就比较麻烦，需要为每条日志维护索引。实现上可以不覆盖写，直接append一条log id=4的NOP日志到日志文件，这样没有问题，因为回放的时候只会回放能找到确认日志的日志到上层应用中。&lt;/p&gt;

&lt;p&gt;成员变更
multi-paxos处理成员变更比较简单，规定第i条日志参与paxos同步的时候，其成员组是第i-k条日志包含的成员组(每条日志里面都包含成员组)。&lt;/p&gt;

&lt;p&gt;multi-paxos协议之外
multi-paxos只是保证大家对日志达成一致。但是具体multi-paxos运用到真实的系统中时，从应用层面上看，可能会出现一些诡异的问题。考虑如下场景：&lt;/p&gt;

&lt;p&gt;如图，1号主写了A,B,C,D,其中B,C,D没有形成多数派，然后A宕机了，2号被选为了主，客户端过来读不到B,C,D，然后B没写任何东西，就挂了，这个时候，A起来后重新被选为主，对B,C,D重新运行paxos，把B,C,D达成了一致，这个时候客户端再次过来读，又能读到B,C,D了。对于multi-paxos本身来说，并没有什么不对的地方，但是上层应用的语义出现了问题：曾经读不到的东西，什么都没做，又能读到了。&lt;/p&gt;

&lt;p&gt;解决这个问题的方法是通过主提供服务之前必须成功写入一条start working日志来解决。如下图：&lt;/p&gt;

&lt;p&gt;如图，每个成员成为主提供服务之前都要首先写一条start working日志，只有达成多数派才能提供服务。1号在重新成为主之后，通过对log id=2的日志运行paxos，将2号start日志恢复了出来，然后对C和D运行paxos恢复出来后，后续回放的时候，如果发现后面日志中带有的timestamp(其实时leader上任时间)比start working带有的timestamp更小，那么就不回放到上层。随后客户端来读仍然读不到B,C,D，前后保持一致。&lt;/p&gt;

&lt;p&gt;1.CAP原理
要想数据高可用，就得写多份数据&lt;/p&gt;

&lt;p&gt;写多分数据就会导致数据一致性问题&lt;/p&gt;

&lt;p&gt;数据一致性问题会引起性能问题&lt;/p&gt;

&lt;p&gt;2.一致性模型
弱一致性&lt;/p&gt;

&lt;p&gt;最终一致性（一段时间达到一致性）&lt;/p&gt;

&lt;p&gt;强一致&lt;/p&gt;

&lt;p&gt;1、2 异步冗余；3是同步冗余&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;扩展服务的方案
数据分区： uid % 16&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;数据镜像：让多有的服务器都有相同的数据，提供相当的服务（冗余存储，一般3份为好）&lt;/p&gt;

&lt;p&gt;4.两种方案的事务问题
A向B汇钱，两个用户不在一个服务器上&lt;/p&gt;

&lt;p&gt;镜像：在不同的服务器上对同一数据的写操作如何保证一致性。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;解决一致性事务问题的技术&lt;/li&gt;
  &lt;li&gt;Master -Slave
读写请求由Master负责&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;写请求写到Master后，由Master同步到Slave上&lt;/p&gt;

&lt;p&gt;由Master push or Slave pull&lt;/p&gt;

&lt;p&gt;通常是由Slave 周期性来pull，所以是最终一致性&lt;/p&gt;

&lt;p&gt;问题： 若在 pull 周期内（不是期间？），master挂掉，那么会导致这个时间片内的数据丢失&lt;/p&gt;

&lt;p&gt;若不想让数据丢掉，Slave 只能成为 ReadOnly方式等Master恢复&lt;/p&gt;

&lt;p&gt;若容忍数据丢失，可以让 Slave代替Master工作&lt;/p&gt;

&lt;p&gt;如何保证强一致性？&lt;/p&gt;

&lt;p&gt;Master 写操作，写完成功后，再写 Slave，两者成功后返回成功。若 Slave失败，两种方法&lt;/p&gt;

&lt;p&gt;标记 Slave 不可用报错，并继续服务（等恢复后，再同步Master的数据，多个Slave少了一个而已）&lt;/p&gt;

&lt;p&gt;回滚自己并返回失败&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Master-Master
数据同步一般是通过 Master 间的异步完成，所以是最终一致&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;好处： 一台Master挂掉，另外一台照样可以提供读写服务。当数据没有被赋值到别的Master上时，数据会丢失。&lt;/p&gt;

&lt;p&gt;对同一数据的处理问题：Dynamo的Vector Clock的设计（记录数据的版本号和修改者），当数据发生冲突时，要开发者自己来处理&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;3.两阶段提交  Two  Phase Commit   _ 2PC
第一阶段：针对准备工作&lt;/p&gt;

&lt;p&gt;协调者问所有节点是否可以执行提交&lt;/p&gt;

&lt;p&gt;参与者开始事务，执行准备工作：锁定资源（获取锁操作）&lt;/p&gt;

&lt;p&gt;参与者响应协调者，如果事务的准备工作成功，则回应”可以提交”，否则，拒绝提交&lt;/p&gt;

&lt;p&gt;第二阶段：&lt;/p&gt;

&lt;p&gt;若都响应可以提交，则协调者项多有参与者发送正式提交的命令（更新值），参与者完成正式提交，释放资源，回应完成。协调者收到所有节点的完成响应后结束这个全局事务.。若参与者回应拒绝提交，则协调者向所有的参与者发送回滚操作，并释放资源，当收到全部节点的回滚回应后，取消全局事务&lt;/p&gt;

&lt;p&gt;存在的问题：若一个没提交，就会进行回滚&lt;/p&gt;

&lt;p&gt;第一阶段：若消息的传递未接收到，则需要协调者作超时处理，要么当做失败，要么重载&lt;/p&gt;

&lt;p&gt;第二阶段：若参与者的回应超时，要么重试，要么把那个参与者即为问题节点，提出整个集群&lt;/p&gt;

&lt;p&gt;在第二阶段中，参与者未收到协调者的指示（也许协调者挂掉），则所有参与者会进入“不知所措” 的状态（但是已经锁定了资源），所以引入了三段提交&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;三段提交：把二段提交的第一阶段 break 成了两段
询问&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;锁定资源（获取锁）&lt;/p&gt;

&lt;p&gt;提交&lt;/p&gt;

&lt;p&gt;核心理念：在询问的时候并不锁定资源，除非所有人都同意了，才开始锁定&lt;/p&gt;

&lt;p&gt;好处：当发生了失败或超时时，三段提交可以继续把状态变为Commit 状态，而二段提交则不知所措？&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Raxos 算法（少数服从多数）
解决的问题：在一个可能发生异常的分布式系统中如何就某个值达成一致，让整个集群的节点对某个值的变更达成一致&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;任何一个节点都可以提出要修改某个数据的提案,是否通过这个提案取决于这个集群中是否有超过半数的节点同意（所以节点数总是单数）—— 版本标记。虽然一致性，但是只能对一个操作进行操作啊？？&lt;/p&gt;

&lt;p&gt;当一个Server接收到比当前版本号小的提案时，则拒绝。当收到比当前大的版本号的提案时，则锁定资源，进行修改，返回OK.   也就是说收到超过一半的最大版本的提案才算成功。&lt;/p&gt;

&lt;p&gt;核心思想：&lt;/p&gt;

&lt;p&gt;在抢占式访问权的基础上引入多个acceptor，也就是说当一个版本号更大的提案可以剥夺版本号已经获取的锁。&lt;/p&gt;

&lt;p&gt;后者认同前者的原则：&lt;/p&gt;

&lt;p&gt;在肯定旧epoch 无法生成确定性取值时，新的 epoch 会提交自己的valu&lt;/p&gt;

&lt;p&gt;一旦 旧epoch形成确定性取值，新的 epoch肯定可以获取到此取值，并且会认同此取值，不会被破坏。&lt;/p&gt;

&lt;p&gt;步骤&lt;/p&gt;

&lt;p&gt;P1 请求Acceptor的 #1,Acceptor 这时并没有其他线程获取到锁，所以把锁交给 P1，并返回这时 #1 的值为null&lt;/p&gt;

&lt;p&gt;然后 P1 向 第一个 Acceptor 提交 #1 的值，Acceptor 接受并返回 OK&lt;/p&gt;

&lt;p&gt;这个时候，P2向Acceptor请求#1上的锁，因为版本号更大，所以直接抢占了 P1 的锁。这时 Acceptor 返回了 OK并且返回了 #1 的值&lt;/p&gt;

&lt;p&gt;这时 P1 P向 后面两个 Acceptor 提交 #1 的值，但是由于中间的那个Acceptor 版本号已经更改为 2 了，所以拒绝P1。第三个 Acceptor 接受了，并且返回了 OK&lt;/p&gt;

&lt;p&gt;由于后者认同前者的原则，这时 P1 已经形成确定性取值了 V1 了，这时新的 P2 会认同此取值，而不是提交自己的取值。所以，P2会选择最新的那个取值 也就是V1 进行提交。这时Acceptor 返回 OK&lt;/p&gt;

&lt;p&gt;6.ZAB 协议 ( Zookeeper Atomic  Broadcast) 原子广播协议：保证了发给各副本的消息顺序相同
定义：原子广播协议 ZAB 是一致性协议，Zookeeper 把其作为数据一致性的算法。ZAB 是在 Paxos 算法基础上进行扩展而来的。Zookeeper 使用单一主进程 Leader用于处理客户端所有事务请求，采用 ZAB 协议将服务器状态以事务形式广播到所有 Follower 上，由于事务间可能存在着依赖关系，ZAB协议保证 Leader 广播的变更序列被顺序的处理，一个状态被处理那么它所依赖的状态也已经提前被处理&lt;/p&gt;

&lt;p&gt;核心思想：保证任意时刻只有一个节点是Leader，所有更新事务由Leader发起去更新所有副本 Follower，更新时用的是 两段提交协议，只要多数节点 prepare 成功，就通知他们commit。各个follower 要按当初 leader 让他们 prepare 的顺序来 apply 事务&lt;/p&gt;

&lt;p&gt;协议状态&lt;/p&gt;

&lt;p&gt;Looking:系统刚启动时 或者 Leader 崩溃后正处于选举状态&lt;/p&gt;

&lt;p&gt;Following：Follower 节点所处的状态，Follower与 Leader处于数据同步状态&lt;/p&gt;

&lt;p&gt;Leading：Leader 所处状态，当前集群中有一个 Leader 为主进程&lt;/p&gt;

&lt;p&gt;ZooKeeper启动时所有节点初始状态为Looking，这时集群会尝试选举出一个Leader节点，选举出的Leader节点切换为Leading状态；当节点发现集群中已经选举出Leader则该节点会切换到Following状态，然后和Leader节点保持同步；当Follower节点与Leader失去联系时Follower节点则会切换到Looking状态，开始新一轮选举；在ZooKeeper的整个生命周期中每个节点都会在Looking、Following、Leading状态间不断转换。&lt;/p&gt;

&lt;p&gt;选举出Leader节点后 ZAB 进入原子广播阶段，这时Leader为和自己同步每个节点 Follower 创建一个操作序列，一个时期一个 Follower 只能和一个Leader保持同步&lt;/p&gt;

&lt;p&gt;阶段&lt;/p&gt;

&lt;p&gt;Election： 在 Looking状态中选举出 Leader节点，Leader的LastZXID总是最新的（只有lastZXID的节点才有资格成为Leade,这种情况下选举出来的Leader总有最新的事务日志）。在选举的过程中会对每个Follower节点的ZXID进行对比只有highestZXID的Follower才可能当选Leader&lt;/p&gt;

&lt;p&gt;每个Follower都向其他节点发送选自身为Leader的Vote投票请求，等待回复；&lt;/p&gt;

&lt;p&gt;Follower接受到的Vote如果比自身的大（ZXID更新）时则投票，并更新自身的Vote，否则拒绝投票；&lt;/p&gt;

&lt;p&gt;每个Follower中维护着一个投票记录表，当某个节点收到过半的投票时，结束投票并把该Follower选为Leader，投票结束；&lt;/p&gt;

&lt;p&gt;Discovery:Follower 节点向准 Leader推送 FollwerInfo,该信息包含了上一周期的epoch，接受准 Leader 的 NEWLEADER 指令&lt;/p&gt;

&lt;p&gt;Sync：将 Follower 与 Leader的数据进行同步，由Leader发起同步指令，最终保持数据的一致性&lt;/p&gt;

&lt;p&gt;Broadcast：Leader广播 Proposal 与 Commit，Follower 接受 Proposal 与 commit。因为一个时刻只有一个Leader节点，若是更新请求，只能由Leader节点执行（若连到的是 Follower 节点，则需转发到Leader节点执行；读请求可以从Follower 上读取，若是要最新的数据，则还是需要在 Leader上读取）&lt;/p&gt;

&lt;p&gt;消息广播使用了TCP协议进行通讯所有保证了接受和发送事务的顺序性。广播消息时Leader节点为每个事务Proposal分配一个全局递增的ZXID（事务ID），每个事务Proposal都按照ZXID顺序来处理（Paxos 保证不了）&lt;/p&gt;

&lt;p&gt;Leader节点为每一个Follower节点分配一个队列按事务ZXID顺序放入到队列中，且根据队列的规则FIFO来进行事务的发送。&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;Recovery ：根据Leader的事务日志对Follower 节点数据进行同步更新&lt;/p&gt;

&lt;p&gt;同步策略：&lt;/p&gt;

&lt;p&gt;SNAP ：如果Follower数据太老，Leader将发送快照SNAP指令给Follower同步数据；&lt;/p&gt;

&lt;p&gt;DIFF ：Leader发送从Follolwer.lastZXID到Leader.lastZXID议案的DIFF指令给Follower同步数据；&lt;/p&gt;

&lt;p&gt;TRUNC ：当Follower.lastZXID比Leader.lastZXID大时，Leader发送从Leader.lastZXID到Follower.lastZXID的TRUNC指令让Follower丢弃该段数据；（当老Leader在Commit前挂掉，但是已提交到本地）&lt;/p&gt;

&lt;p&gt;Follower将所有事务都同步完成后Leader会把该节点添加到可用Follower列表中；&lt;/p&gt;

&lt;p&gt;Follower接收Leader的NEWLEADER指令，如果该指令中epoch比当前Follower的epoch小那么Follower转到Election阶段&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Raft 算法
Raft 算法也是一种少数服从多数的算法，在任何时候一个服务器可以扮演以下角色之一：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Leader：负责 Client 交互 和 log 复制，同一时刻系统中最多存在一个&lt;/p&gt;

&lt;p&gt;Follower：被动响应请求 RPC，从不主动发起请求 RPC&lt;/p&gt;

&lt;p&gt;Candidate : 由Follower 向Leader转换的中间状态&lt;/p&gt;

&lt;p&gt;在选举Leader的过程中，是有时间限制的，raft 将时间分为一个个 Term，可以认为是“逻辑时间”：&lt;/p&gt;

&lt;p&gt;每个 Term中至多存在1个 Leader&lt;/p&gt;

&lt;p&gt;某些 Term由于不止一个得到的票数一样，就会选举失败，不存在Leader。则会出现 Split Vote  ，再由候选者发出邀票&lt;/p&gt;

&lt;p&gt;每个 Server 本地维护 currentTerm&lt;/p&gt;

&lt;p&gt;选举过程：&lt;/p&gt;

&lt;p&gt;自增 CurrentTerm，由Follower 转换为 Candidate，设置 votedFor 为自身，并行发起 RequestVote RPC,不断重试，直至满足下列条件之一为止：&lt;/p&gt;

&lt;p&gt;获得超过半数的Server的投票，转换为 Leader，广播 HeatBeat&lt;/p&gt;

&lt;p&gt;接收到 合法 Leader 的 AppendEnties RPC，转换为Follower&lt;/p&gt;

&lt;p&gt;选举超时，没有 Server选举成功，自增 currentTerm ,重新选举&lt;/p&gt;

&lt;p&gt;当Candidate 在等待投票结果的过程中，可能会接收到来自其他Leader的 AppendEntries RPC ,如果该 Leader 的 Term 不小于本地的 Current Term，则认可该Leader身份的合法性，主动降级为Follower，反之，则维持 candida 身份继续等待投票结果&lt;/p&gt;

&lt;p&gt;Candidate 既没有选举成功，也没有收到其他 Leader 的 RPC (多个节点同时发起选举，最终每个 Candidate都将超时)，为了减少冲突，采取随机退让策略，每个 Candidate 重启选举定时器&lt;/p&gt;

&lt;p&gt;日志更新问题：&lt;/p&gt;

&lt;p&gt;如果在日志复制过程中，发生了网络分区或者网络通信故障，使得Leader不能访问大多数Follwers了，那么Leader只能正常更新它能访问的那些Follower服务器，而大多数的服务器Follower因为没有了Leader，他们重新选举一个候选者作为Leader，然后这个Leader作为代表于外界打交道，如果外界要求其添加新的日志，这个新的Leader就按上述步骤通知大多数Followers，如果这时网络故障修复了，那么原先的Leader就变成Follower，在失联阶段这个老Leader的任何更新都不能算commit，都回滚，接受新的Leader的新的更新。&lt;/p&gt;

&lt;p&gt;流程：&lt;/p&gt;

&lt;p&gt;Client 发送command 命令给 Leader&lt;/p&gt;

&lt;p&gt;Leader追加日志项，等待 commit 更新本地状态机，最终响应 Client&lt;/p&gt;

&lt;p&gt;若 Client超时，则不断重试，直到收到响应为止（重发 command，可能被执行多次，在被执行但是由于网络通信问题未收到响应）&lt;/p&gt;

&lt;p&gt;解决办法：Client 赋予每个 Command唯一标识，Leader在接收 command 之前首先检查本地log&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;paxos 算法与 raft 算法的差异
raft强调是唯一leader的协议，此leader至高无上&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;raft：新选举出来的leader拥有全部提交的日志，而 paxos 需要额外的流程从其他节点获取已经被提交的日志，它允许日志有空洞&lt;/p&gt;

&lt;p&gt;相同点：得到大多数的赞成，这个 entries 就会定下来，最终所有节点都会赞成&lt;/p&gt;

&lt;p&gt;NWR模型
N： N个备份&lt;/p&gt;

&lt;p&gt;W：要写入至少 w 份才认为成功&lt;/p&gt;

&lt;p&gt;R : 至少读取 R 个备份&lt;/p&gt;

&lt;p&gt;W+ R &amp;gt; N    ——&amp;gt;    R &amp;gt; N - W(未更新成功的) ，代表每次读取，都至少读取到一个最新的版本（更新成功的），从而不会读到一份旧数据&lt;/p&gt;

&lt;p&gt;问题：并非强一致性，会出现一些节点上的数据并不是最新版本，但却进行了最新的操作&lt;/p&gt;

&lt;p&gt;版本冲突问题：矢量钟 Vector Clock ： 谁更新的我，我的版本号是什么（对于同一个操作者的同一操作，版本号递增）&lt;/p&gt;

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;http://www.tuicool.com/articles/IfQR3u3&lt;/p&gt;

&lt;p&gt;http://blog.csdn.net/chen77716/article/details/7309915&lt;/p&gt;

&lt;p&gt;http://www.infoq.com/cn/articles/distributed-system-transaction-processing/&lt;/p&gt;

&lt;p&gt;http://www.jdon.com/artichect/raft.html&lt;/p&gt;

&lt;p&gt;http://blog.csdn.net/cszhouwei/article/details/38374603&lt;/p&gt;

</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/04/paxos_raft.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/04/paxos_raft.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>p2p</title>
        <description>&lt;p&gt;1.1 Peer-To-Peer 介绍&lt;/p&gt;

&lt;p&gt;最近几年，对等计算( Peer-to-Peer，简称P2P) 迅速成为计算机界关注的热门话题之一，财富杂志更将P2P列为影响Internet未来的四项科技之一。&lt;/p&gt;

&lt;p&gt;目前,在学术界、工业界对于P2P没有一个统一的定义，下面列举几个常用的定义供参考：&lt;/p&gt;

&lt;p&gt;定义:1、Peer-to-peer is a type of Internet network allowing a group of computer users with the same networking program to connect with each other for the purposes of directly accessing files from one another’s hard drives.&lt;/p&gt;

&lt;p&gt;2、Peer-to-peer networking (P2P) is an application that runs on a personal computer and shares files with other users across the Internet. P2P networks work by connecting individual computers together to share files instead of having to go through a central server.&lt;/p&gt;

&lt;p&gt;3、P2P是一种分布式网络，网络的参与者共享他们所拥有的一部分硬件资源（处理能力、存储能力、网络连接能力、打印机等），这些共享资源需要由网络提供服务和内容，能被其它对等节点(Peer)直接访问而无需经过中间实体。在此网络中的参与者既是资源（服务和内容）提供者（Server），又是资源（服务和内容）获取者（Client）。&lt;/p&gt;

&lt;p&gt;虽然上述定义稍有不同，但共同点都是P2P打破了传统的Client/Server (C/S)模式，在网络中的每个结点的地位都是对等的。每个结点既充当服务器，为其他结点提供服务，同时也享用其他结点提供的服务。P2P与C/S模式的对比如下图所示:&lt;/p&gt;

&lt;p&gt;Client/Server模式&lt;/p&gt;

&lt;p&gt;Peer to Peer 模式&lt;/p&gt;

&lt;p&gt;P2P技术的特点体现在以下几个方面。&lt;/p&gt;

&lt;p&gt;非中心化（Decentralization）：网络中的资源和服务分散在所有结点上，信息的传输和服务的实现都直接在结点之间进行，可以无需中间环节和服务器的介入，避免了可能的瓶颈。P2P的非中心化基本特点，带来了其在可扩展性、健壮性等方面的优势。
可扩展性：在P2P网络中，随着用户的加入，不仅服务的需求增加了，系统整体的资源和服务能力也在同步地扩充，始终能较容易地满足用户的需要。整个体系是全分布的，不存在瓶颈。理论上其可扩展性几乎可以认为是无限的。
健壮性：P2P架构天生具有耐攻击、高容错的优点。由于服务是分散在各个结点之间进行的，部分结点或网络遭到破坏对其它部分的影响很小。P2P网络一般在部分结点失效时能够自动调整整体拓扑，保持其它结点的连通性。P2P网络通常都是以自组织的方式建立起来的，并允许结点自由地加入和离开。P2P网络还能够根据网络带宽、结点数、负载等变化不断地做自适应式的调整。
高性能/价格比：性能优势是P2P被广泛关注的一个重要原因。随着硬件技术的发展，个人计算机的计算和存储能力以及网络带宽等性能依照摩尔定理高速增长。采用P2P架构可以有效地利用互联网中散布的大量普通结点，将计算任务或存储资料分布到所有结点上。利用其中闲置的计算能力或存储空间，达到高性能计算和海量存储的目的。通过利用网络中的大量空闲资源，可以用更低的成本提供更高的计算和存储能力。
隐私保护: 在P2P网络中，由于信息的传输分散在各节点之间进行而无需经过某个集中环节，用户的隐私信息被窃听和泄漏的可能性大大缩小。此外，目前解决Internet隐私问题主要采用中继转发的技术方法，从而将通信的参与者隐藏在众多的网络实体之中。在传统的一些匿名通信系统中，实现这一机制依赖于某些中继服务器节点。而在P2P中，所有参与者都可以提供中继转发的功能，因而大大提高了匿名通讯的灵活性和可靠性，能够为用户提供更好的隐私保护。
负载均衡: Peer to Peer网络环境下由于每个节点既是服务器又是客户机，减少了对传统C/S结构服务器计算能力、存储能力的要求，同时因为资源分布在多个节点，更好的实现了整个网络的负载均衡。
与传统的分布式系统相比，P2P技术具有无可比拟的优势。同时，P2P技术具有广阔的应用前景。Internt上各种P2P应用软件层出不穷，用户数量急剧增加。2004年3月来自www.slyck.com的数据显示，大量P2P软件的用户使用数量分布从几十万、几百万到上千万并且急剧增加，并给Internet带宽带来巨大冲击。P2P计算技术正不断应用到军事领域，商业领域，政府信息，通讯等领域。&lt;/p&gt;

&lt;p&gt;根据具体应用不同，可以把P2P分为以下这些类型：&lt;/p&gt;

&lt;p&gt;提供文件和其它内容共享的P2P网络，例如Napster、Gnutella、eDonkey、emule、BitTorrent等；
挖掘P2P对等计算能力和存储共享能力，例如SETI@home、Avaki、Popular Power等；
基于P2P方式的协同处理与服务共享平台，例如JXTA、Magi、Groove、.NET My Service等；
即时通讯交流，包括ICQ、OICQ、Yahoo Messenger等；
安全的P2P通讯与信息共享，例如Skype、Crowds、Onion Routing等。
　&lt;/p&gt;

&lt;p&gt;1.2国内外P2P技术研究现状&lt;/p&gt;

&lt;p&gt;1.2.1 P2P网络中的拓扑结构研究&lt;/p&gt;

&lt;p&gt;拓扑结构是指分布式系统中各个计算单元之间的物理或逻辑的互联关系，结点之间的拓扑结构一直是确定系统类型的重要依据。目前互联网络中广泛使用集中式、层次式等拓扑结构，Interne本身是世界上最大的非集中式的互联网络，但是九十年代所建立的一些网络应用系统却是完全的集中式的系统、很多Web应用都是运行在集中式的服务器系统上。集中式拓扑结构系统目前面临着过量存储负载、Dos攻击等一些难以解决的问题。&lt;/p&gt;

&lt;p&gt;P2P系统一般要构造一个非集中式的拓扑结构，在构造过程中需要解决系统中所包含的大量结点如何命名、组织以及确定结点的加入/离开方式、出错恢复等问题。&lt;/p&gt;

&lt;p&gt;根据拓扑结构的关系可以将P2P研究分为4种形式：中心化拓扑（Centralized Topology）；全分布式非结构化拓扑（Decentralized Unstructured Topology）；全分布式结构化拓扑（Decentralized Structured Topology，也称作DHT网络）和半分布式拓扑（Partially Decentralized Topology）。&lt;/p&gt;

&lt;p&gt;其中，中心化拓扑最大的优点是维护简单发现效率高。由于资源的发现依赖中心化的目录系统，发现算法灵活高效并能够实现复杂查询。最大的问题与传统客户机/服务器结构类似，容易造成单点故障，访问的“热点”现象和法律等相关问题，这是第一代P2P网络采用的结构模式，经典案例就是著名的MP3共享软件Napster。&lt;/p&gt;

&lt;p&gt;Napster是最早出现的P2P系统之一，并在短期内迅速成长起来。Napster实质上并非是纯粹的P2P系统，它通过一个中央服务器保存所有Napster用户上传的音乐文件索引和存放位置的信息。当某个用户需要某个音乐文件时，首先连接到Napster服务器，在服务器进行检索，并由服务器返回存有该文件的用户信息；再由请求者直接连到文件的所有者传输文件。&lt;/p&gt;

&lt;p&gt;Napster首先实现了文件查询与文件传输的分离，有效地节省了中央服务器的带宽消耗，减少了系统的文件传输延时。这种方式最大的隐患在中央服务器上，如果该服务器失效，整个系统都会瘫痪。当用户数量增加到105或者更高时，Napster的系统性能会大大下降。另一个问题在于安全性上，Napster并没有提供有效的安全机制。&lt;/p&gt;

&lt;p&gt;在Napster模型中，一群高性能的中央服务器保存着网络中所有活动对等计算机共享资源的目录信息。当需要查询某个文件时，对等机会向一台中央服务器发出文件查询请求。中央服务器进行相应的检索和查询后，会返回符合查询要求的对等机地址信息列表。查询发起对等机接收到应答后，会根据网络流量和延迟等信息进行选择，和合适的对等机建立连接，并开始文件传输。Napster的工作原理如图1所示。&lt;/p&gt;

&lt;p&gt;这种对等网络模型存在很多问题，主要表现为：&lt;/p&gt;

&lt;p&gt;(1)中央服务器的瘫痪容易导致整个网络的崩馈，可靠性和安全性较低。&lt;/p&gt;

&lt;p&gt;(2)随着网络规模的扩大，对中央索引服务器进行维护和更新的费用将急剧增加，所需成本过高。&lt;/p&gt;

&lt;p&gt;(3)中央服务器的存在引起共享资源在版权问题上的纠纷，并因此被攻击为非纯粹意义上的P2P网络模型。对小型网络而言，集中目录式模型在管理和控制方面占一定优势。但鉴于其存在的种种缺陷，该模型并不适合大型网络应用。&lt;/p&gt;

&lt;p&gt;Napster结构&lt;/p&gt;

&lt;p&gt;Napster界面&lt;/p&gt;

&lt;p&gt;全分布非结构化网络在重叠网络（overlay）采用了随机图的组织方式，结点度数服从”Power-law”[a][b]规律，从而能够较快发现目的结点，面对网络的动态变化体现了较好的容错能力，因此具有较好的可用性。同时可以支持复杂查询，如带有规则表达式的多关键词查询，模糊查询等，最典型的案例是Gnutella。&lt;/p&gt;

&lt;p&gt;Gnutella是一个P2P文件共享系统，它和Napster最大的区别在于Gnutella是纯粹的P2P系统，没有索引服务器，它采用了基于完全随机图的洪泛（Flooding）发现和随机转发（Random Walker）机制。为了控制搜索消息的传输，通过TTL (Time To Live)的减值来实现。具体协议参照［Gnutella协议中文版］&lt;/p&gt;

&lt;p&gt;在Gnutella分布式对等网络模型N中，每一个联网计算机在功能上都是对等的，既是客户机同时又是服务器，所以被称为对等机(Servent，Server+Client的组合)。&lt;/p&gt;

&lt;p&gt;随着联网节点的不断增多，网络规模不断扩大，通过这种洪泛方式定位对等点的方法将造成网络流量急剧增加，从而导致网络中部分低带宽节点因网络资源过载而失效。所以在初期的Gnutella网络中，存在比较严重的分区，断链现象。也就是说，一个查询访问只能在网络的很小一部分进行，因此网络的可扩展性不好。所以，解决Gnutella网络的可扩展性对该网络的进一步发展至关重要。&lt;/p&gt;

&lt;p&gt;由于没有确定拓扑结构的支持，非结构化网络无法保证资源发现的效率。即使需要查找的目的结点存在发现也有可能失败。由于采用TTL（Time-to-Live）、洪泛（Flooding）、随机漫步或有选择转发算法，因此直径不可控，可扩展性较差。&lt;/p&gt;

&lt;p&gt;因此发现的准确性和可扩展性是非结构化网络面临的两个重要问题。目前对此类结构的研究主要集中于改进发现算法和复制策略以提高发现的准确率和性能。&lt;/p&gt;

&lt;p&gt;最初的Gnutella采用的Flooding搜索算法示意图&lt;/p&gt;

&lt;p&gt;采用第二代Gnutella协议最经典的软件-Bearshare&lt;/p&gt;

&lt;p&gt;由于非结构化网络将重叠网络认为是一个完全随机图，结点之间的链路没有遵循某些预先定义的拓扑来构建。这些系统一般不提供性能保证，但容错性好，支持复杂的查询，并受结点频繁加入和退出系统的影响小。但是查询的结果可能不完全，查询速度较慢，采用广播查询的系统对网络带宽的消耗非常大，并由此带来可扩展性差等问题。&lt;/p&gt;

&lt;p&gt;另外，由于非结构化系统中的随机搜索造成的不可扩展性，大量的研究集中在如何构造一个高度结构化的系统。目前研究的重点放在了如何有效地查找信息上，最新的成果都是基于DHT的分布式发现和路由算法。这些算法都避免了类似Napster的中央服务器，也不是像Gnutella那样基于广播进行查找，而是通过分布式散列函数，将输入的关键字惟一映射到某个结点上，然后通过某些路由算法同该结点建立连接。&lt;/p&gt;

&lt;p&gt;最新的研究成果体现在采用分布式散列表（DHT）[a]的完全分布式结构化拓扑网络。&lt;/p&gt;

&lt;p&gt;分布式散列表（DHT）实际上是一个由广域范围大量结点共同维护的巨大散列表。散列表被分割成不连续的块，每个结点被分配给一个属于自己的散列块，并成为这个散列块的管理者。DHT的结点既是动态的结点数量也是巨大的，因此非中心化和原子自组织成为两个设计的重要目标。通过加密散列函数，一个对象的名字或关键词被映射为128位或160位的散列值。一个采用DHT的系统内所有结点被映射到一个空间 ，如果散列函数映射一个 位的名字到一个散列值 ，则有 。&lt;/p&gt;

&lt;p&gt;分布式散列表起源于SDDS（Scalable Distribute Data Structures）[a]研究，Gribble等实现了一个高度可扩展，容错的SDDS集群。&lt;/p&gt;

&lt;p&gt;最近的研究集中在采用新的拓扑图构建重叠路由网络，以减少路由表容量和路由延时。这些新的拓扑关系的基本原理是在DHT表一维空间的基础上引入更多的拓扑结构图来反映底层网络的结构。&lt;/p&gt;

&lt;p&gt;DHT类结构能够自适应结点的动态加入/退出，有着良好的可扩展性、鲁棒性、结点ID分配的均匀性和自组织能力。由于重叠网络采用了确定性拓扑结构，DHT可以提供精确的发现。只要目的结点存在于网络中DHT总能发现它，发现的准确性得到了保证，最经典的案例是Tapestry，Chord,CAN,和Pastry。&lt;/p&gt;

&lt;p&gt;Tapestry提供了一个分布式容错查找和路由基础平台，在此平台基础之上，可以开发各种P2P应用(OceanStore即是此平台上的一个应用) 。Tapestry的思想来源于Plaxton。在Plaxton中，结点使用自己所知道的邻近结点表，按照目的ID来逐步传递消息。Tapestry基于Plaxtion的思想，加入了容错机制，从而可适应P2P的动态变化的特点。OceanStore是以Tapestry为路由和查找基础设施的P2P平台。它是一个适合于全球数据存储的P2P应用系统。任何用户均可以加入OceanStore系统，或者共享自己的存储空间，或者使用该系统中的资源。通过使用复制和缓存技术，OceanStore可提高查找的效率。最近，Tapstry为适应P2P网络的动态特性，作了很多改进，增加了额外的机制实现了网络的软状态（soft state），并提供了自组织、鲁棒性、可扩展性和动态适应性，当网络高负载且有失效结点时候性能有限降低，消除了对全局信息的依赖、根结点易失效和弹性（resilience）差的问题。&lt;/p&gt;

&lt;p&gt;Pastry是微软研究院提出的可扩展的分布式对象定位和路由协议，可用于构建大规模的P2P系统。在Pastry中，每个结点分配一个128位的结点标识符号(nodeID) ，所有的结点标识符形成了一个环形的nodeID空间，范围从0到2128 - 1 ，结点加入系统时通过散列结点IP地址在128位nodeID空间中随机分配。&lt;/p&gt;

&lt;p&gt;在MIT，开展了多个与P2P相关的研究项目：Chord，GRID和RON。Chord项目的目标是提供一个适合于P2P环境的分布式资源发现服务，它通过使用DHT技术使得发现指定对象只需要维护O(logN)长度的路由表。&lt;/p&gt;

&lt;p&gt;在DHT技术中，网络结点按照一定的方式分配一个唯一结点标识符(Node ID) ，资源对象通过散列运算产生一个唯一的资源标识符(Object ID) ，且该资源将存储在结点ID与之相等或者相近的结点上。需要查找该资源时，采用同样的方法可定位到存储该资源的结点。因此，Chord的主要贡献是提出了一个分布式查找协议，该协议可将指定的关键字(Key) 映射到对应的结点(Node) 。从算法来看，Chord是相容散列算法的变体。MIT的GRID和RON项目则提出了在分布式广域网中实施查找资源的系统框架。&lt;/p&gt;

&lt;p&gt;AT&amp;amp;T ACIRI中心的CAN(Content Addressable Networks) 项目独特之处在于采用多维的标识符空间来实现分布式散列算法。CAN将所有结点映射到一个n维的笛卡尔空间中，并为每个结点尽可能均匀的分配一块区域。CAN采用的散列函数通过对(key, value) 对中的key进行散列运算，得到笛卡尔空间中的一个点，并将(key, value) 对存储在拥有该点所在区域的结点内。CAN采用的路由算法相当直接和简单，知道目标点的坐标后，就将请求传给当前结点四邻中坐标最接近目标点的结点。CAN是一个具有良好可扩展性的系统，给定N个结点，系统维数为d，则路由路径长度为O(n1/d) ，每结点维护的路由表信息和网络规模无关为O(d) 。&lt;/p&gt;

&lt;p&gt;DHT类结构最大的问题是DHT的维护机制较为复杂，尤其是结点频繁加入退出造成的网络波动（Churn）会极大增加DHT的维护代价。DHT所面临的另外一个问题是DHT仅支持精确关键词匹配查询，无法支持内容/语义等复杂查询。&lt;/p&gt;

&lt;p&gt;Chord的Identifier Circle&lt;/p&gt;

&lt;p&gt;Pastry的消息路由&lt;/p&gt;

&lt;p&gt;半分布式结构（有的文献称作 Hybrid Structure）吸取了中心化结构和全分布式非结构化拓扑的优点，选择性能较高（处理、存储、带宽等方面性能）的结点作为超级点（英文文献中多称作：SuperNodes, Hubs)，在各个超级点上存储了系统中其他部分结点的信息，发现算法仅在超级点之间转发，超级点再将查询请求转发给适当的叶子结点。半分布式结构也是一个层次式结构，超级点之间构成一个高速转发层，超级点和所负责的普通结点构成若干层次。最典型的案例就是KaZaa。&lt;/p&gt;

&lt;p&gt;KaZaa是现在全世界流行的几款p2p软件之一。根据CA公司统计，全球KaZaa的下载量超过2.5亿次。使用KaZaa软件进行文件传输消耗了互联网40%的带宽。之所以它如此的成功，是因为它结合了Napster和Gnutella共同的优点。从结构 上来说，它使用了Gnutella的全分布式的结构，这样可以是系统更好的扩展，因为它无需中央索引服务器存储文件名，它是自动的把性能好的机器成为SuperNode，它存储着离它最近的叶子节点的文件信息，这些SuperNode,再连通起来形成一个Overlay Network. 由于SuperNode的索引功能，使搜索效率大大提高。&lt;/p&gt;

&lt;p&gt;半分布式结构（含有SuperNode）&lt;/p&gt;

&lt;p&gt;kaZaa界面&lt;/p&gt;

&lt;p&gt;半分布式结构的优点是性能、可扩展性较好，较容易管理，但对超级点依赖性大，易于受到攻击，容错性也受到影响。下表比较了4种结构的综合性能，比较结果如表1-1所示。&lt;/p&gt;

&lt;p&gt;表1：4种结构的性能比较&lt;/p&gt;

&lt;p&gt;　&lt;/p&gt;

&lt;p&gt;比较标准／拓扑结构&lt;/p&gt;

&lt;p&gt;中心化拓扑&lt;/p&gt;

&lt;p&gt;全分布式非结构化拓扑&lt;/p&gt;

&lt;p&gt;全分布式结构化拓扑&lt;/p&gt;

&lt;p&gt;半分布式拓扑&lt;/p&gt;

&lt;p&gt;可扩展性&lt;/p&gt;

&lt;p&gt;差&lt;/p&gt;

&lt;p&gt;差&lt;/p&gt;

&lt;p&gt;好&lt;/p&gt;

&lt;p&gt;中&lt;/p&gt;

&lt;p&gt;可靠性&lt;/p&gt;

&lt;p&gt;差&lt;/p&gt;

&lt;p&gt;好&lt;/p&gt;

&lt;p&gt;好&lt;/p&gt;

&lt;p&gt;中&lt;/p&gt;

&lt;p&gt;可维护性&lt;/p&gt;

&lt;p&gt;最好&lt;/p&gt;

&lt;p&gt;最好&lt;/p&gt;

&lt;p&gt;好&lt;/p&gt;

&lt;p&gt;中&lt;/p&gt;

&lt;p&gt;发现算法效率&lt;/p&gt;

&lt;p&gt;最高&lt;/p&gt;

&lt;p&gt;中&lt;/p&gt;

&lt;p&gt;高&lt;/p&gt;

&lt;p&gt;中&lt;/p&gt;

&lt;p&gt;复杂查询&lt;/p&gt;

&lt;p&gt;支持&lt;/p&gt;

&lt;p&gt;支持&lt;/p&gt;

&lt;p&gt;不支持&lt;/p&gt;

&lt;p&gt;支持&lt;/p&gt;

&lt;p&gt;1.2.2 国内的P2P研究现状&lt;/p&gt;

&lt;p&gt;学术机构研发&lt;/p&gt;

&lt;p&gt;北京大学—Maze
　　　Maze 是北京大学网络实验室开发的一个中心控制与对等连接相融合的对等计算文件共享系统，在结构上类似Napster，对等计算搜索方法类似于Gnutella。网络上的一台计算机，不论是在内网还是外网，可以通过安装运行Maze的客户端软件自由加入和退出Maze系统。每个节点可以将自己的一个或多个目录下的文件共享给系统的其他成员，也可以分享其他成员的资源。Maze支持基于关键字的资源检索，也可以通过好友关系直接获得。&lt;/p&gt;

&lt;p&gt;清华大学—Granary
　　　Granary是清华大学自主开发的对等计算存储服务系统。它以对象格式存储数据。另外，Granary设计了专门的结点信息收集算法PeerWindow的结构化覆盖网络路由协议Tourist。&lt;/p&gt;

&lt;p&gt;华中科技大学—AnySee
　　　AnySee是华中科大设计研发的视频直播系统。它采用了一对多的服务模式，支持部分NAT和防火墙的穿越，提高了视频直播系统的可扩展性；同时，它利用近播原则、分域调度的思想，使用Landmark路标算法直接建树的方式构建应用层上的组播树，克服了ESM等一对多模式系统由联接图的构造和维护带来的负载影响。&lt;/p&gt;

&lt;p&gt;　　更详细介绍见［中国计算机学会通讯　Page 38-51　郑纬民等　对等计算研究概论］&lt;/p&gt;

&lt;p&gt;企业研发产品&lt;/p&gt;

&lt;p&gt;广州数联软件技术有限公司-Poco
    POCO 是中国最大的 P2P用户分享平台 , 是有安全、流量控制力的，无中心服务器的第三代 Peer to Peer资源交换平台 , 也是世界范围内少有的盈利的 Peer to Peer平台。目前已经形成了 2600 万海量用户，平均在线 58.5 万，在线峰值突破 71 万，并且全部是宽带用户的用户群。 成为中国地区第一的 Peer to Peer分享平台。[a]&lt;/p&gt;

&lt;p&gt;深圳市点石软件有限公司-OP
     OP-又称为Openext Media Desktop，一个网络娱乐内容平台，Napster的后继者，它可以最直接的方式找到您想要的音乐、影视、软件、游戏、图片、书籍以及各种文档，随时在线共享文件容量数以亿计“十万影视、百万音乐、千万图片”。OP整合了Internet Explorer、Windows Media Player、RealOne Player和ACDSee ，是国内的网络娱乐内容平台。[a]&lt;/p&gt;

&lt;p&gt;基于P2P的在线电视直播-PPLive
    PPLive是一款用于互联网上大规模视频直播的共享软件。它使用网状模型，有效解决了当前网络视频点播服务的带宽和负载有限问题，实现用户越多，播放越流畅的特性，整 体服务质量大大提高！（2005年的超级女声决赛期间，这款软件非常的火爆，同时通过它看湖南卫视的有上万观众）&lt;/p&gt;

&lt;p&gt;其他商业软件这里不一一列举，请访问P2P门户网站 http://www.ppcn.net/&lt;/p&gt;

&lt;p&gt;1.2.3 P2P技术的应用研究&lt;/p&gt;

&lt;p&gt;　　国外开展P2P研究的学术团体主要包括P2P工作组(P2PWG) 、全球网格论坛(Global Grid Forum ，GGF) 。P2P工作组成立的主要目的是希望加速P2P计算基础设施的建立和相应的标准化工作。P2PWG成立之后，对P2P计算中的术语进行了统一，也形成相关的草案，但是在标准化工作方面工作进展缓慢。目前P2PWG已经和GGF合并，由该论坛管理P2P计算相关的工作。GGF负责网格计算和P2P计算等相关的标准化工作。&lt;/p&gt;

&lt;p&gt;从国外公司对P2P计算的支持力度来看，Microsoft公司、Sun公司和Intel公司投入较大。Microsoft公司成立了Pastry项目组，主要负责P2P计算技术的研究和开发工作。目前Microsoft公司已经发布了基于Pastry的软件包SimPastry/ VisPastry。Rice大学也在Pastry的基础之上发布了FreePastry软件包。&lt;/p&gt;

&lt;p&gt;在2000年8月，Intel公司宣布成立P2P工作组，正式开展P2P的研究。工作组成立以后，积极与应用开发商合作，开发P2P应用平台。2002年Intel发布了. Net基础架构之上的Accelerator Kit (P2P加速工具包) 和P2P安全API软件包，从而使得微软. NET开发人员能够迅速地建立P2P安全Web应用程序。&lt;/p&gt;

&lt;p&gt;Sun公司以Java技术为背景，开展了JXTA项目。JXTA是基于Java的开源P2P平台，任何个人和组织均可以加入该项目。因此，该项目不仅吸引了大批P2P研究人员和开发人员，而且已经发布了基于JXTA的即时聊天软件包。JXTA定义了一组核心业务：认证、资源发现和管理。在安全方面，JXTA加入了加密软件包，允许使用该加密包进行数据加密，从而保证消息的隐私、可认证性和完整性。在JXTA核心之上，还定义了包括内容管理、信息搜索以及服务管理在内的各种其它可选JXTA服务。在核心服务和可选服务基础上，用户可以开发各种JXTA平台上的P2P应用。&lt;/p&gt;

&lt;p&gt;P2P实际的应用主要体现在以下几个方面：&lt;/p&gt;

&lt;p&gt;P2P分布式存储&lt;/p&gt;

&lt;p&gt;P2P分布式存储系统是一个用于对等网络的数据存储系统，它可以提供高效率的、鲁棒的和负载平衡的文件存取功能。这些研究包括：OceanStore，Farsite等。其中，基于超级点结构的半分布式P2P应用如Kazza、Edonkey、Morpheus、Bittorrent等也是属于分布式存储的范畴，并且用户数量急剧增加。&lt;/p&gt;

&lt;p&gt;计算能力的共享&lt;/p&gt;

&lt;p&gt;加入对等网络的结点除了可以共享存储能力之外，还可以共享CPU处理能力。目前已经有了一些基于对等网络的计算能力共享系统。比如SETI@home。目前SETI@home采用的仍然是类似于Napster的集中式目录策略。Xenoservers向真正的对等应用又迈进了一步。这种计算能力共享系统可以用于进行基因数据库检索和密码破解等需要大规模计算能力的应用。&lt;/p&gt;

&lt;p&gt;P2P应用层组播。&lt;/p&gt;

&lt;p&gt;应用层组播，就是在应用层实现组播功能而不需要网络层的支持。这样就可以避免出现由于网络层迟迟不能部署对组播的支持而使组播应用难以进行的情况。应用层组播需要在参加的应用结点之间实现一个可扩展的，支持容错能力的重叠网络，而基于DHT的发现机制正好为应用层组播的实现提供了良好的基础平台。&lt;/p&gt;

&lt;p&gt;Internet间接访问基础结构（Internet Indirection Infrastructure）。&lt;/p&gt;

&lt;p&gt;为了使Internet更好地支持组播、单播和移动等特性，Internet间接访问基础结构提出了基于汇聚点的通信抽象。在这一结构中，并不把分组直接发向目的结点，而是给每个分组分配一个标识符，而目的结点则根据标识符接收相应的分组。标识符实际上表示的是信息的汇聚点。目的结点把自己想接收的分组的标识符预先通过一个触发器告诉汇聚点，当汇聚点收到分组时，将会根据触发器把分组转发该相应的目的结点。Internet间接访问基础结构实际上在Internet上构成了一个重叠网络，它需要对等网络的路由系统对它提供相应的支持。&lt;/p&gt;

&lt;p&gt;P2P技术从出现到各个领域的应用展开，仅用了几年的时间。从而证明了P2P技术具有非常广阔的应用前景。&lt;/p&gt;

&lt;p&gt;　&lt;/p&gt;

&lt;p&gt;1.3　对研究内容有重大影响的几个方面&lt;/p&gt;

&lt;p&gt;　　随着P2P应用的蓬勃发展，作为P2P应用中核心问题的发现技术除了遵循技术本身的逻辑以外，也受到某些技术的发展趋势、需求趋势的深刻影响。　&lt;/p&gt;

&lt;p&gt;1.3.1　度数和直径的折衷关系（tradeoff）对发现算法的影响&lt;/p&gt;

&lt;p&gt;　　如上所述，DHT发现技术完全建立在确定性拓扑结构的基础上，从而表现出对网络中路由的指导性和网络中结点与数据管理的较强控制力。但是，对确定性结构的认识又限制了发现算法效率的提升。研究分析了目前基于DHT的发现算法，发现衡量发现算法的两个重要参数度数（表示邻居关系数、路由表的容量）和链路长度（发现算法的平均路径长度）之间存在渐进曲线的关系。&lt;/p&gt;

&lt;p&gt;　　研究者采用图论中度数(Degree)和直径(Diameter)两个参数研究DHT发现算法，发现这些DHT发现算法在度数和直径之间存在渐进曲线关系，如下图所示。在N个结点网络中，图中直观显示出当度数为N时，发现算法的直径为O(1)；当每个结点仅维护一个邻居时，发现算法的直径为O(N)。这是度数和直径关系的2种极端情况。同时，研究以图论的理论分析了O(d)的度和O(d)的直径的算法是不可能的。&lt;/p&gt;

&lt;p&gt;度数和直径之间的渐进曲线关系&lt;/p&gt;

&lt;p&gt;从渐进曲线关系可以看出，如果想获得更短的路径长度，必然导致度数的增加；而网络实际连接状态的变化造成大度数邻居关系的维护复杂程度增加。另外，研究者证明O(logN)甚至O(logN/loglogN)的平均路径长度也不能满足状态变化剧烈的网络应用的需求。新的发现算法受到这种折衷关系制约的根本原因在于DHT对网络拓扑结构的确定性认识。&lt;/p&gt;

&lt;p&gt;1.3.2 Small world 理论对P2P发现技术的影响&lt;/p&gt;

&lt;p&gt;非结构化P2P系统中发现技术一直采用洪泛转发的方式，与DHT的启发式发现算法相比，可靠性差，对网络资源的消耗较大。最新的研究从提高发现算法的可靠性和寻找随机图中的最短路径两个方面展开。也就是对重叠网络的重新认识。其中，small world特征和幂规律证明实际网络的拓扑结构既不是非结构化系统所认识的一个完全随机图，也不是DHT发现算法采用的确定性拓扑结构。&lt;/p&gt;

&lt;p&gt;实际网络体现的幂规律分布的含义可以简单解释为在网络中有少数结点有较高的“度”，多数结点的“度”较低。度较高的结点同其他结点的联系比较多，通过它找到待查信息的概率较高。&lt;/p&gt;

&lt;p&gt;Small-world[a][b]模型的特性：网络拓扑具有高聚集度和短链的特性。在符合small world特性的网络模型中，可以根据结点的聚集度将结点划分为若干簇(Cluster)，在每个簇中至少存在一个度最高的结点为中心结点。大量研究证明了以Gnutella为代表的P2P网络符合small world特征，也就是网络中存在大量高连通结点，部分结点之间存在“短链”现象。&lt;/p&gt;

&lt;p&gt;因此，P2P发现算法中如何缩短路径长度的问题变成了如何找到这些“短链”的问题。尤其是在DHT发现算法中，如何产生和找到“短链”是发现算法设计的一个新的思路。small world特征的引入会对P2P发现算法产生重大影响。&lt;/p&gt;

&lt;p&gt;1.3.3 语义查询和DHT的矛盾&lt;/p&gt;

&lt;p&gt;现有DHT算法由于采用分布式散列函数，所以只适合于准确的查找，如果要支持目前Web上搜索引擎具有的多关键字查找的功能，还要引入新的方法。主要的原因在于DHT的工作方式。&lt;/p&gt;

&lt;p&gt;基于DHT的P2P系统采用相容散列函数根据精确关键词进行对象的定位与发现。散列函数总是试图保证生成的散列值均匀随机分布，结果两个内容相似度很高但不完全相同的对象被生成了完全不同的散列值，存放到了完全随机的两个结点上。因此，DHT可以提供精确匹配查询，但是支持语义是非常困难的。&lt;/p&gt;

&lt;p&gt;目前在DHT基础上开展带有语义的资源管理技术的研究还非常少。由于DHT的精确关键词映射的特性决定了无法和信息检索等领域的研究成果结合，阻碍了基于DHT的P2P系统的大规模应用。[a]&lt;/p&gt;

&lt;p&gt;1.4 P2P发现技术研究的成果与不足&lt;/p&gt;

&lt;p&gt;　　P2P发现技术中最重要的研究成果应该是基于small world理论的非结构化发现算法和基于DHT的结构化发现算法。尤其是DHT及其发现技术为资源的组织与查找提供了一种新的方法。&lt;/p&gt;

&lt;p&gt;随着P2P系统实际应用的发展，物理网络中影响路由的一些因素开始影响P2P发现算法的效率。一方面，实际网络中结点之间体现出较大的差异，即异质性。由于客户机/服务器模式在Internet和分布式领域十几年的应用和大量种类的电子设备的普及，如手提电脑、移动电话或PDA。这些设备在计算能力、存储空间和电池容量上差别很大。另外，实际网络被路由器和交换机分割成不同的自治区域，体现出严密的层次性。&lt;/p&gt;

&lt;p&gt;另一方面，网络波动的程度严重影响发现算法的效率。网络波动（Churn、fluctuation of network）包括结点的加入、退出、失败、迁移、并发加入过程、网络分割等。DHT的发现算法如Chord、CAN、Koorde等都是考虑网络波动的最差情况下的设计与实现。由于每个结点的度数尽量保持最小，这样需要响应的成员关系变化的维护可以比较小，从而可以快速恢复网络波动造成的影响。但是每个结点仅有少量路由状态的代价是发现算法的高延时，因为每一次查找需要联系多个结点，在稳定的网络中这种思路是不必要的。&lt;/p&gt;

&lt;p&gt;同时，作为一种资源组织与发现技术必然要支持复杂的查询，如关键词、内容查询等。尽管信息检索和数据挖掘领域提供了大量成熟的语义查询技术，由于DHT精确关键词映射的特性阻碍了DHT在复杂查询方面的应用。&lt;/p&gt;

&lt;p&gt;2复杂P2P网络拓扑模型&lt;/p&gt;

&lt;p&gt;2.1 Internet拓扑模型&lt;/p&gt;

&lt;p&gt;Internet作为当今人类社会信息化的标志,其规模正以指数速度高速增长.如今Internet的“面貌”已与其原型ARPANET大相径庭,依其高度的复杂性,可以将其看作一个由计算机构成的“生态系统”.虽然Internet是人类亲手建造的,但却没有人能说出这个庞然大物看上去到底是个什么样子,运作得如何.Internet拓扑建模研究就是探求在这个看似混乱的网络之中蕴含着哪些还不为我们所知的规律.发现Internet拓扑的内在机制是认识Internet的必然过程,是在更高层次上开发利用Internet的基础.然而,Internet与生俱来的异构性动态性发展的非集中性以及如今庞大的规模都给拓扑建模带来巨大挑战.Internet拓扑建模至今仍然是一个开放性问题,在计算机网络研究中占有重要地位.&lt;/p&gt;

&lt;p&gt;Internet拓扑作为Internet这个自组织系统的“骨骼”,与流量协议共同构成模拟Internet的3个组成部分,即在拓扑网络中节点间执行协议,形成流量.Internet拓扑模型是建立Internet系统模型的基础,由此而体现的拓扑建模意义也可以说就是Internet建模的意义,即作为一种工具,人们用其来对Internet进行分析预报决策或控制.Internet模型中的拓扑部分刻画的是Internet在宏观上的特征,反映一种总体趋势,所以其应用也都是在大尺度上展开的.对Internet拓扑模型的需求主要来自以下几个方面:(1) 许多新应用或实验不适合直接应用于Internet,其中一些具有危害性,如蠕虫病毒在大规模网络上的传播模拟;(2) 对于一些依赖于网络拓扑的协议(如多播协议),在其研发阶段,当前Internet拓扑只能提供一份测试样本,无法对协议进行全面评估,需要提供多个模拟拓扑环境来进行实验;(3) 从国家安全角度考虑,需要在线控制网络行为,如美国国防高级研究计划局(DARPA)的NMS(network modeling and simulation)项目.&lt;/p&gt;

&lt;p&gt;2.1.1随机网络与拓扑模型&lt;/p&gt;

&lt;p&gt;随机网络是由N个顶点构成的图中，可以存在 条边，我们从中随机连接M条边所构成的网络。还有一种生成随机网络的方法是，给一个概率p，对于 中任何一个可能连接，我们都尝试一遍以概率p的连接。如果我们选择M = p ，这两种随机网络模型就可以联系起来。对于如此简单的随机网络模型，其几何性质的研究却不是同样的简单。随机网络几何性质的研究是由Paul，Alfréd Rényi和Béla Bollobás在五十年代到六十年代之间完成的。随机网络在Internet的拓扑中占有很重要的位置。&lt;/p&gt;

&lt;p&gt;2.1.2随机网络参数&lt;/p&gt;

&lt;p&gt;描述随机网络有一些重要的参数。一个节点所拥有的度是该节点与其他节点相关联的边数，度是描述网络局部特性的基本参数。网络中并不是所有节点都具有相同的度，系统中节点度的分布情况，可以用分布函数 描述，度分布函数反映了网络系统的宏观统计特征。理论上利用度分布可以计算出其他表征全局特性参数的量化数值。&lt;/p&gt;

&lt;p&gt;聚集系数是描述与第三个节点连接的一对节点被连接的概率。从连接节点的边的意义上，若 为第i个节点的度，在由k.个近邻节点构成的子网中，实际存在的边数E(i)与全部k.个节点完全连接时的总边数充 的比值定义为节点i的聚集系数&lt;/p&gt;

&lt;p&gt;　&lt;/p&gt;

&lt;p&gt;2.1.3拓扑模型&lt;/p&gt;

&lt;p&gt;Internet拓扑模型可分为两类:一类是描述Internet拓扑特征,包括Waxman模型,Tiers,Transit -Stub和幂律;另一类是描述拓扑特征形成的机理,包括Barabási与Albert提出的BA和ESF以及一种改进模型GLP.由于后一类模型对Internet幂律形成机理的描述还不成熟,更多的是作为一种产生幂律的图生成算法。对于第1类模型来说,Internet拓扑特征的发现,实际上就是刻画该特征的度量(metric)的发现.一个属于第一类的拓扑模型就是包含若干已存在的或新发现的度量,然后根据实际的Internet拓扑数据求出这些度量的值.因此,对这类模型进行评价需要从两方面入手:一方面,对它所采用的拓扑数据进行评价;另一方面,对其度量进行评价.在所有已经发现的Internet拓扑度量中,最为基本的就是节点出度频率fd.其分布是判断一幅拓扑图是否与Internet拓扑相类似的最重要的依据.在研究早期,研究人员或者认为Internet节点出度是完全随机的(如Waxman模型),或者认为节点出度是正规的(如Tiers模型).而幂律的发现证明了Internet拓扑结构介于两者之间,呈幂率分布.根据对出度频率分布的刻画,可将Internet拓扑模型分为以下3类:&lt;/p&gt;

&lt;p&gt;(1) 随机型,即认为Internet拓扑图处于一个完全无序的状态,在大尺度上是均一的.Waxman模型，是一种类似于Erds-Rényi模型的随机模型,出度频率呈泊松分布.这个模型有两个版本:RG1,先将节点随机布置在直角坐标网格中,节点间的距离就是其欧几里德距离;RG2,依据(0,L)均匀随机分布为节点对指定距离.两个版本中,节点间相接的概率P(u,v)与其距离相关,服从泊松分布,距离越近,概率越大.&lt;/p&gt;

&lt;p&gt;其中,d(u,v)表示节点u与v之间的距离,L为节点间最长距离,á与a取值范围是(0,1).&lt;/p&gt;

&lt;p&gt;(2) 层次型,来自对Internet结构所具有的层次特征的认识,在同一层上的节点出度接近,不同层间节点出度差别很大.对同一层上的节点布置借用Waxman模型方法.Tiers(等级)模型将Internet划分为LAN(局域网),MAN(城域网)和WAN(广域网)3个层次.在该模型中,WAN只有一个,通过指定LAN和MAN数量以及各自内部所包含节点的数量来构造拓扑图.Transit-Stub模型将AS域划分为Transit类和Stub类.在该模型中,Transit节点彼此互联构成一个节点群,一个或多个Transit节点群构成拓扑图的核心,而Stub节点分布在Transit节点群四周与Transit节点相连.Transit -Stub是GT-ITM(georgia tech Internetwork topology models)软件包的一部分,有时GT-ITM也就是指Transit-Stub模型.&lt;/p&gt;

&lt;p&gt;(3) 幂率型.1999年,Faloutsos等人对NLANR(National Lab for Applied Network Research)在1997~1998年间的3份BGP数据以及1995年的一份traceroute测量数据进行分析,发现Internet拓扑中存在着4条幂律.&lt;/p&gt;

&lt;p&gt;幂律是指形如 的方程,对于两个变量X和Y,存在一个常数C,使得Y与X的C次幂成比例.有两个声明:(1) 节点v的等级为 ,v是在按出度降序排列序列中的索引值;(2) 邻接矩阵特征值按降序排列,第i个特征值为li.幂律1(等级指数R):节点出度dv与该节点等级rv的R次幂成比例.幂律2(出度指数O):出度频率fd与该出度d的O次幂成比例.&lt;/p&gt;

&lt;p&gt;近似幂律(hop-plot指数H):h跳内节点对(pairs of nodes)的数量与h的H次幂成比例.幂律3(特征指数E):特征值li与其次序i的E次幂成比例.&lt;/p&gt;

&lt;p&gt;　&lt;/p&gt;

&lt;p&gt;2.2 Small World网络&lt;/p&gt;

&lt;p&gt;在现实的Internet环境中，网络拓扑并不完全满足随机网络拓扑。Watt和Strogatz发现，只需要在规则网络上稍作随机改动就可以同时具备以上两个性质。改动的方法是，对于规则网络的每一个顶点的所有边，以概率p断开一个端点，并重新连接，连接的新的端点从网络中的其他顶点里随机选择，如果所选的顶点已经与此顶点相连，则再随机选择别的顶点来重连。当p = 0时就是规则网络，p = 1则为随机网络，对于0 &amp;lt; p &amp;lt; 1的情况，存在一个很大的p的区域，同时拥有较大的集聚程度和较小的最小距离。Small World网络的几何性质如图所示。&lt;/p&gt;

&lt;p&gt;p值不同的small world网络&lt;/p&gt;

&lt;p&gt;(1)平均路径。图中被随机选择又重新连结后的线称为捷径，它对整个网络的平均路径有着很大影响，分析表明:当p&amp;gt;=21(NK)，即在保证系统中至少出现一条捷径的情况下，系统的平均路径开始下降。即使是相当少的捷径也能够显著地减小网络的平均路径长度。这是因为每出现一条捷径，它对整个系统的影响是非线性的，它不仅影响到被这条线直接连着的两点，也影响到了这两点的最近邻、次近邻，以及次次近邻等。&lt;/p&gt;

&lt;p&gt;(2) WS网络的集团系数。r1初始固定的节点数可计算t1{ p=0时规则网络的聚集系数为C(0), C(0)取决于网络结构而与尺寸N无关，因此有相对较大的值。随着边按一定的概率p随机化，聚集系数在CYO}的附近变化。&lt;/p&gt;

&lt;p&gt;(3)度分布。WS模型是介于规则网络和随机网络之间的模型,p-O时规则网络的度分布是中心点位于K=k的8函数，P=1时随机网络Poisson分布，在K=k点达到极大值。P从。变化到1的过程中，原来S函数形式的度分布逐渐拓宽最终形成Poisson分布。在Small World网络的研究兴起之后，越来越多的科学家投入到复杂网络的研究中去。大家发现其实更多的其他几何量的特征也具有很大程度上的普适性和特定的结构功能关系。Scale Free网络就是其中的一个重要方面。Scale Free网络指的是网络的度分布符合幂率分布，由于其缺乏一个描述问题的特征尺度而被称为无标度网络。我们都知道在统计物理学相变与临界现象，以及在自组织临界性（SOC）中幂率具有特殊地位。Scale-free 其实也具有small world 的特性。&lt;/p&gt;

&lt;p&gt;　&lt;/p&gt;

&lt;p&gt;3　非结构化P2P搜索算法&lt;/p&gt;

&lt;p&gt;　　按照搜索策略，可以分为两大类：盲目搜索和信息搜索。盲目搜索通过在网络中传播查询信息并且把这些信息不断扩散给每个节点。通过这种洪泛方式来搜索想要的资源。而信息搜索在搜索的过程中利用一些已有的信息来辅助查找过程。由于信息搜索对资源的存储有一些知识，所以信息搜索能够比较快的找到资源。&lt;/p&gt;

&lt;p&gt;3.1 Blind search&lt;/p&gt;

&lt;p&gt;　　在最初的Gnutella协议中，使用的Flooding方法，就是一种典型的盲目搜索。在网络中，每个节点都不知道其他节点的资源。当它要寻找某个文件，把这个查询信息传递给它的相邻节点，如果相邻节点含有这个资源，就返回一个QueryHit的信息给Requester。如果它相邻的节点都没有命中这个被查询文件，就把这条消息转发给自己的相邻节点。这种方式像洪水在网络中各个节点流动一样，所以叫做Flooding搜索。由于这种搜索策略是首先遍历自己的邻接点，然后再向下传播，所以又称为宽度优先搜索方法（BFS）。&lt;/p&gt;

&lt;p&gt;BFS搜索把消息传播给所有的邻接点，它消耗了大量的网络带宽，使消息堵塞严重，效率比较低，扩展性不好。一些人在BFS的基础上进行改进，它的方法是随机抽取一定比例的相邻节点传递消息，而不是像Flooding一样把查询信息传播给所有邻接点。这种修正的极大地减少了网络中的查询信息，但是在命中率上不如BFS。&lt;/p&gt;

&lt;p&gt;Iterative Deepening：这种搜索策略是在初始阶段，给TTL一个很小的值，如果在TTL减为0，还没有搜索到资源，则给TTL重新赋更高的值。这种策略可以减少搜索的半径，但是在最坏的情况下，延迟很大。&lt;/p&gt;

&lt;p&gt;Random Walk: 在随机漫步中，请求者发出K个查询请求给随机挑选的K个相邻节点。然后每个查询信息在以后的漫步过程中直接与请求者保持联系，询问是否还要继续下一步。如果请求者同意继续漫步，则又开始随机选择下一步漫步的节点，否则中止搜索。&lt;/p&gt;

&lt;p&gt;Gnutella2: 建立super-node,它存储着离它最近的叶子节点的文件信息，这些SuperNode,再连通起来形成一个Overlay Network.当叶子节点需要查询文件，它首先从它连接的SuperNode的索引中寻找，如果找到了文件，则直接根据文件所存储的机器的IP地址建立连接，如果没有找到，则SuperNode把这个查询请求发给它连接的其他超级节点，直到得到想要的资源。&lt;/p&gt;

&lt;p&gt;3.2 Informed Search Methods&lt;/p&gt;

&lt;p&gt;3.2.1 Cache Method&lt;/p&gt;

&lt;p&gt;这种方法不同于盲目搜索很大的地方在与它在每个节点上，不管是中央节点还是简单节点都存有路径信息。这就是Cache的思路。新的搜索并不需要直接达到资源的存储地，只要在搜索的路径中找到以前搜索的记录也就是在它以前的搜索的基础上找到源IP地址，就可以把请求消息返回。这样可以极大的减少搜索的消息，提高效率。&lt;/p&gt;

&lt;p&gt;3.2.2 Mobile Agent based Method&lt;/p&gt;

&lt;p&gt;移动Agent是一个能在异构网络中自主地从一台主机迁移到另一台主机，并可与其他Agent或资源进行交互的程序。Agent非常适合在网络环境中来帮助用户完成信息检索的任务。现在意大利的一些研究人员在Mobile Agent 结合P2P方面做了一些前沿的研究，其中的一些想法，就是通过在P2P软件中嵌入Agent的运行时环境。当有节点需要搜索的时候，它发送一个移动Agent 给它相邻的节点，移动Agent记录着它的一些搜索的信息。当这个Agent到达一台新的机器上，然后在这个机器上进行资源搜索任务，如果这台机器上没有它想要的资源，则它把这些搜索的信息传给它的邻节点，如果找到资源，则返回给请求的机器。&lt;/p&gt;

&lt;p&gt;　　关于搜索方法综述：参照[ Chonggang Wang Bo Li： Peer-to-Peer Overlay Networks: A Survey]&lt;/p&gt;

&lt;p&gt;[Hybrid Search Schemes for Unstructured Peer-to-Peer Networks]&lt;/p&gt;

&lt;p&gt;4　P2P带来的信息安全问题&lt;/p&gt;

&lt;p&gt;4.1 知识产权保护&lt;/p&gt;

&lt;p&gt;在P2P共享网络中普遍存在着知识产权保护问题。尽管目前Gnutella、Kazaa等P2P共享软件宣传其骨干服务器上并没有存储任何涉及产权保护的内容的备份，而仅仅是保存了各个内容在互联网上的存储索引。但无疑的是，P2P共享软件的繁荣加速了盗版媒体的分发，提高了知识产权保护的难点。美国唱片工业协会RIAA(Recording Industry Association of America)与这些共享软件公司展开了漫长的官司拉锯战，著名的Napster便是这场战争的第一个牺牲者。另一个涉及面很关的战场则是RIAA和使用P2P来交换正版音乐的平民。从2004年1月至今RIAA已提交了1000份有关方面的诉讼。尽管如此，至今每个月仍然有超过150,000,000的歌曲在网络上被自由下载。后Napster时代的P2P共享软件较Napster更具有分散性，也更难加以控制。即使P2P共享软件的运营公司被判违法而关闭，整个网络仍然会存活，至少会正常工作一段时间。
　　另一方面，Napster以后的P2P共享软件也在迫切寻找一个和媒体发布厂商的共生互利之道。如何更加合法合理的应用这些共享软件，是一个新时代的课题。毕竟P2P除了共享盗版软件，还可以共享相当多的有益的信息。
　　网络社会与自然社会一样，其自身具有一种自发地在无序和有序之间寻找平衡的趋势。P2P技术为网络信息共享带来了革命性的改进，而这种改进如果想要持续长期地为广大用户带来好处，必须以不损害内容提供商的基本利益为前提。这就要求在不影响现有P2P共享软件性能的前提下，一定程度上实现知识产权保护机制。目前，已经有些P2P厂商和其它公司一起在研究这样的问题。这也许将是下一代P2P共享软件面临的挑战性技术问题之一。&lt;/p&gt;

&lt;p&gt;4.2 网络病毒传播&lt;/p&gt;

&lt;p&gt;随着计算机网络应用的深入发展，计算机病毒对信息安全的威胁日益增加。特别是在P2P环境下，方便的共享和快速的选路机制，为某些网络病毒提供了更好的入侵机会。
　　由于P2P网络中逻辑相邻的节点，地理位置可能相隔很远，而参与P2P网络的节点数量 又非常大，因此通过P2P系统传播的病毒，波及范围大，覆盖面广，从而造成的损失会很大。
　　在P2P网络中，每个节点防御病毒的能力是不同的。只要有一个节点感染病毒，就可以通过内部共享和通信机制将病毒扩散到附近的邻居节点。在短时间内可以造成网络拥塞甚至瘫痪，共享信息丢失，机密信息失窃，甚至通过网络病毒可以完全控制整个网络。
　　一个突出的例子就是2003年通过即时通讯（Instant Message）软件传播病毒的案例显著增多。包括Symantec公司和McAfee公司的高层技术主管都预测即时通讯软件将会成为网络病毒传播和黑客攻击的主要载体之一。
　　随着P2P技术的发展，将来会出现各种专门针对P2P系统的网络病毒。利用系统漏洞，达到迅速破坏、瓦解、控制系统的目的。因此，网络病毒的潜在危机对P2P系统安全性和健壮性提出了更高的要求，迫切需要建立一套完整、高效、安全的防毒体系。&lt;/p&gt;

&lt;p&gt;其它信息安全问题还包括反动影片、色情影片的在P2P泛滥，对国家、青少年造成的负面影响。&lt;/p&gt;

&lt;p&gt;详细可参照[程学旗等  信息技术快报 2004年第三期 P2P技术与信息安全]&lt;/p&gt;

&lt;p&gt;5 结尾&lt;/p&gt;

&lt;p&gt;5.1 P2P一些资源&lt;/p&gt;

&lt;p&gt;http://www.ppcn.net/  中国的P2P门户网站&lt;/p&gt;

&lt;p&gt;http://iptps05.cs.cornell.edu/IPTPS_cfp.htm  P2P领域著名的国际会议&lt;/p&gt;

&lt;p&gt;http://www.jxta.org/  Java Peer to Peer技术网站&lt;/p&gt;

&lt;p&gt;http://www.hpl.hp.com/techreports/2002/HPL-2002-57R1.pdf   非常经典的P2P综述文章&lt;/p&gt;

&lt;p&gt;http://david.weekly.org/code/napster.php3 第一个Peer to Peer软件-Napster 协议&lt;/p&gt;

&lt;p&gt;http://www.globus.org/alliance/publications/papers/kazaa.pdf 非常著名的kaZaa软件解析&lt;/p&gt;

&lt;p&gt;http://p2p.weblogsinc.com/ 比较著名的p2p weblog&lt;/p&gt;

&lt;p&gt;http://www.google.com   最后，当然是好帮手 Google 了 ：）&lt;/p&gt;

&lt;p&gt;5.2 致谢和建议&lt;/p&gt;

&lt;p&gt;这篇文章是我在中科院计算所的一篇技术报告的部分内容（修改）。最先是在2005-4-25号挂在自己的个人主页上。目的是让更多的朋友通过阅读对P2P技术有一个大概的了解。为了让文章变得通俗易懂，我删除了其中非常细节的部分，这次又对第一版进行了修改。承蒙厚爱，在这期间，我收到了很多朋友，包括学校的同学、公司的技术人员给我写的email，表达对文章的肯定，希望一起探讨p2p的问题,这里表示感谢。&lt;/p&gt;

&lt;p&gt;因为P2P技术包括很多方面，以一篇文章之力肯定不能面面俱到，所以要想对P2P进行深入的研究，个人觉得，一定多看Paper，多看程序。记得刚写这篇文章时候，包括从Google, Citeseer,IEEE , ACM，Wanfang期刊网等搜索了624篇paper（英文610篇，中文14篇），然后对其进行了分类，包括各种综述、拓扑网络、搜索算法、安全等等。这绝对是一个必由之路。另外，由于大多数P2P软件都是开源的，所以通过看源码可以对其进行更深的了解。&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/04/p2p.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/04/p2p.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>nat</title>
        <description>&lt;ol&gt;
  &lt;li&gt;IPv4协议和NAT的由来&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;今天，无数快乐的互联网用户在尽情享受Internet带来的乐趣。他们浏览新闻，搜索资料，下载软件，广交新朋，分享信息，甚至于足不出户获取一切日用所需。企业利用互联网发布信息，传递资料和订单，提供技术支持，完成日常办公。然而，Internet在给亿万用户带来便利的同时，自身却面临一个致命的问题：构建这个无所不能的Internet的基础IPv4协议已经不能再提供新的网络地址了。&lt;/p&gt;

&lt;p&gt;2011年2月3日中国农历新年， IANA对外宣布：IPv4地址空间最后5个地址块已经被分配给下属的5个地区委员会。2011年4月15日，亚太区委员会APNIC对外宣布，除了个别保留地址外，本区域所有的IPv4地址基本耗尽。一时之间，IPv4地址作为一种濒危资源身价陡增，各大网络公司出巨资收购剩余的空闲地址。其实，IPv4地址不足问题已不是新问题，早在20年以前，IPv4地址即将耗尽的问题就已经摆在Internet先驱们面前。这不禁让我们想去了解，是什么技术使这一危机延缓了尽20年。&lt;/p&gt;

&lt;p&gt;要找到问题的答案，让我们先来简略回顾一下IPv4协议。&lt;/p&gt;

&lt;p&gt;IPv4即网际网协议第4版——Internet Protocol Version 4的缩写。IPv4定义一个跨越异种网络互连的超级网，它为每个网际网的节点分配全球唯一IP地址。如果我们把Internet比作一个邮政系统，那么IP地址的作用就等同于包含城市、街区、门牌编号在内的完整地址。IPv4使用32bits整数表达一个地址，地址最大范围就是232 约为43亿。以IP创始时期可被联网的设备来看，这样的一个空间已经很大，很难被短时间用完。然而，事实远远超出人们的设想，计算机网络在此后的几十年里迅速壮大，网络终端数量呈爆炸性增长。&lt;/p&gt;

&lt;p&gt;更为糟糕的是，为了路由和管理方便，43亿的地址空间被按照不同前缀长度划分为A,B,C,D类地址网络和保留地址。其中，A类网络地址127段，每段包括主机地址约1678万个。B类网络地址16384段，每段包括65536个主机地址。&lt;/p&gt;

&lt;p&gt;P2P技术详解(一)：NAT详解——详细原理、P2P简介_图1 IPv4网络地址划分 
图1 IPv4网络地址划分&lt;/p&gt;

&lt;p&gt;IANA向超大型企业/组织分配A类网络地址，一次一段。向中型企业或教育机构分配B类网络地址，一次一段。这样一种分配策略使得IP地址浪费很严重，很多被分配出去的地址没有真实被利用，地址消耗很快。以至于二十世纪90年代初，网络专家们意识到，这样大手大脚下去，IPv4地址很快就要耗光了。于是，人们开始考虑IPv4的替代方案，同时采取一系列的措施来减缓IPv4地址的消耗。正是在这样一个背景之下，本期的主角闪亮登场，它就是网络地址转换——NAT。&lt;/p&gt;

&lt;p&gt;NAT是一项神奇的技术，说它神奇在于它的出现几乎使IPv4起死回生。在IPv4已经被认为行将结束历史使命之后近20年时间里，人们几乎忘了IPv4的地址空间即将耗尽这样一个事实——在新技术日新月异的时代，20年可算一段漫长的历史。更不用说，在NAT产生以后，网络终端的数量呈加速上升趋势，对IP地址的需求剧烈增加。此足见NAT技术之成功，影响之深远。&lt;/p&gt;

&lt;p&gt;说它神奇，更因为NAT给IP网络模型带来了深远影响，其身影遍布网络每个角落。根据一份最近的研究报告，70%的P2P用户位于NAT网关以内。因为P2P主要运行在终端用户的个人电脑之上，这个数字意味着大多数PC通过NAT网关连接到Internet。如果加上2G和3G方式联网的智能手机等移动终端，在NAT网关之后的用户远远超过这个比例。&lt;/p&gt;

&lt;p&gt;然而当我们求本溯源时却发现一个很奇怪的事实：NAT这一意义重大的技术，竟然没有公认的发明者。NAT第一个版本的RFC作者，只是整理归纳了已被广泛采用的技术。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;NAT的工作模型和特点&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2.1 NAT的概念模型&lt;/p&gt;

&lt;p&gt;NAT名字很准确，网络地址转换，就是替换IP报文头部的地址信息。NAT通常部署在一个组织的网络出口位置，通过将内部网络IP地址替换为出口的IP地址提供公网可达性和上层协议的连接能力。那么，什么是内部网络IP地址？&lt;/p&gt;

&lt;p&gt;RFC1918规定了三个保留地址段落：10.0.0.0-10.255.255.255；172.16.0.0-172.31.255.255；192.168.0.0-192.168.255.255。这三个范围分别处于A,B,C类的地址段，不向特定的用户分配，被IANA作为私有地址保留。这些地址可以在任何组织或企业内部使用，和其他Internet地址的区别就是，仅能在内部使用，不能作为全球路由地址。这就是说，出了组织的管理范围这些地址就不再有意义，无论是作为源地址，还是目的地址。对于一个封闭的组织，如果其网络不连接到Internet，就可以使用这些地址而不用向IANA提出申请，而在内部的路由管理和报文传递方式与其他网络没有差异。&lt;/p&gt;

&lt;p&gt;对于有Internet访问需求而内部又使用私有地址的网络，就要在组织的出口位置部署NAT网关，在报文离开私网进入Internet时，将源IP替换为公网地址，通常是出口设备的接口地址。一个对外的访问请求在到达目标以后，表现为由本组织出口设备发起，因此被请求的服务端可将响应由Internet发回出口网关。出口网关再将目的地址替换为私网的源主机地址，发回内部。这样一次由私网主机向公网服务端的请求和响应就在通信两端均无感知的情况下完成了。依据这种模型，数量庞大的内网主机就不再需要公有IP地址了。&lt;/p&gt;

&lt;p&gt;P2P技术详解(一)：NAT详解——详细原理、P2P简介_图2 NAT转换过程示意图 
图2 NAT转换过程示意图&lt;/p&gt;

&lt;p&gt;虽然实际过程远比这个复杂，但上面的描述概括了NAT处理报文的几个关键特点：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;网络被分为私网和公网两个部分，NAT网关设置在私网到公网的路由出口位置，双向流量必须都要经过NAT网关；&lt;/li&gt;
  &lt;li&gt;网络访问只能先由私网侧发起，公网无法主动访问私网主机；&lt;/li&gt;
  &lt;li&gt;NAT网关在两个访问方向上完成两次地址的转换或翻译，出方向做源信息替换，入方向做目的信息替换；&lt;/li&gt;
  &lt;li&gt;NAT网关的存在对通信双方是保持透明的；&lt;/li&gt;
  &lt;li&gt;NAT网关为了实现双向翻译的功能，需要维护一张关联表，把会话的信息保存下来。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;随着后面对NAT的深入描述，读者会发现，这些特点是鲜明的，但又不是绝对的。其中第二个特点打破了IP协议架构中所有节点在通讯中的对等地位，这是NAT最大的弊端，为对等通讯带来了诸多问题，当然相应的克服手段也应运而生。事实上，第四点是NAT致力于达到的目标，但在很多情况下，NAT并没有做到，因为除了IP首部，上层通信协议经常在内部携带IP地址信息。这些我们稍后解释。
2.2 一对一的NAT&lt;/p&gt;

&lt;p&gt;如果一个内部主机唯一占用一个公网IP，这种方式被称为一对一模型。此种方式下，转换上层协议就是不必要的，因为一个公网IP就能唯一对应一个内部主机。显然，这种方式对节约公网IP没有太大意义，主要是为了实现一些特殊的组网需求。比如用户希望隐藏内部主机的真实IP，或者实现两个IP地址重叠网络的通信。
2.3 一对多的NAT&lt;/p&gt;

&lt;p&gt;NAT最典型的应用场景就如同图2描述的，一个组织网络，在出口位置部署NAT网关，所有对公网的访问表现为一台主机。这就是所谓的一对多模型。这种方式下，出口设备只占用一个由Internet服务提供商分配的公网IP地址。面对私网内部数量庞大的主机，如果NAT只进行IP地址的简单替换，就会产生一个问题：当有多个内部主机去访问同一个服务器时，从返回的信息不足以区分响应应该转发到哪个内部主机。此时，需要NAT设备根据传输层信息或其他上层协议去区分不同的会话，并且可能要对上层协议的标识进行转换，比如TCP或UDP端口号。这样NAT网关就可以将不同的内部连接访问映射到同一公网IP的不同传输层端口，通过这种方式实现公网IP的复用和解复用。这种方式也被称为端口转换PAT、NAPT或IP伪装，但更多时候直接被称为NAT，因为它是最典型的一种应用模式。
2.4 按照NAT端口映射方式分类&lt;/p&gt;

&lt;p&gt;在一对多模型中，按照端口转换的工作方式不同，又可以进行更进一步的划分。为描述方便，以下将IP和端口标记为(nAddr:nPort)，其中n代表主机或NAT网关的不同角色。&lt;/p&gt;

&lt;p&gt;P2P技术详解(一)：NAT详解——详细原理、P2P简介_ 图3 按照端口转换映射方式分类 
图3 按照端口转换映射方式分类&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;全锥形NAT 其特点为：一旦内部主机端口对(iAddr:iPort)被NAT网关映射到(eAddr:ePort)，所有后续的(iAddr:iPort)报文都会被转换为(eAddr:ePort)；任何一个外部主机发送到(eAddr:ePort)的报文将会被转换后发到(iAddr:iPort)。

限制锥形NAT 其特点为：一旦内部主机端口对(iAddr:iPort)被映射到(eAddr:ePort)，所有后续的(iAddr:iPort)报文都会被转换为(eAddr:ePort)；只有 (iAddr:iPort)向特定的外部主机hAddr发送过数据，主机hAddr从任意端口发送到(eAddr:ePort)的报文将会被转发到(iAddr:iPort)。

端口限制锥形NAT 其特点为：一旦内部主机端口对(iAddr:iPort)被映射到(eAddr:ePort)，所有后续的(iAddr:iPort)报文都会被转换为(eAddr:ePort)；只有(iAddr:iPort)向特定的外部主机端口对(hAddr:hPort)发送过数据，由 (hAddr:hPort)发送到(eAddr:ePort)的报文将会被转发到(iAddr:iPort)。

对称型NAT 其特点为：NAT网关会把内部主机“地址端口对”和外部主机“地址端口对”完全相同的报文看作一个连接，在网关上创建一个公网“地址端口对”映射进行转换，只有收到报文的外部主机从对应的端口对发送回应的报文，才能被转换。即使内部主机使用之前用过的地址端口对去连接不同外部主机(或端口)时，NAT网关也会建立新的映射关系。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;事实上，这些术语的引入是很多混淆的起源。现实中的很多NAT设备是将这些转换方式混合在一起工作的，而不单单使用一种，所以这些术语只适合描述一种工作方式，而不是一个设备。比如，很多NAT设备对内部发出的连接使用对称型NAT方式，而同时支持静态的端口映射，后者可以被看作是全锥型NAT方式。而有些情况下，NAT设备的一个公网地址和端口可以同时映射到内部几个服务器上以实现负载分担，比如一个对外提供WEB服务器的站点可能是有成百上千个服务器在提供HTTP服务，但是对外却表现为一个或少数几个IP地址。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;NAT的限制与解决方案&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3.1 IP端到端服务模型&lt;/p&gt;

&lt;p&gt;IP协议的一个重要贡献是把世界变得平等。在理论上，具有IP地址的每个站点在协议层面有相当的获取服务和提供服务的能力，不同的IP地址之间没有差异。人们熟知的服务器和客户机实际是在应用协议层上的角色区分，而在网络层和传输层没有差异。一个具有IP地址的主机既可以是客户机，也可以是服务器，大部分情况下，既是客户机，也是服务器。端到端对等看起来是很平常的事情，而意义并不寻常。但在以往的技术中，很多协议体系下的网络限定了终端的能力。正是IP的这个开放性，使得TCP/IP协议族可以提供丰富的功能，为应用实现提供了广阔平台。因为所有的IP主机都可以服务器的形式出现，所以通讯设计可以更加灵活。使用UNIX/LINUX的系统充分利用了这个特性，使得任何一个主机都可以建立自己的HTTP、SMTP、POP3、DNS、DHCP等服务。与此同时，很多应用也是把客户端和服务器的角色组合起来完成功能。例如在VoIP应用中，用户端向注册服务器登录自己的IP地址和端口信息过程中，主机是客户端；而在呼叫到达时，呼叫处理服务器向用户端发送呼叫请求时，用户端实际工作在服务器模式下。在语音媒体流信道建立过程后，通讯双向发送语音数据，发送端是客户模式，接收端是服务器模式。而在P2P的应用中，一个用户的主机既为下载的客户，同时也向其他客户提供数据，是一种C/S混合的模型。上层应用之所以能这样设计，是因为IP协议栈定义了这样的能力。试想一下，如果IP提供的能力不对等，那么每个通信会话都只能是单方向发起的，这会极大限制通信的能力。细心的读者会发现，前面介绍NAT的一个特性正是这样一种限制。没错，NAT最大的弊端正在于此——破坏了IP端到端通信的能力。
3.2 NAT的弊端&lt;/p&gt;

&lt;p&gt;NAT在解决IPv4地址短缺问题上，并非没有副作用，其实存在很多问题。&lt;/p&gt;

&lt;p&gt;首先，NAT使IP会话的保持时效变短。因为一个会话建立后会在NAT设备上建立一个关联表，在会话静默的这段时间，NAT网关会进行老化操作。这是任何一个NAT网关必须做的事情，因为IP和端口资源有限，通信的需求无限，所以必须在会话结束后回收资源。通常TCP会话通过协商的方式主动关闭连接，NAT网关可以跟踪这些报文，但总是存在例外的情况，要依赖自己的定时器去回收资源。而基于UDP的通信协议很难确定何时通信结束，所以NAT网关主要依赖超时机制回收外部端口。通过定时器老化回收会带来一个问题，如果应用需要维持连接的时间大于NAT网关的设置，通信就会意外中断。因为网关回收相关转换表资源以后，新的数据到达时就找不到相关的转换信息，必须建立新的连接。当这个新数据是由公网侧向私网侧发送时，就会发生无法触发新连接建立，也不能通知到私网侧的主机去重建连接的情况。这时候通信就会中断，不能自动恢复。即使新数据是从私网侧发向公网侧，因为重建的会话表往往使用不同于之前的公网IP和端口地址，公网侧主机也无法对应到之前的通信上，导致用户可感知的连接中断。NAT网关要把回收空闲连接的时间设置到不发生持续的资源流失，又维持大部分连接不被意外中断，是一件比较有难度的事情。在NAT已经普及化的时代，很多应用协议的设计者已经考虑到了这种情况，所以一般会设置一个连接保活的机制，即在一段时间没有数据需要发送时，主动发送一个NAT能感知到而又没有实际数据的保活消息，这么做的主要目的就是重置NAT的会话定时器。&lt;/p&gt;

&lt;p&gt;其次，NAT在实现上将多个内部主机发出的连接复用到一个IP上，这就使依赖IP进行主机跟踪的机制都失效了。如网络管理中需要的基于网络流量分析的应用无法跟踪到终端用户与流量的具体行为的关系。基于用户行为的日志分析也变得困难，因为一个IP被很多用户共享，如果存在恶意的用户行为，很难定位到发起连接的那个主机。即便有一些机制提供了在NAT网关上进行连接跟踪的方法，但是把这种变换关系接续起来也困难重重。基于IP的用户授权不再可靠，因为拥有一个IP的不等于一个用户或主机。一个服务器也不能简单把同一IP的访问视作同一主机发起的，不能进行关联。有些服务器设置有连接限制，同一时刻只接纳来自一个IP的有限访问(有时是仅一个访问)，这会造成不同用户之间的服务抢占和排队。有时服务器端这样做是出于DOS攻击防护的考虑，因为一个用户正常情况下不应该建立大量的连接请求，过度使用服务资源被理解为攻击行为。但是这在NAT存在时不能简单按照连接数判断。总之，因为NAT隐蔽了通信的一端，把简单的事情复杂化了。&lt;/p&gt;

&lt;p&gt;我们来深入理解NAT一下对IP端到端模型的破坏力。NAT通过修改IP首部的信息变换通信的地址。但是在这个转换过程中只能基于一个会话单位。当一个应用需要保持多个双向连接时，麻烦就很大。NAT不能理解多个会话之间的关联性，无法保证转换符合应用需要的规则。当NAT网关拥有多个公有IP地址时，一组关联会话可能被分配到不同的公网地址，这通常是服务器端无法接受的。更为严重的是，当公网侧的主机要主动向私网侧发送数据时，NAT网关没有转换这个连接需要的关联表，这个数据包无法到达私网侧的主机。这些反方向发送数据的连接总有应用协议的约定或在初始建立的会话中进行过协商。但是因为NAT工作在网络层和传输层，无法理解应用层协议的行为，对这些信息是无知的。NAT希望自己对通信双方是透明的，但是在这些情况下这是一种奢望。&lt;/p&gt;

&lt;p&gt;P2P技术详解(一)：NAT详解——详细原理、P2P简介_图4 NAT对端到端通信模型的破坏 
图4 NAT对端到端通信模型的破坏&lt;/p&gt;

&lt;p&gt;此外，NAT工作机制依赖于修改IP包头的信息，这会妨碍一些安全协议的工作。因为NAT篡改了IP地址、传输层端口号和校验和，这会导致认证协议彻底不能工作，因为认证目的就是要保证这些信息在传输过程中没有变化。对于一些隧道协议，NAT的存在也导致了额外的问题，因为隧道协议通常用外层地址标识隧道实体，穿过NAT的隧道会有IP复用关系，在另一端需要小心处理。ICMP是一种网络控制协议，它的工作原理也是在两个主机之间传递差错和控制消息，因为IP的对应关系被重新映射，ICMP也要进行复用和解复用处理，很多情况下因为ICMP报文载荷无法提供足够的信息，解复用会失败。IP分片机制是在信息源端或网络路径上，需要发送的IP报文尺寸大于路径实际能承载最大尺寸时，IP协议层会将一个报文分成多个片断发送，然后在接收端重组这些片断恢复原始报文。IP这样的分片机制会导致传输层的信息只包括在第一个分片中，NAT难以识别后续分片与关联表的对应关系，因此需要特殊处理。
3.3 NAT穿越技术&lt;/p&gt;

&lt;p&gt;前面解释了NAT的弊端，为了解决IP端到端应用在NAT环境下遇到的问题，网络协议的设计者们创造了各种武器来进行应对。但遗憾的是，这里每一种方法都不完美，还需要在内部主机、应用程序或者NAT网关上增加额外的处理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;应用层网关 应用层网关(ALG)是解决NAT对应用层协议无感知的一个最常用方法，已经被NAT设备厂商广泛采用，成为NAT设备的一个必需功能。因为NAT不感知应用协议，所以有必要额外为每个应用协议定制协议分析功能，这样NAT网关就能理解并支持特定的协议。ALG与NAT形成互动关系，在一个NAT网关检测到新的连接请求时，需要判断是否为已知的应用类型，这通常是基于连接的传输层端口信息来识别的。在识别为已知应用时，再调用相应功能对报文的深层内容进行检查，当发现任何形式表达的IP地址和端口时，将会把这些信息同步转换，并且为这个新连接创建一个附加的转换表项。这样，当报文到达公网侧的目的主机时，应用层协议中携带的信息就是NAT网关提供的地址和端口。一旦公网侧主机开始发送数据或建立连接到此端口，NAT网关就可以根据关联表信息进行转换，再把数据转发到私网侧的主机。很多应用层协议实现不限于一个初始连接(通常为信令或控制通道)加一个数据连接，可能是一个初始连接对应很多后续的新连接。比较特别的协议，在一次协商中会产生一组相关连接，比如RTP/RTCP协议规定，一个RTP通道建立后占用连续的两个端口，一个服务于数据，另一个服务于控制消息。此时，就需要ALG分配连续的端口为应用服务。ALG能成功解决大部分协议的NAT穿越需求，但是这个方法也有很大的限制。因为应用协议的数量非常多而且在不断发展变化之中，添加到设备中的ALG功能都是为特定协议的特定规范版本而开发的，协议的创新和演进要求NAT设备制造商必须跟踪这些协议的最近标准，同时兼容旧标准。尽管有如Linux这种开放平台允许动态加载新的ALG特性，但是管理成本仍然很高，网络维护人员也不能随时了解用户都需要什么应用。因此为每个应用协议开发ALG代码并跟踪最新标准是不可行的，ALG只能解决用户最常用的需求。此外，出于安全性需要，有些应用类型报文从源端发出就已经加密，这种报文在网络中间无法进行分析，所以ALG无能为力。

探针技术STUN和TURN 所谓探针技术，是通过在所有参与通信的实体上安装探测插件，以检测网络中是否存在NAT网关，并对不同NAT模型实施不同穿越方法的一种技术。STUN服务器被部署在公网上，用于接收来自通信实体的探测请求，服务器会记录收到请求的报文地址和端口，并填写到回送的响应报文中。客户端根据接收到的响应消息中记录的地址和端口与本地选择的地址和端口进行比较，就能识别出是否存在NAT网关。如果存在NAT网关，客户端会使用之前的地址和端口向服务器的另外一个IP发起请求，重复前面的探测。然后再比较两次响应返回的结果判断出NAT工作的模式。由前述的一对多转换模型得知，除对称型NAT以外的模型，NAT网关对内部主机地址端口的映射都是相对固定的，所以比较容易实现NAT穿越。而对称型NAT为每个连接提供一个映射，使得转换后的公网地址和端口对不可预测。此时TURN可以与STUN绑定提供穿越NAT的服务，即在公网服务器上提供一个“地址端口对”，所有此“地址端口对”接收到的数据会经由探测建立的连接转发到内网主机上。TURN分配的这个映射“地址端口对”会通过STUN响应发给内部主机，后者将此信息放入建立连接的信令中通知通信的对端。这种探针技术是一种通用方法，不用在NAT设备上为每种应用协议开发功能，相对于ALG方式有一定普遍性。但是TURN中继服务会成为通信瓶颈。而且在客户端中增加探针功能要求每个应用都要增加代码才能支持。

中间件技术 这也是一种通过开发通用方法解决NAT穿越问题的努力。与前者不同之处是，NAT网关是这一解决方案的参与者。与ALG的不同在于，客户端会参与网关公网映射信息的维护，此时NAT网关只要理解客户端的请求并按照要求去分配转换表，不需要自己去分析客户端的应用层数据。其中UPnP就是这样一种方法。UPnP中文全称为通用即插即用，是一个通用的网络终端与网关的通信协议，具备信息发布和管理控制的能力。其中，网关映射请求可以为客户动态添加映射表项。此时，NAT不再需要理解应用层携带的信息，只转换IP地址和端口信息。而客户端通过控制消息或信令发到公网侧的信息中，直接携带公网映射的IP地址和端口，接收端可以按照此信息建立数据连接。NAT网关在收到数据或连接请求时，按照UPnP建立的表项只转换地址和端口信息，不关心内容，再将数据转发到内网。这种方案需要网关、内部主机和应用程序都支持UPnP技术，且组网允许内部主机和NAT网关之间可以直接交换UPnP信令才能实施。

中继代理技术 准确说它不是NAT穿越技术，而是NAT旁路技术。简单说，就是在NAT网关所在的位置旁边放置一个应用服务器，这个服务器在内部网络和外部公网分别有自己的网络连接。客户端特定的应用产生网络请求时，将定向发送到应用代理服务器。应用代理服务器根据代理协议解析客户端的请求，再从服务器的公网侧发起一个新的请求，把客户端请求的内容中继到外部网络上，返回的相应反方向中继。这项技术和ALG有很大的相似性，它要求为每个应用类型部署中继代理业务，中间服务器要理解这些请求。

特定协议的自穿越技术 在所有方法中最复杂也最可靠的就是自己解决自己的问题。比如IKE和IPsec技术，在设计时就考虑了到如何穿越NAT的问题。因为这个协议是一个自加密的协议并且具有报文防修改的鉴别能力，其他通用方法爱莫能助。因为实际应用的NAT网关基本都是NAPT方式，所有通过传输层协议承载的报文可以顺利通过NAT。IKE和IPsec采用的方案就是用UDP在报文外面再加一层封装，而内部的报文就不再受到影响。IKE中还专门增加了NAT网关是否存在的检查能力以及绕开NAT网关检测IKE协议的方法。 4. NAT的应用和实现
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.1 NAT的应用&lt;/p&gt;

&lt;p&gt;NAT在当代Internet中被广泛采用，小至家庭网关，大到企业广域网出口甚至运营商业务网络出口。其实NAT在用户身边随处可见，一般家庭宽带接入的ADSL Modem和SOHO路由器都内置了NAT功能，WindowsXP支持网络连接共享，一个用户连接到公网可能会经过多层NAT而对此一无所知。很多企业也为节约IP费用采用NAT接入Internet，但是相比家庭用户有更复杂的需求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAT多实例应用 在VPN网络中，多实例路由意味着一个物理拓扑上承载多个逻辑拓扑，网络终端被分配到相互隔离的逻辑拓扑中，彼此之间没有路由的通路。但在访问Internet或者一些关键服务器资源时，被隔离的网络之间又存在共享资源的需求。NAT的多实例实现就是跨越这种逻辑拓扑的方法，把一个空间的网络地址映射到另一个空间。

NAT的高可靠性组网 提高网络可靠性是一个广泛的需求，NAT作为私网到公网的关键路径自然也需要高可靠性。当一个设备提供多个公网接口时，在多接口上部署NAT可以提供更高带宽和多ISP就近访问的能力。但是，当部署多个出口时，访问的流量可能会从不匹配的接口返回，这就要求NAT方案有良好的路由规划和部署合适的策略保证这种流量能够正确处理。在多个物理设备承担NAT功能时，不同设备之间的信息备份和流量分担也是一个组网难题。

同时转换源和目的地址的应用 前面我们介绍的所有NAT应用中，由内网向外网访问过程中，都是将源地址进行转换而目的地址保持不变，报文反方向进入时则处理目的地址。但有一些特殊应用需要在由内向外的IP通路上，替换目的IP地址。通常，这种应用会同时替换源地址和目的地址，在经过NAT网关以后完成两次地址转换。当两个均规划使用私属IP地址范围的网络进行合并时，终端用户都不想调整自己的IP地址方案，又希望开放一些网络资源给彼此访问。这时就可以通过NAT的两次地址转换来解决路由和地址规划无法解决的问题。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;P2P技术详解(一)：NAT详解——详细原理、P2P简介_图5 同时转换源和目的地址的应用 
图5 同时转换源和目的地址的应用
4.2 NAT的设备实现&lt;/p&gt;

&lt;p&gt;NAT作为一个IP层业务特性，在产品实现中与防火墙、会话管理等特性有紧密联系，这是因为NAT判断一个进入设备的报文是否需要NAT处理，判断报文是否为一个新的连接，都需要通过匹配访问控制列表规则和查询会话关联表进行判断。为了满足不同应用场景的NAT需求， NAT的管理界面可提供用户多种配置策略。按照NAT的具体工作方式，又可以做如下分类。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;静态一对一地址映射 这种工作方式下，NAT把一个私网地址和一个公网地址做静态关联，在从内而外的方向，将源IP匹配的私网IP替换为公网IP，反方向则将目的IP匹配公网IP的报文替换为私网IP。网络层以上的部分不进行替换处理，只修正校验和。

静态多对多地址映射 这种方式与上一种类似，只是把一段私网地址映射到一段公网地址。工作机制与前述的方式没有差别，只是简化配置工作量。

动态端口映射 这是最基本的工作方式，即前面多次介绍的将一段内网地址动态翻译为一个或多个公网IP，同时对传输层端口或其他上层协议信息进行转换，以实现IP复用。对由内而外的报文，替换源地址和端口，反向报文替换目的地址和端口。仅以连接公网的接口IP作为NAT转换的公网地址时，这种配置最简化，又被称为EasyIP。当以一段公网IP地址作为NAT转换地址时，需要配置一个地址池，NAT会自动在地址池中选择使用公网IP。

动态地址映射(no-pat) 这是介于静态多对多地址映射和动态端口映射方式之间的一种工作机制。当有一个私网向公网侧访问到达NAT网关时，NAT网关会检查这个私网IP是否已经有关联的公网IP映射。如果已经存在，则按照转换表直接替换IP，不修改上层协议。如果不存在关联表项，则在空闲的公网IP池中占用一个IP，并写入关联表中，以后按照这个关联关系进行地址转换。当这个私网主机发起的所有对外访问均关闭或超时后，回收公网IP。这种方式可以理解为一组内网主机抢占式地共享一个公网IP地址池。当公网IP地址池用完以后，新连接将无法建立。

静态端口映射 通过静态配置，把一个固定的私网IP地址和端口关联到一个公网地址和端口上。这种方式等同于前面介绍过的全锥模式，但是不需要内网主机首先发出报文。这种方式适用于在NAT网关上把一个知名服务（如HTTP）映射到一个内部主机上，也称为port forwarding。

应用层网关(ALG) 在所有NAT产品实现中，ALG是一个必需的功能组件。但在不同实现中，有些产品可以动态加载不同的ALG模块，有些产品可以提供ALG开关控制，有些则不提供任何用户接口。ALG解析上层应用协议的内容，并且根据需要修改IP和端口相关信息，创建和维护附加的关联表项。

NAT转换关联表 无论哪一种NAT工作方式，都要用到地址转换关联表，在不同产品的实现中，这个关联表的存储结构和在IP转发中调用的方式有很大不同。关联表中会记录源IP、目的IP、连接协议类型、传输层源端口、目的端口，以及转换后的源IP、源端口，目的IP、目的端口信息，这里的源和目的都是对应于从内网到外网的访问方向。依据NAT具体工作方式，这些信息可能全部填充，也可能部分填充。例如只按照IP做静态映射的方式，就不需要填入任何端口相关信息；对于静态端口映射，则只填入源相关的内容，而目的端的信息为空。 5. 后IPv4时代的NAT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NAT是为延缓IPv4地址耗尽而推出的技术。毫无疑问，它已经出色完成了自己的历史使命，IPv4比预期走得更远。作为继任者的IPv6吸取了IPv4的教训，被赋予充足地址空间的同时在各个方面做了优化——安全、高效、简洁。但是IPv6无法平滑地取代IPv4，导致IP升级步伐缓慢。尽管网络协议的分层设计很清晰，大量应用层协议和互联网软件中仍内嵌了IPv4地址的处理，要Internet全网升级到IPv6，必须先完成应用的改造。因为NAT和它的穿越技术结合能够满足大部分用户的需求，所以IPv6时代被不断推迟。&lt;/p&gt;

&lt;p&gt;随着IPv4地址的濒临耗尽，再经济的模式也无以为继，IPv4必须退出历史舞台。人们自然会认为，NAT作为IPv4的超级补丁技术使命已经完结。实际情况是，IPv4向IPv6过渡的阶段，NAT仍然是一项必不可少的技术手段。因为Internet无法在一日之内完成全网升级，必然是局部升级，逐渐替换。在两套协议并存的时期，用户和服务资源分布在不同网络之间，跨网访问的需求必须得到满足。这正是NAT所擅长的领域，地址替换，因此NAT-PT应运而生。由于IPv4和IPv6之间的差异，NAT要做的事比以往更复杂，有更多的限制和细节。&lt;/p&gt;

&lt;p&gt;此外，IETF也在制定纯IPv6网络使用的NAT规范。虽然人们还看不到这种应用的强烈需求，但是NAT仍有其独特的作用，比如隐藏内部网络的地址，实现重叠地址网络的合并等。&lt;/p&gt;

&lt;p&gt;毫不夸张地说，正是有了NAT，以IPv4为基础的Internet才能容纳数十亿的用户终端，成就今日之辉煌。IPv4已至日暮西山，IPv6的黎明尚未来临，Internet比任何时刻都更依赖NAT这项过渡技术。NAT的历史再次证明，翻天覆地的划时代进步不一定有市场，抱残守缺的修修补补未必不会成功。在世代更替之时让我们走近NAT，领略IP领域更多细微但不高深的知识，理解NAT就是理解变换万千的应用世界。
&lt;!-- more --&gt;
1、内容概述&lt;/p&gt;

&lt;p&gt;P2P即点对点通信，或称为对等联网，与传统的服务器客户端模式（如下图“P2P结构模型”所示）有着明显的区别，在即时通讯方案中应用广泛（比如IM应用中的实时音视频通信、实时文件传输甚至文字聊天等）。&lt;/p&gt;

&lt;p&gt;P2P可以是一种通信模式、一种逻辑网络模型、一种技术、甚至一种理念。在P2P网络中（如右图所示），所有通信节点的地位都是对等的，每个节点都扮演着客户机和服务器双重角色，节点之间通过直接通信实现文件信息、处理器运算能力、存储空间等资源的共享。P2P网络具有分散性、可扩展性、健壮性等特点，这使得P2P技术在信息共享、即时通讯、协同工作、分布式计算、网络存储等领域都有广阔的应用。&lt;/p&gt;

&lt;p&gt;图1 - 经典的CS模式：                                                      &lt;br /&gt;
 P2P技术详解(二)：P2P中的NAT穿越(打洞)方案详解_2.jpg&lt;/p&gt;

&lt;p&gt;图2 - P2P结构模型：
 P2P技术详解(二)：P2P中的NAT穿越(打洞)方案详解_1.jpg&lt;/p&gt;

&lt;p&gt;NAT技术和P2P技术作为经典的两项网络技术，在现在的网络上有着广泛的应用，P2P主机位于NAT网关后面的情况屡见不鲜。NAT技术虽然在一定程度上解决了IPv4地址短缺的问题，在构建防火墙、保证网络安全方面都发挥了一定的作用，却破坏了端到端的网络通信。NAT阻碍主机进行P2P通信的主要原因是NAT不允许外网主机主动访问内网主机，但是P2P技术却要求通信双方都能主动发起访问，所以要在NAT网络环境中进行有效的P2P通信，就必须采用新的解决方案。&lt;/p&gt;

&lt;p&gt;P2P作为一项实用的技术，有很大的优化空间，并且相对于网络设备，基于P2P的应用程序在实现上更为灵活。所以为了兼容NAT，基于P2P的应用程序在开发的时候大多会根据自身特点加入一些穿越NAT的功能以解决上述问题。以下着重介绍几种常见的P2P穿越NAT方案。
2、反向链接技术：一种特殊的P2P场景（通信双方中只有一方位于NAT设备之后）&lt;/p&gt;

&lt;p&gt;此种情况是所有P2P场景中最简单的，它使用一种被称为“反向链接技术”来解决这个问题。大致的原理如下所述。&lt;/p&gt;

&lt;p&gt;如图3所示，客户端A位于NAT之后，它通过TCP端口1234连接到服务器的TCP端口1235上，NAT设备为这个连接重新分配了TCP端口62000。客户端B也通过TCP端口1234连接到服务器端口1235上。A和B从服务器处获知的对方的外网地址二元组{IP地址:端口号}分别为{138.76.29.7:1234}和{155.99.25.11:62000}，它们在各自的本地端口上进行侦听。&lt;/p&gt;

&lt;p&gt;由于B 拥有外网IP地址，所以A要发起与B的通信，可以直接通过TCP连接到B。但如果B尝试通过TCP连接到A进行P2P通信，则会失败，原因是A位于NAT设备后，虽然B发出的TCP SYN请求能够到达NAT设备的端口62000，但NAT设备会拒绝这个连接请求。要想与Client A通信， B不是直接向A发起连接，而是通过服务器给A转发一个连接请求，反过来请求A连接到B（即进行反向链接），A在收到从服务器转发过来的请求以后，会主动向B发起一个TCP的连接请求，这样在NAT设备上就会建立起关于这个连接的相关表项，使A和B之间能够正常通信，从而建立起它们之间的TCP连接。&lt;/p&gt;

&lt;p&gt;图3 - 反向链接示意图：
 P2P技术详解(二)：P2P中的NAT穿越(打洞)方案详解_3.jpg 
3、基于UDP协议的P2P打洞技术详解&lt;/p&gt;

&lt;p&gt;1原理概述&lt;/p&gt;

&lt;p&gt;UDP打洞技术是通过中间服务器的协助在各自的NAT网关上建立相关的表项，使P2P连接的双方发送的报文能够直接穿透对方的NAT网关，从而实现P2P客户端互连。如果两台位于NAT设备后面的P2P客户端希望在自己的NAT网关上打个洞，那么他们需要一个协助者——集中服务器，并且还需要一种用于打洞的Session建立机制。&lt;/p&gt;

&lt;p&gt;什么是集中服务器？
集中服务器本质上是一台被设置在公网上的服务器，建立P2P的双方都可以直接访问到这台服务器。位于NAT网关后面的客户端A和B都可以与一台已知的集中服务器建立连接，并通过这台集中服务器了解对方的信息并中转各自的信息。&lt;/p&gt;

&lt;p&gt;同时集中服务器的另一个重要作用在于判断某个客户端是否在NAT网关之后。具体的方法是：一个客户端在集中服务器上登陆的时候，服务器记录下该客户端的两对地址二元组信息{IP地址:UDP端口}，一对是该客户端与集中服务器进行通信的自身的IP地址和端口号，另一对是集中服务器记录下的由服务器“观察”到的该客户端实际与自己通信所使用的IP地址和端口号。我们可以把前一对地址二元组看作是客户端的内网IP地址和端口号，把后一对地址二元组看作是客户端的内网IP地址和端口号经过NAT转换后的外网IP地址和端口号。集中服务器可以从客户端的登陆消息中得到该客户端的内网相关信息，还可以通过登陆消息的IP头和UDP头得到该客户端的外网相关信息。如果该客户端不是位于NAT设备后面，那么采用上述方法得到的两对地址二元组信息是完全相同的。&lt;/p&gt;

&lt;p&gt;P2P的Session建立原理：
假定客户端A要发起对客户端B的直接连接，具体的“打洞”过程如下：
1）A最初不知道如何向客户端B发起连接，于是A向集中服务器发送消息，请求集中服务器帮助建立与客户端B的UDP连接。
2）集中服务器将含有B的外网和内网的地址二元组发给A，同时，集中服务器将包含有A的外网和内网的地址二元组信息的消息也发给B。这样一来， A与B就都知道对方外网和内网的地址二元组信息了。
3）当A收到由集中服务器发来的包含B的外网和内网的地址二元组信息后， A开始向B的地址二元组发送UDP数据包，并且A会自动锁定第一个给出响应的B的地址二元组。同理，当B收到由集中服务器发来的A的外网和内网地址二元组信息后，也会开始向A的外网和内网的地址二元组发送UDP数据包，并且自动锁定第一个得到A回应的地址二元组。由于A与B互相向对方发送UDP数据包的操作是异步的，所以A和B发送数据包的时间先后并没有时序要求。&lt;/p&gt;

&lt;p&gt;下面来看下这三者之间是如何进行UDP打洞的。在这我们分三种具体情景来讨论：
第一种是最简单的一种情景，两个客户端都位于同一个NAT设备后面，即位于同一内网中；
第二种是最普遍的一种情景，两个客户端分别位于不同的NAT设备后面，分属不同的内网；
第三种是客户端位于两层NAT设备之后，通常最上层的NAT是由网络提供商提供的，第二层NAT是家用的NAT路由器之类的设备提供的。&lt;/p&gt;

&lt;p&gt;2典型P2P情景1：  两客户端位于同一NAT设备后面（即相同内网中）&lt;/p&gt;

&lt;p&gt;这是最简单的一种情况（如图4所示）：客户端A和B分别与集中服务器建立UDP连接，经过NAT转换后，A的公网端口被映射为62000，B的公网端口映射为62005。&lt;/p&gt;

&lt;p&gt;图4 - 位于同一个NAT设备后的UDP打洞过程：
 P2P技术详解(二)：P2P中的NAT穿越(打洞)方案详解_4.png&lt;/p&gt;

&lt;p&gt;当A向集中服务器发出消息请求与B进行连接，集中服务器将B的外网地址二元组以及内网地址二元组发给A，同时把A的外网以及内网的地址二元组信息发给B。A和B发往对方公网地址二元组信息的UDP数据包不一定会被对方收到，这取决于当前的NAT设备是否支持不同端口之间的UDP数据包能否到达（即Hairpin转换特性），无论如何A与B发往对方内网的地址二元组信息的UDP数据包是一定可以到达的，内网数据包不需要路由，且速度更快。A与B推荐采用内网的地址二元组信息进行常规的P2P通信。&lt;/p&gt;

&lt;p&gt;假定NAT设备支持Hairpin转换，P2P双方也应忽略与内网地址二元组的连接，如果A 和B采用外网的地址二元组做为P2P通信的连接，这势必会造成数据包无谓地经过NAT设备，这是一种对资源的浪费。就目前的网络情况而言，应用程序在“打洞”的时候，最好还是把外网和内网的地址二元组都尝试一下。如果都能成功，优先以内网地址进行连接。&lt;/p&gt;

&lt;p&gt;什么是Hairpin技术？
Hairpin技术又被称为Hairpin NAT、Loopback NAT或Hairpin Translation。Hairpin技术需要NAT网关支持，它能够让两台位于同一台NAT网关后面的主机，通过对方的公网地址和端口相互访问，NAT网关会根据一系列规则，将对内部主机发往其NAT公网IP地址的报文进行转换，并从私网接口发送给目标主机。目前有很多NAT设备不支持该技术，这种情况下，NAT网关在一些特定场合下将会阻断P2P穿越NAT的行为，打洞的尝试是无法成功的。好在现在已经有越来越多的NAT设备商开始加入到对该转换的支持中来。&lt;/p&gt;

&lt;p&gt;3典型P2P情景2： 两客户端位于不同的NAT设备后面（分属不同的内网）&lt;/p&gt;

&lt;p&gt;这是最普遍的一种情况（如图5所示）：客户端A与B经由各自的NAT设备与集中服务器建立UDP连接， A与B的本地端口号均为4321，集中服务器的公网端口号为1234。在向外的会话中， A的外网IP被映射为155.99.25.11，外网端口为62000；B的外网IP被映射为138.76.29.7，外网端口为31000。&lt;/p&gt;

&lt;p&gt;如下所示：
1
2
客户端A——&amp;gt;本地IP:10.0.0.1，本地端口:4321，外网IP:155.99.25.11，外网端口:62000
客户端B——&amp;gt;本地IP:10.1.1.3，本地端口:4321，外网IP:138.76.29.7，外网端口:31000&lt;/p&gt;

&lt;p&gt;图5 - 位于不同NAT设备后的UDP打洞过程：
 P2P技术详解(二)：P2P中的NAT穿越(打洞)方案详解_5.png&lt;/p&gt;

&lt;p&gt;在A向服务器发送的登陆消息中，包含有A的内网地址二元组信息，即10.0.0.1:4321；服务器会记录下A的内网地址二元组信息，同时会把自己观察到的A的外网地址二元组信息记录下来。同理，服务器也会记录下B的内网地址二元组信息和由服务器观察到的客户端B的外网地址二元组信息。无论A与B二者中的任何一方向服务器发送P2P连接请求，服务器都会将其记录下来的上述的外网和内网地址二元组发送给A或B。&lt;/p&gt;

&lt;p&gt;A和B分属不同的内网，它们的内网地址在外网中是没有路由的，所以发往各自内网地址的UDP数据包会发送到错误的主机或者根本不存在的主机上。当A的第一个消息发往B的外网地址（如图3所示），该消息途经A的NAT设备，并在该设备上生成一个会话表项，该会话的源地址二元组信息是{10.0.0.1:4321}，和A与服务器建立连接的时候NAT生成的源地址二元组信息一样，但它的目的地址是B的外网地址。在A的NAT设备支持保留A的内网地址二元组信息的情况下，所有来自A的源地址二元组信息为{10.0.0.1:4321}的数据包都沿用A与集中服务器事先建立起来的会话，这些数据包的外网地址二元组信息均被映射为{155.99.25.11:62000}。&lt;/p&gt;

&lt;p&gt;A向B的外网地址发送消息的过程就是“打洞”的过程，从A的内网的角度来看应为从{10.0.0.1:4321}发往{138.76.29.7:31000}，从A在其NAT设备上建立的会话来看，是从{155.99.25.11:62000}发到{138.76.29.7:31000}。如果A发给B的外网地址二元组的消息包在B向A发送消息包之前到达B的NAT设备，B的NAT设备会认为A发过来的消息是未经授权的外网消息，并丢弃该数据包。&lt;/p&gt;

&lt;p&gt;B发往A的消息包也会在B的NAT设备上建立一个{10.1.1.3:4321，155.99.25.11:62000}的会话（通常也会沿用B与集中服务器连接时建立的会话，只是该会话现在不仅接受由服务器发给B的消息，还可以接受从A的NAT设备{155.99.25.11:6200}发来的消息）。&lt;/p&gt;

&lt;p&gt;一旦A与B都向对方的NAT设备在外网上的地址二元组发送了数据包，就打开了A与B之间的“洞”，A与B向对方的外网地址发送数据，等效为向对方的客户端直接发送UDP数据包了。一旦应用程序确认已经可以通过往对方的外网地址发送数据包的方式让数据包到达NAT后面的目的应用程序，程序会自动停止继续发送用于“打洞”的数据包，转而开始真正的P2P数据传输。&lt;/p&gt;

&lt;p&gt;4典型P2P情景3： 两客户端位于两层(或多层)NAT设备之后（分属不同的内网）&lt;/p&gt;

&lt;p&gt;此种情景最典型的部署情况就像这样：最上层的NAT设备通常是由网络提供商（ISP）提供，下层NAT设备是家用路由器。&lt;/p&gt;

&lt;p&gt;如图6所示：假定NAT C是由ISP提供的NAT设备，NAT C提供将多个用户节点映射到有限的几个公网IP的服务，NAT A和NAT B作为NAT C的内网节点将把用户的内部网络接入NAT C的内网，用户的内部网络就可以经由NAT C访问公网了。从这种拓扑结构上来看，只有服务器与NAT C是真正拥有公网可路由IP地址的设备，而NAT A和NAT B所使用的公网IP地址，实际上是由ISP服务提供商设定的（相对于NAT C而言）内网地址（我们将这种由ISP提供的内网地址称之为“伪”公网地址）。同理，隶属于NAT A与NAT B的客户端，它们处于NAT A，NAT B的内网，以此类推，客户端可以放到到多层NAT设备后面。客户端A和客户端B发起对服务器S的连接的时候，就会依次在NAT A和NAT B上建立向外的Session，而NAT A、NAT B要联入公网的时候，会在NAT C上再建立向外的Session。&lt;/p&gt;

&lt;p&gt;图6 - 多层NAT下的打洞过程：
 P2P技术详解(二)：P2P中的NAT穿越(打洞)方案详解_6.png&lt;/p&gt;

&lt;p&gt;现在假定客户端A和B希望通过UDP“打洞”完成两个客户端的P2P直连。最优化的路由策略是客户端A向客户端B的“伪公网”IP上发送数据包，即ISP服务提供商指定的内网IP，NAT B的“伪”公网地址二元组，{10.0.1.2:55000}。由于从服务器的角度只能观察到真正的公网地址，也就是NAT A，NAT B在NAT C建立session的真正的公网地址{155.99.25.11:62000}以及{155.99.25.11:62005}，非常不幸的是客户端A与客户端B是无法通过服务器知道这些“伪”公网的地址，而且即使客户端A和B通过某种手段可以得到NAT A和NAT B的“伪”公网地址，我们仍然不建议采用上述的“最优化”的打洞方式，这是因为这些地址是由ISP服务提供商提供的或许会存在与客户端本身所在的内网地址重复的可能性（例如:NAT A的内网的IP地址域恰好与NAT A在NAT C的“伪”公网IP地址域重复，这样就会导致打洞数据包无法发出的问题）。&lt;/p&gt;

&lt;p&gt;因此客户端别无选择，只能使用由公网服务器观察到的A，B的公网地址二元组进行“打洞”操作，用于“打洞”的数据包将由NAT C进行转发。&lt;/p&gt;

&lt;p&gt;当客户端A向客户端B的公网地址二元组{155.99.25.11:62005}发送UDP数据包的时候，NAT A首先把数据包的源地址二元组由A的内网地址二元组{10.0.0.1:4321}转换为“伪”公网地址二元组{10.0.1.1:45000}，现在数据包到了NAT C，NAT C应该可以识别出来该数据包是要发往自身转换过的公网地址二元组，如果NAT C可以给出“合理”响应的话，NAT C将把该数据包的源地址二元组改为{155.99.25.11:62000}，目的地址二元组改为{10.0.1.2:55000}，即NAT B的“伪”公网地址二元组，NAT B最后会将收到的数据包发往客户端B。同样，由B发往A的数据包也会经过类似的过程。目前也有很多NAT设备不支持类似这样的“Hairpin转换”，但是已经有越来越多的NAT设备商开始加入对该转换的支持中来。&lt;/p&gt;

&lt;p&gt;5一个需要考虑的现实问题：UDP在空闲状态下的超时&lt;/p&gt;

&lt;p&gt;当然，从应用的角度上来说，在完成打洞过程的同时，还有一些技术问题需要解决，如UDP在空闲状态下的超时问题。由于UDP转换协议提供的“洞”不是绝对可靠的，多数NAT设备内部都有一个UDP转换的空闲状态计时器，如果在一段时间内没有UDP数据通信，NAT设备会关掉由“打洞”过程打出来的“洞”。如果P2P应用程序希望“洞”的存活时间不受NAT网关的限制，就最好在穿越NAT以后设定一个穿越的有效期。&lt;/p&gt;

&lt;p&gt;对于有效期目前没有标准值，它与NAT设备内部的配置有关，某些设备上最短的只有20秒左右。在这个有效期内，即使没有P2P数据包需要传输，应用程序为了维持该“洞”可以正常工作，也必须向对方发送“打洞”心跳包。这个心跳包是需要双方应用程序都发送的，只有一方发送不会维持另一方的Session正常工作。除了频繁发送“打洞”心跳包以外，还有一个方法就是在当前的“洞”超时之前，P2P客户端双方重新“打洞”，丢弃原有的“洞”，这也不失为一个有效的方法。
4、基于TCP协议的P2P打洞技术详细&lt;/p&gt;

&lt;p&gt;建立穿越NAT设备的P2P的TCP连接只比UDP复杂一点点，TCP协议的”“打洞”从协议层来看是与UDP的“打洞”过程非常相似的。尽管如此，基于TCP协议的打洞至今为止还没有被很好的理解，这也造成了的对其提供支持的NAT设备不是很多。在NAT设备支持的前提下，基于TCP的“打洞”技术实际上与基于UDP的“打洞”技术一样快捷、可靠。实际上，只要NAT设备支持的话，基于TCP的P2P技术的健壮性将比基于UDP技术的更强一些，因为TCP协议的状态机给出了一种标准的方法来精确的获取某个TCP session的生命期，而UDP协议则无法做到这一点。&lt;/p&gt;

&lt;p&gt;1套接字和TCP端口的重用&lt;/p&gt;

&lt;p&gt;实现基于TCP协议的P2P打洞过程中，最主要的问题不是来自于TCP协议，而是来自于应用程序的API接口。这是由于标准的伯克利(Berkeley)套接字的API是围绕着构建客户端/服务器程序而设计的，API允许TCP流套接字通过调用connect()函数来建立向外的连接，或者通过listen()和accept函数接受来自外部的连接，但是，API不提供类似UDP那样的，同一个端口既可以向外连接，又能够接受来自外部的连接。而且更糟的是，TCP的套接字通常仅允许建立1对1的响应，即应用程序在将一个套接字绑定到本地的一个端口以后，任何试图将第二个套接字绑定到该端口的操作都会失败。&lt;/p&gt;

&lt;p&gt;为了让TCP“打洞”能够顺利工作，我们需要使用一个本地的TCP端口来监听来自外部的TCP连接，同时建立多个向外的TCP连接。幸运的是，所有的主流操作系统都能够支持特殊的TCP套接字参数，通常叫做“SO_REUSEADDR”，该参数允许应用程序将多个套接字绑定到本地的一个地址二元组（只要所有要绑定的套接字都设置了SO_REUSEADDR参数即可）。BSD系统引入了SO_REUSEPORT参数，该参数用于区分端口重用还是地址重用，在这样的系统里面，上述所有的参数必须都设置才行。&lt;/p&gt;

&lt;p&gt;2打开P2P的TCP流&lt;/p&gt;

&lt;p&gt;假定客户端A希望建立与B的TCP连接。我们像通常一样假定A和B已经与公网上的已知服务器建立了TCP连接。服务器记录下来每个接入的客户端的公网和内网的地址二元组，如同为UDP服务的时候一样。&lt;/p&gt;

&lt;p&gt;从协议层来看，TCP“打洞”与UDP“打洞”是几乎完全相同的过程：
l  客户端A使用其与服务器的连接向服务器发送请求，要求服务器协助其连接客户端B；
l  服务器将B的公网和内网的TCP地址的二元组信息返回给A，同时，服务器将A的公网和内网的地址二元组也发送给B；
l  客户端A和B使用连接服务器的端口异步地发起向对方的公网、内网地址二元组的TCP连接，同时监听各自的本地TCP端口是否有外部的连接联入；
l  A和B开始等待向外的连接是否成功，检查是否有新连接联入。如果向外的连接由于某种网络错误而失败，如：“连接被重置”或者“节点无法访问”，客户端只需要延迟一小段时间（例如延迟一秒钟），然后重新发起连接即可，延迟的时间和重复连接的次数可以由应用程序编写者来确定；
l  TCP连接建立起来以后，客户端之间应该开始鉴权操作，确保目前联入的连接就是所希望的连接。如果鉴权失败，客户端将关闭连接，并且继续等待新的连接联入。客户端通常采用“先入为主”的策略，只接受第一个通过鉴权操作的客户端，然后将进入P2P通信过程不再继续等待是否有新的连接联入。&lt;/p&gt;

&lt;p&gt;图7 - TCP打洞：
 P2P技术详解(二)：P2P中的NAT穿越(打洞)方案详解_7.png&lt;/p&gt;

&lt;p&gt;与UDP不同的是，因为使用UDP协议的每个客户端只需要一个套接字即可完成与服务器的通信，而TCP客户端必须处理多个套接字绑定到同一个本地TCP端口的问题，如图7所示。现在来看实际中常见的一种情景，A与B分别位于不同的NAT设备后面，如图5所示，并且假定图中的端口号是TCP协议的端口号，而不是UDP的端口号。图中向外的连接代表A和B向对方的内网地址二元组发起的连接，这些连接或许会失败或者无法连接到对方。如同使用UDP协议进行“打洞”操作遇到的问题一样，TCP的“打洞”操作也会遇到内网的IP与“伪”公网IP重复造成连接失败或者错误连接之类的问题。&lt;/p&gt;

&lt;p&gt;客户端向彼此公网地址二元组发起连接的操作，会使得各自的NAT设备打开新的“洞”允许A与B的TCP数据通过。如果NAT设备支持TCP“打洞”操作的话，一个在客户端之间的基于TCP协议的流通道就会自动建立起来。如果A向B发送的第一个SYN包发到了B的NAT设备，而B在此前没有向A发送SYN包，B的NAT设备会丢弃这个包，这会引起A的“连接失败”或“无法连接”问题。而此时，由于A已经向B发送过SYN包，B发往A的SYN包将被看作是由A发往B的包的回应的一部分，所以B发往A的SYN包会顺利地通过A的NAT设备，到达A，从而建立起A与B的P2P连接。&lt;/p&gt;

&lt;p&gt;3从应用程序的角度来看TCP“打洞”&lt;/p&gt;

&lt;p&gt;从应用程序的角度来看，在进行TCP“打洞”的时候都发生了什么呢？假定A首先向B发出SYN包，该包发往B的公网地址二元组，并且被B的NAT设备丢弃，但是B发往A的公网地址二元组的SYN包则通过A的NAT到达了A，然后，会发生以下的两种结果中的一种，具体是哪一种取决于操作系统对TCP协议的实现：&lt;/p&gt;

&lt;p&gt;（1）A的TCP实现会发现收到的SYN包就是其发起连接并希望联入的B的SYN包，通俗一点来说就是“说曹操，曹操到”的意思，本来A要去找B，结果B自己找上门来了。A的TCP协议栈因此会把B作为A向B发起连接connect的一部分，并认为连接已经成功。程序A调用的异步connect()函数将成功返回，A的listen()等待从外部联入的函数将没有任何反映。此时，B联入A的操作在A程序的内部被理解为A联入B连接成功，并且A开始使用这个连接与B开始P2P通信。&lt;/p&gt;

&lt;p&gt;由于收到的SYN包中不包含A需要的ACK数据，因此，A的TCP将用SYN-ACK包回应B的公网地址二元组，并且将使用先前A发向B的SYN包一样的序列号。一旦B的TCP收到由A发来的SYN-ACK包，则把自己的ACK包发给A，然后两端建立起TCP连接。简单的说，第一种，就是即使A发往B的SYN包被B的NAT丢弃了，但是由于B发往A的包到达了A。结果是，A认为自己连接成功了，B也认为自己连接成功了，不管是谁成功了，总之连接是已经建立起来了。&lt;/p&gt;

&lt;p&gt;（2）另外一种结果是，A的TCP实现没有像（1）中所讲的那么“智能”，它没有发现现在联入的B就是自己希望联入的。就好比在机场接人，明明遇到了自己想要接的人却不认识，误认为是其他的人，安排别人给接走了，后来才知道是自己错过了机会，但是无论如何，人已经接到了任务已经完成了。然后，A通过常规的listen()函数和accept()函数得到与B的连接，而由A发起的向B的公网地址二元组的连接会以失败告终。尽管A向B的连接失败，A仍然得到了B发起的向A的连接，等效于A与B之间已经联通，不管中间过程如何，A与B已经连接起来了，结果是A和B的基于TCP协议的P2P连接已经建立起来了。&lt;/p&gt;

&lt;p&gt;第一种结果适用于基于BSD的操作系统对于TCP的实现，而第二种结果更加普遍一些，多数Linux和Windows系统都会按照第二种结果来处理。
5、本文小结&lt;/p&gt;

&lt;p&gt;在IP地址极度短缺的今天，NAT几乎已经是无所不在的一项技术了，以至于现在任何一项新技术都不得不考虑和NAT的兼容。作为当下应用最广泛的技术之一，P2P技术也必然要面对NAT这个障碍。&lt;/p&gt;

&lt;p&gt;打洞技术看起来是一项近似乎蛮干的技术，却不失为一种有效的技术手段。在集中服务器的帮助下，P2P的双方利用端口预测的技术在NAT网关上打出通道，从而实现NAT穿越，解决了NAT对于P2P的阻隔，为P2P技术在网络中更广泛的推广作出了非常大的贡献。&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/04/nat.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/04/nat.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>Chubby与Paxos</title>
        <description>&lt;p&gt;Chubby是一个面向松耦合分布式系统的锁服务，GFS（Google File System）和Big Table等大型系统都是用它来解决分布式协作、元数据存储和Master选举等一些列与分布式锁服务相关的问题。Chubby的底层一致性实现就是以Paxos算法为基础，Chubby提供了粗粒度的分布式锁服务，开发人员直接调用Chubby的锁服务接口即可实现分布式系统中多个进程之间粗粒度的同控制，从而保证分布式数据的一致性。&lt;/p&gt;

&lt;p&gt;　　2.1 设计目标&lt;/p&gt;

&lt;p&gt;　　Chubby被设计成为一个需要访问中心化的分布式锁服务。&lt;/p&gt;

&lt;p&gt;　　① 对上层应用程序的侵入性更小，使用一个分布式锁服务的接口方式对上层应用程序的侵入性更小，应用程序只需调用相应的接口即可使用分布式一致性特性，并且更易于保持系统已有的程序结构和网络通信模式。&lt;/p&gt;

&lt;p&gt;　　② 便于提供数据的发布与订阅，在Chubby进行Master选举时，需要使用一种广播结果的机制来向所有客户端公布当前Master服务器，这意味着Chubby应该允许其客户端在服务器上进行少量数据的存储和读取（存储主Master地址等信息），也就是对小文件的读写操作。数据的发布与订阅功能和锁服务在分布式一致性特性上是相通的。&lt;/p&gt;

&lt;p&gt;　　③ 开发人员对基于锁的接口更为熟悉，Chubby提供了一套近乎和单机锁机制一直的分布式锁服务接口。&lt;/p&gt;

&lt;p&gt;　　④ 更便捷地构建更可靠的服务，Chubby中通常使用5台服务器来组成一个集群单元（Cell），根据Quorum机制（在一个由若干个机器组成的集群中，在一个数据项值的选定过程中，要求集群中过半的机器达成一致），只要整个集群中有3台服务器是正常运行的，那么整个集群就可以对外提供正常的服务。&lt;/p&gt;

&lt;p&gt;　　⑤ 提供一个完整的、独立的分布式锁服务，Chubby对于上层应用程序的侵入性特别低，对于Master选举同时将Master信息等级并广播的场景，应用程序只需要向Chubby请求一个锁，并且在获得锁之后向相应的锁文件写入Master信息即可，其余的客户端就可以通过读取这个锁文件来获取Master信息。&lt;/p&gt;

&lt;p&gt;　　⑥ 提供粗粒度的锁服务，Chubby针对的应用场景是客户端获得锁之后会进行长时间持有（数小时或数天），而非用于短暂获取锁的场景。当锁服务短暂失效时（服务器宕机），Chubby需要保持所有锁的持有状态，以避免持有锁的客户端出现问题。而细粒度锁通常设计为为锁服务一旦失效就释放所有锁，因为其持有时间很短，所以其放弃锁带来的代价较小。&lt;/p&gt;

&lt;p&gt;　　⑦ 高可用、高可靠，对于一个由5太机器组成的集群而言，只要保证3台正常运行的机器，整个集群对外服务就能保持可用，另外，由于Chubby支持通过小文件读写服务的方式来进行Master选举结果的发布与订阅，因此在Chubby的实际应用中，必须能够支撑成百上千个Chubby客户端对同一个文件进行监控和读取。&lt;/p&gt;

&lt;p&gt;　　⑧ 提供时间通知机制，Chubby客户端需要实时地感知到Master的变化情况，这可以通过让你客户端反复轮询来实现，但是在客户端规模不断增大的情况下，客户端主动轮询的实时性效果并不理想，且对服务器性能和网络带宽压力都非常大，因此，Chubby需要有能力将服务端的数据变化情况以时间的形式通知到所有订阅的客户端。&lt;/p&gt;

&lt;p&gt;　　2.2 技术架构&lt;/p&gt;

&lt;p&gt;　　Chubby的整个系统结构主要由服务端和客户端两部分组成，客户端通过RPC调用和服务端进行通信，如下图所示。&lt;/p&gt;

&lt;p&gt;　　一个典型的Chubby集群（Chubby Cell），通常由5台服务器组成，这些副本服务器采用Paxos协议，通过投票的方式来选举产生一个获得过半投票的服务器作为Master，一旦成为Master，Chubby就会保证在一段时间内不会再有其他服务器成为Master，这段时期称为Master租期（Master Lease），在运行过程中，Master服务器会通过不断续租的方式来延长Master租期，而如果Master服务器出现故障，那么余下的服务器会进行新一轮的Master选举，最终产生新的Master服务器，开始新的Master租期。&lt;/p&gt;

&lt;p&gt;　　集群中的每个服务器都维护着一份服务端数据库的副本，但在实际运行过程中，只有Master服务器才能对数据库进行写操作，而其他服务器都是使用Paxos协议从Master服务器上同步数据库数据的更新。&lt;/p&gt;

&lt;p&gt;　　Chubby客户端通过向记录有Chubby服务端机器列表的DNS来请求获取所有的Chubby服务器列表，然后逐一发起请求询问该服务器是否是Master，在这个询问过程中，那些非Master的服务器，则会将当前Master所在的服务器标志反馈给客户端，这样客户端就能很快速的定位到Master服务器了。&lt;/p&gt;

&lt;p&gt;　　只要该Master服务器正常运行，那么客户端就会将所有的请求都发送到该Master服务器上，针对写请求，Master会采用一致性协议将其广播给集群中所有的副本服务器，并且在过半的服务器接受了该写请求后，再响应给客户端正确的应答，而对于读请求，则不需要在集群内部进行广播处理，直接由Master服务器单独处理即可。&lt;/p&gt;

&lt;p&gt;　　若该Master服务器发生故障，那么集群中的其他服务器会在Master租期到期后，重新开启新的一轮Master选举，通常一次Master选举大概花费几秒钟的时间，而如果其他副本服务器崩溃，那么整个集群继续工作，该崩溃的服务器会在恢复之后自动加入到Chubby集群中去，新加入的服务器首先需要同步Chubby最新的数据库数据，完成数据库同步之后，新的服务器就可以加入到正常的Paxos运作流程中与其他服务器副本一起协同工作。若崩溃后几小时后仍无法恢复工作，那么需要加入新的机器，同时更新DNS列表（新IP代替旧IP），Master服务器会周期性地轮询DNS列表，因此会很快感知服务器地址的变更，然后Master就会将集群数据库中的地址列表做相应的变更，集群内部的其他副本服务器通过复制方式就可以获取到最新的服务器地址列表了。&lt;/p&gt;

&lt;p&gt;　　2.3 目录与文件&lt;/p&gt;

&lt;p&gt;　　Chubby对外提供了一套与Unix文件系统非常相近但是更简单的访问接口。Chubby的数据结构可以看作是一个由文件和目录组成的树，其中每一个节点都可以表示为一个使用斜杠分割的字符串，典型的节点路径表示如下：&lt;/p&gt;

&lt;p&gt;　　/ls/foo/wombat/pouch&lt;/p&gt;

&lt;p&gt;　　其中，ls是所有Chubby节点所共有的前缀，代表着锁服务，是Lock Service的缩写；foo则指定了Chubby集群的名字，从DNS可以查询到由一个或多个服务器组成该Chubby集群；剩余部分的路径/wombat/pouch则是一个真正包含业务含义的节点名字，由Chubby服务器内部解析并定位到数据节点。&lt;/p&gt;

&lt;p&gt;　　Chubby的命名空间，包括文件和目录，我们称之为节点（Nodes，我们以数据节点来泛指Chubby的文件或目录）。在同一个Chubby集群数据库中，每一个节点都是全局唯一的。和Unix系统一样，每个目录都可以包含一系列的子文件和子目录列表，而每个文件中则会包含文件内容。Chubby没有符号链接和硬连接的概念。&lt;/p&gt;

&lt;p&gt;　　Chubby上的每个数据节点都分为持久节点和临时节点两大类，其中持久节点需要显式地调用接口API来进行删除，而临时节点则会在其对应的客户端会话失效后被自动删除。也就是说，临时节点的生命周期和客户端会话绑定，如果该临时节点对应的文件没有被任何客户端打开的话，那么它就会被删除掉。因此，临时节点通常可以用来进行客户端会话有效性的判断依据。&lt;/p&gt;

&lt;p&gt;　　Chubby的每个数据节点都包含了少量的元数据信息，其中包括用于权限控制的访问控制列表（ACL）信息，同时每个节点的元数据还包括4个单调递增的64位标号：&lt;/p&gt;

&lt;p&gt;　　① 实例标号，用于标识创建该数据节点的顺序，节点的创建顺序不同，其实例编号也不相同，可以通过实例编号来识别两个名字相同的数据节点是否是同一个数据节点，因为创建时间晚的数据节点，其实例编号必定大于任意先前创建的同名节点。&lt;/p&gt;

&lt;p&gt;　　② 文件内容编号（针对文件），用于标识文件内容的变化情况，该编号会在文件内容被写入时增加。&lt;/p&gt;

&lt;p&gt;　　③ 锁编号，用于标识节点锁状态的变更情况，该编号会在节点锁从自由状态转化到被持有状态时增加。&lt;/p&gt;

&lt;p&gt;　　④ ACL编号，用于标识节点的ACL信息变更情况，该编号会在节点的ACL配置信息被写入时增加。&lt;/p&gt;

&lt;p&gt;　　2.4 锁和锁序列器&lt;/p&gt;

&lt;p&gt;　　分布式环境中锁机制十分复杂，消息的延迟或是乱序都有可能引起锁的失效，如客户端C1获得互斥锁L后发出请求R，但请求R迟迟没有到达服务端（网络延迟或消息重发等），这时应用程序会认为该客户端进程已经失败，于是为另一个客户端C2分配锁L，然后在发送请求R，并成功应用到了服务器上。然而，之前的请求R经过一波三折后也到达了服务器端，此时，它可能不瘦任何锁控制的情况下被服务端处理，而覆盖了客户端C2的操作，也是导致了数据不一致问题。&lt;/p&gt;

&lt;p&gt;　　在Chubby中，任意一个数据节点均可被当做一个读写锁来使用：一种是单个客户端排他（写）模式持有这个锁，另一种则是任意数目的客户端以共享（读）模式持有这个锁，Chubby放弃了严格的强制锁，客户端可以在没有获取任何锁的情况下访问Chubby的文件。&lt;/p&gt;

&lt;p&gt;　　Chubby采用了锁延迟和锁序列器两种策略来解决上述由于消息延迟和重排序引起的分布式锁问题，对于锁延迟而言，如果一个客户端以正常的方式主动释放了一个锁，那么Chubby服务端将会允许其他客户端能够立即获取到该锁；如果锁是以异常情况被释放的话，那么Chubby服务器会为该锁保留一定的时间，这称之为锁延迟，这段时间内，其他客户端无法获取该锁，其可以很好的防止一些客户端由于网络闪断的原因而与服务器暂时断开的场景。对于锁序列器而言，其需要Chubby的上层应用配合在代码中加入相应的修改逻辑，任何时候，锁的持有者都可以向Chubby请求一个锁序列器，其包括锁的名字、锁模式（排他或共享）、锁序号，当客户端应用程序在进行一些需要锁机制保护的操作时，可以将该锁序列器一并发送给服务端，服务端收到该请求后，会首先检测该序列器是否有效，以及检查客户端是否处于恰当的锁模式，如果没有通过检查，那么服务端就会拒绝该客户端的请求。&lt;/p&gt;

&lt;p&gt;　　2.5 事件通知机制&lt;/p&gt;

&lt;p&gt;　　为了避免大量客户端轮询Chubby服务端状态所带来的压力，Chubby提供了事件通知机制，客户端可以向服务端注册事件通知，当触发这些事件时，服务端就会向客户端发送对应的时间通知，消息通知都是通过异步的方式发送给客户端的。常见的事件如下&lt;/p&gt;

&lt;p&gt;　　① 文件内容变更 ② 节点删除 ③ 子节点新增、删除 ④ Master服务器转移&lt;/p&gt;

&lt;p&gt;　　2.6 缓存&lt;/p&gt;

&lt;p&gt;　　其是为了减少客户端与服务端之间的频繁的读请求对服务端的压力设计的，会在客户端对文件内容和元数据信息进行缓存，虽然缓存提高了系统的整体性能，但是其也带来了一定复杂性，如如何保证缓存的一致性。其通过租期机制来保证缓存的一致性。&lt;/p&gt;

&lt;p&gt;　　缓存的生命周期和Master租期机制紧密相关，Master会维护每个客户端的数据缓存情况，并通过向客户端发送过期信息的方式来保证客户端数据的一致性，在此机制下，Chubby能够保证客户端要么能够从缓存中访问到一致的数据，要么访问出错，而一定不会访问到不一致的数据。具体的讲，每个客户端的缓存都有一个租期，一旦该租期到期，客户端就需要向服务端续订租期以继续维持缓存的有效性，当文件数据或元数据被修改时，Chubby服务端首先会阻塞该修改操作，然后由Master向所有可能缓存了该数据的客户端发送缓存过期信号，使其缓存失效，等到Master在接收到所有相关客户端针对该过期信号的应答（客户端明确要求更新缓存或客户端允许缓存租期过期）后，再进行之前的修改操作。&lt;/p&gt;

&lt;p&gt;　　2.7 会话&lt;/p&gt;

&lt;p&gt;　　Chubby客户端和服务端之间通过创建一个TCP连接来进行所有的网络通信操作，这称之为会话，会话存在生命周期，存在超时时间，在超时时间内，客户端和服务端之间可以通过心跳检测来保持会话的活性，以使会话周期得到延续，这个过程称为KeepAlive（会话激活），如果能够成功地通过KeepAlive过程将Chubby会话一直延续下去，那么客户端创建的句柄、锁、缓存数据等将仍然有效。&lt;/p&gt;

&lt;p&gt;　　2.8 KeepAlive请求&lt;/p&gt;

&lt;p&gt;　　Master服务端在收到客户端的KeepAlive请求时，首先会将该请求阻塞住，并等到该客户端的当前会话租期即将过期时，才为其续租该客户端的会话租期，之后再向客户端响应这个KeepAlive请求，并同时将最新的会话租期超时时间反馈给客户端，在正常情况下，每个客户端总是会有一个KeepAlive请求阻塞在Master服务器上。&lt;/p&gt;

&lt;p&gt;　　2.9 会话超时&lt;/p&gt;

&lt;p&gt;　　客户端也会维持一个和Master端近似相同（由于KeepAlive响应在网络传输过程中会花费一定的时间、Master服务端和客户端存在时钟不一致的现象）的会话租期。客户端在运行过程中，按照本地的会话租期超时时间，检测到其会话租期已经过期却尚未收到Master的KeepAlive响应，此时，它将无法确定Master服务端是否已经中止了当前会话，这个时候客户端处于危险状态，此时，客户端会清空其本地缓存并将其标记为不可用，同时客户端还会等待一个被称作宽限期的时间周期，默认为45秒，若在宽限期到期前，客户端与服务端之间成功地进行了KeepAlive，那么客户端就会再次开启本地缓存，否则，客户端就会认为当前会话已经过期了，从而终止本次会话。&lt;/p&gt;

&lt;p&gt;　　在客户端进入危险状态时，客户端会通过一个“jeopardy”事件来通知上层应用程序，如果恢复正常，客户端同样会以一个“safe”事件来通知应用程序可以继续正常运行，但如果客户端最终没能从危险状态中恢复过来，那么客户端会以一个“expired”事件来通知应用程序当前Chubby会话已经超时。&lt;/p&gt;

&lt;p&gt;　　2.10 Master故障恢复&lt;/p&gt;

&lt;p&gt;　　Master服务端会运行着会话租期计时器，用来管理所有的会话的生命周期，如果Master在运行过程中出现故障，那么该计时器就会停止，直到新的Master选举产生后，计时器才会继续计时，即从旧的Master崩溃到新的Master选举产生所花费的时间将不计入会话超时的计算中，这就等价于延长了客户端的会话租期，如果Master在短时间内就选举产生了，那么客户端就可以在本地会话租期过期前与其创建连接，而如果Master的选举花费了较长的时间，就会导致客户端只能清空本地的缓存而进入宽限期进行等待，由于宽限期的存在，使得会话能够很好地在服务端Master转化的过程中得到维持，整个Master的故障恢复过程中服务端和客户端的交互情况如下&lt;/p&gt;

&lt;p&gt;　　如上图所示，一开始在旧的Master服务器上维持了一个会话租期lease M1，在客户端上维持对应的lease C1，同时客户端的KeepAlive请求1一直被Master服务器阻塞着。一段时间后，Master向客户端反馈了KeepAlive响应2，同时开始了新的会话租期lease M2，而客户端在接收到该KeepAlive响应后，立即发送了新的KeepAlive请求3，并同时也开始了新的会话租期lease C2。至此，客户端和服务端的交互都是正常的，随后，Master发生了故障，从而无法反馈客户端的KeepAlive请求3，在此过程中，客户端检测到会话租期lease C2已经过期，它会清空本地缓存并进入宽限期，这段时间内，客户端无法确定Master上的会话周期是否也已经过期，因此它不会销毁它的本地会话，而是将所有应用程序对它的API调用都阻塞住，以避免出现数据不一致的现象。同时，在客户端宽限期开始时，客户端会向上层应用程序发送一个“jeopardy”事件，一段时间后，服务器开始选举产生新的Master，并为该客户端初始化了新的会话租期lease M3，当客户端向新的Master发送KeepAlive请求4时，Master检测到该客户端的Master周期号已经过期，因此会在KeepAlive响应5中拒绝这个客户端请求，并将最新的Master周期号发送给客户端，之后，客户端会携带上最新的Master周期号，再次发送KeepAlive请求6给Master，最终，整个客户端和服务端之间的会话就会再次回复正常。&lt;/p&gt;

&lt;p&gt;　　在Master转化的这段时间内，只要客户端的宽限足够长，那么客户端应用程序就可以在没有任何察觉的情况下，实现Chubby的故障恢复，但如果客户端的宽限期设置得比较短，那么Chubby客户端就会丢弃当前会话，并将这个异常情况通知给上层应用程序。&lt;/p&gt;

&lt;p&gt;　　一旦客户端与新的Master建立上连接之后，客户端和Master之间会通过互相配合来实现对故障的平滑恢复，新的Master会设法将上一个Master服务器的内存状态构建出来，同时，由于本地数据库记录了每个客户端的会话信息，以及持有的锁和临时文件等信息，因此Chubby会通过读取本地磁盘上的数据来恢复一部分状态。一个新的Master服务器选举之后，会进行如下处理。&lt;/p&gt;

&lt;p&gt;　　① 确定Master周期。Master周期用来唯一标识一个Chubby集群的Master统治轮次，以便区分不同的Master，只要发生Master重新选举，就一定会产生新的Master周期，即使选举前后Master是同一台机器。&lt;/p&gt;

&lt;p&gt;　　② 新的Master能够对客户端的Master寻址请求进行响应，但是不会立即开始处理客户端会话相关的请求操作。&lt;/p&gt;

&lt;p&gt;　　③ Master根据本地数据库中存储的会话和锁信息来构建服务器的内存状态。&lt;/p&gt;

&lt;p&gt;　　④ 至此，Master已经能够处理客户端的KeepAlive请求，但仍然无法处理其他会话相关的操作。&lt;/p&gt;

&lt;p&gt;　　⑤ Master会发送一个Master故障切换事件给每一个会话，客户端接收到这个事件后，会清空它的本地缓存，并警告上层应用程序可能已经丢失了别的事件，之后再向Master反馈应答。&lt;/p&gt;

&lt;p&gt;　　⑥ 此时，Master会一直等待客户端的应答，直到每一个会话都应答了这个切换事件。&lt;/p&gt;

&lt;p&gt;　　⑦ Master接收了所有客户端的应答之后，就能够开始处理所有的请求操作。&lt;/p&gt;

&lt;p&gt;　　⑧若客户端使用了一个故障切换之间创建的句柄，Master会重新为其创建这个句柄的内存对象，并执行调用，如果该句柄在之前的Master周期中就已经被关闭了，那么它就不能在这个Master周期内再次被重建了，这一机制就确保了由于网络原因使得Master接收到那些延迟或重发的网络数据包也不会错误的重建一个已经关闭的句柄。&lt;/p&gt;

&lt;p&gt;三、Paxos协议实现&lt;/p&gt;

&lt;p&gt;　　Chubby服务端的基本架构大致分为三层&lt;/p&gt;

&lt;p&gt;　　① 最底层是容错日志系统（Fault-Tolerant Log），通过Paxos算法能够保证集群所有机器上的日志完全一致，同时具备较好的容错性。&lt;/p&gt;

&lt;p&gt;　　② 日志层之上是Key-Value类型的容错数据库（Fault-Tolerant DB），其通过下层的日志来保证一致性和容错性。&lt;/p&gt;

&lt;p&gt;　　③ 存储层之上的就是Chubby对外提供的分布式锁服务和小文件存储服务。&lt;/p&gt;

&lt;p&gt;　　Paxos算法用于保证集群内各个副本节点的日志能够保持一致，Chubby事务日志（Transaction Log）中的每一个Value对应Paxos算法中的一个Instance（对应Proposer），由于Chubby需要对外提供不断的服务，因此事务日志会无限增长，于是在整个Chubby运行过程中，会存在多个Paxos Instance，同时，Chubby会为每个Paxos Instance都按序分配一个全局唯一的Instance编号，并将其顺序写入到事务日志中去。&lt;/p&gt;

&lt;p&gt;　　在多Paxos Instance的模式下，为了提升算法执行的性能，就必须选举出一个副本作为Paxos算法的主节点，以避免因为每一个Paxos Instance都提出提议而陷入多个Paxos Round并存的情况，同时，Paxos会保证在Master重启或出现故障而进行切换的时候，允许出现短暂的多个Master共存却不影响副本之间的一致性。&lt;/p&gt;

&lt;p&gt;　　在Paxos中，每一个Paxos Instance都需要进行一轮或多轮的Prepare-&amp;gt;Promise-&amp;gt;Propose-&amp;gt;Accept这样完整的二阶段请求过程来完成对一个提议值的选定，为了保证正确性的前提下尽可能地提高算法运行性能，可以让多个Instance共用一套序号分配机制，并将Prepare-&amp;gt;Promise合并为一个阶段。具体做法如下：&lt;/p&gt;

&lt;p&gt;　　① 当某个副本节点通过选举成为Master后，就会使用新分配的编号N来广播一个Prepare消息，该Prepare消息会被所有未达成一致的Instance和目前还未开始的Instance共用。&lt;/p&gt;

&lt;p&gt;　　② 当Acceptor接收到Prepare消息后，必须对多个Instance同时做出回应，这通常可以通过将反馈信息封装在一个数据包中来实现，假设最多允许K个Instance同时进行提议值的选定，那么：&lt;/p&gt;

&lt;p&gt;　　-当前之多存在K个未达成一致的Instance，将这些未决的Instance各自最后接受的提议值封装进一个数据包，并作为Promise消息返回。&lt;/p&gt;

&lt;p&gt;　　-同时，判断N是否大于当前Acceptor的highestPromisedNum值（当前已经接受的最大的提议编号值），如果大于，那么就标记这些未决Instance和所有未来的Instance的highestPromisedNum的值为N，这样，这些未决Instance和所有未来Instance都不能再接受任何编号小于N的提议。&lt;/p&gt;

&lt;p&gt;　　③ Master对所有未决Instance和所有未来Instance分别执行Propose-&amp;gt;Accept阶段的处理，如果Master能够一直稳定运行的话，那么在接下来的算法运行过程中，就不再需要进行Prepare-&amp;gt;Promise处理了。但是，一旦Master发现Acceptor返回了一个Reject消息，说明集群中存在另一个Master并且试图使用更大的提议编号发送了Prepare消息，此时，当前Master就需要重新分配新的提议编号并再次进行Prepare-&amp;gt;Promise阶段的处理。&lt;/p&gt;

&lt;p&gt;　　利用上述改进的Paxos算法，在Master稳定运行的情况下，只需要使用同一个编号来依次执行每一个Instance的Promise-&amp;gt;Accept阶段处理。&lt;/p&gt;

&lt;p&gt;　　在集群的某台机器在宕机重启后，为了恢复机器的状态，最简单的办法就是将已记录的所有日志重新执行一遍，但是如果机器上的日志已经很多，则耗时长，因此需要定期对状态机数据做一个数据快照并将其存入磁盘，然后就可以将数据快照点之前的事务日志清除。&lt;/p&gt;

&lt;p&gt;　　在恢复过程中，会出现磁盘未损坏和损坏两种情况，若未损坏，则通过磁盘上保存的数据库快照和事务日志就可以恢复到之前的某个时间点的状态，之后再向集群中其他正常运行的副本节点索取宕机后缺失的部分数据变更记录即可；若磁盘损坏，就笑从其他副本节点索取全部的状态数据。
　　chubby使用paxos作为日志错误容错的复制算法，在协议栈的最底层，paxos算法确保了每个replica的本地日志都有相同的entries，replicas的通信则是通过paxos-specific protocal，一旦某个值进入容错日志，每个replica会调用发送一个callback给客户端应用程序，告诉这个已提交的值&lt;/p&gt;

&lt;p&gt;chubby的paxos描述【隐藏了propose和promise的消息发送】&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;选出一个coordinator.&lt;/li&gt;
  &lt;li&gt;选择一个value，并广播给所有的replicas，该消息称为accept message，其他replicas不是acknowledge message就是reject it&lt;/li&gt;
  &lt;li&gt;一旦收到大多数的acknowledge， coordinator就广播commit 消息 to notify replicas&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;选择coordinator？&lt;/p&gt;

&lt;p&gt;在chubby中选择coordinator上，paxos确保在某一个值上【选出一个coordinator】达成一致通过引入两种机制：&lt;/p&gt;

&lt;p&gt;1）分配一个有序编号给后续的coordinators【通过产生一个最新的编号来成为coordinator， 如Replica r 的编号为s mod n = Ir ， 然后broadcasts all replicas in a propose message 等待replicas 回复 promise】&lt;/p&gt;

&lt;p&gt;2）严格限制每个coordinator的选值【什么意思呢？在选择coordinator的时候，使用的是fully paxos，即二阶段paxos，如果在第一阶段后收到一个已确定的值，则使用该值(使用该coordinator)】&lt;/p&gt;

&lt;p&gt;与ZK的一点区别？
chubby可以不断轮换coordinator。原文描述为if consensus was achieved by a previous coordinator, the new coordinator is guaranteed to hear about the value decided upon from at least one replica. By induction, that value will have the highest sequence number of all responses received, and so will be selected by the new coordinator. 这样可以轮流换coordinator，这点和ZK还是有区别，ZK一旦leader确定下载，如果不失败的话是不会改变的，但chubby可以不断轮换coordinator&lt;/p&gt;

&lt;p&gt;轮换coordinator 怎么解决活锁的问题呢？如果某段时间内有两个replicas一直争抢着要成为coordinator，这个怎么解决？&lt;/p&gt;

&lt;p&gt;chubby 解决活锁的办法：In our implementation the master periodically boosts its sequence number by running a full round of the Paxos algorithm, including sending propose messages. Boosting with the right frequency avoids this type of master churn in most cases. 即在希望成为master的时候，是间断性的 right frequency 来避免这一活锁问题&lt;/p&gt;

&lt;p&gt;对paxos还有点小不明白的就是并发的情况是怎么样的？有案例吗？&lt;/p&gt;

&lt;p&gt;我在看chubby的论文的时候，一直看到那个sequencer的东西，但这个sequencer似乎并不是看得很懂，paxos都是异步的，只要满足序号，并发处理是没有啥问题的&lt;/p&gt;

&lt;p&gt;处理拖后腿的replica？&lt;/p&gt;

&lt;p&gt;对于拖后腿的replica则让其catch up 那些leading replicas&lt;/p&gt;

&lt;p&gt;chubby如何处理优化的？
In a system where replicas are in close network proximity, disk flush time can dominate the overall latency of the implementation.
在replicas都很近的情况下，刷磁盘会很耗代价
使用了lambort的优化made simple的优化：
Propose messages may be omitted if the coordinator identity does not change between instances. This does not interfere with the properties of Paxos because any replica at any time can still try to become coordinator by broadcasting a propose message with a higher sequence number.
为了利用这一优化，所以尽可能的少轮换coordinatorpick a coordinator for long periods of time, trying not to let the coordinator change，但它又说利用了这点每个replica只要写一次磁盘就行了，master在发送accept的消息前写磁盘，其他的replicas在发送acknowledge消息前，写一次磁盘就行了
另一种为增加吞吐量，In order to get additional throughput in a concurrent system, it is possible to batch a collection of values submitted by di?erent application threads into a single Paxos instance&lt;/p&gt;

&lt;p&gt;如何处理disk corruption
原因是A disk may be corrupted due to a media failure or due to an operator error (an operator may accidentally erase critical data). When a replica’s disk is corrupted and it loses its persistent state, it may renege on promises it has made to other replicas in the past.
由于不保证它的promise，因此，这违背了paxos的算法的一个关键假设：P2B If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v. 即如果一个议案被选择了，那么此后，任何proposer提出的议案(编号更高)包含的决议都是v
解决disk corruptions的情况：
Either filele(s) contents may change or file(s) may become inaccessible，解决第一种是将文件checksum存储在文件中 后边的问题检测是replica在第一次启动后存放一个marker在GFS中，如果再次启动发现一个empty disk 并且在GFS中有这个标示，则说明是corrupted disk
重建replica的状态  It participates in Paxos as a non-voting member; meaning that it uses the catch-up mechanism to catch up but does not respond with promise or acknowledgment messages. It remains in this state until it observes one complete instance of Paxos that was started after the replica started rebuilding its state. By waiting for the extra instance of Paxos, we ensure that this replica could not have reneged on an earlier promise
另外，论文还说到，利用这一机制还能降低系统的延迟，即可以接受不需要立即刷磁盘。但他们也说还没有实现，不知道现在有没有实现好&lt;/p&gt;

&lt;p&gt;master lease的问题
as long as the master has the lease, it is guaranteed that other replicas cannot successfully submit values to Paxos. Thus a master with the lease has up-to-date information in its local data structure which can be used to serve a read operation purely locally. By making the master attempt to renew its lease before it expires we can ensure that a master has a lease most of the time. With our system, masters successfully maintain leases for several days at a time.
意思是如果master有lease的话，则其他的replicas不能重新选coordinator，这里的coordinator和master其实是同一个概念，即每一次只有一个master，然后他们的系统中说master一次成功维持了几天。通过master lease 这样 read 操作总是能读到本地最新的数据，但，是不是也可能读到stale的值呢？我想应该也是有可能的，比如我刚提交的一个值，我马上读，这有可能就读不到，这点其实和ZK应该是一样的，区别应该还是在于chubby可以要求成为master，当然，chubby后面也说了，基于这个的lease机制，如果replicas使用的话，也可以让clients直接从本地replicas上读，说还没有实现。&lt;/p&gt;

&lt;p&gt;master turnover以及abort会有什么影响？那如何检测master turnover和abort operations？
似乎对客户端也没有什么影响，只是需要告诉客户端master改变了，你应该发送请求到另外的一个master去，如果返回epoch number相同表明没有变换过&lt;/p&gt;

&lt;p&gt;chubby 如何通知客户端master是哪个？
每个replica都知道的&lt;/p&gt;

&lt;p&gt;关于group membership
对于group membership似乎还不是很清楚，chubby中也只是说文献没有指出 paxos也没有证明使用该算法来实现group membership的正确性，基本上在该文献中是一笔带过，回头看看其他文献有没有说明这个东西&lt;/p&gt;

&lt;p&gt;snapshots的问题
两个问题： it requires unbounded amounts of disk space; and perhaps worse, it may result in unbounded recovery time since a recovering replica has to replay a potentially long log before it has fully caught up with other replicas
所以有必要在某一点上的操作日志对应的内存的数据结构序列化到磁盘，truncate掉之前的日志记录
另外，snapshots的问题并不是由paxos framework来做的，因为its only concern is the consistency of the replicated log.&lt;/p&gt;

&lt;p&gt;the lagging replica asks for and receives the remaining log records from the leading replica to bring it fully up-to-date&lt;/p&gt;

&lt;p&gt;关于snapshot的时候必须对应日志号，那么在线的时候应该怎么处理？以下是论文中的说法&lt;/p&gt;

&lt;p&gt;Our ﬁrst implementation of the fault-tolerant database blocked the system very brieﬂy while making an in-memory copy of the (small) database. It then stored the copied data on disk via a separate thread. Subsequently we implemented virtually pause-less snapshots. We now use a “shadow” data structure to track updates while the underlying database is serialized to disk.&lt;/p&gt;

&lt;p&gt;如果直接accept消息而不是完全paxos的算法会有什么问题吗？
这回到了为什么要使用两阶段的问题，按lambort的说话，paxos made simple 中说第一阶段保证了某一proposal被选择了，第二阶段保证最高的proposal对应的值都是value&lt;/p&gt;

&lt;p&gt;chubby使用的是TCP还是UDP？&lt;/p&gt;

&lt;p&gt;原文这样描述：This would seem ideal, except that it introduced a tension in our choice of protocol. TCP’s back off policies pay no attention to higher-level timeouts such as Chubby leases, so TCP-based KeepAlives led to many lost sessions at times of high network congestion. We were forced to send KeepAlive RPCs via UDP rather than TCP; UDP has no congestion avoidance mechanisms, so we would prefer to use UDP only when high-level timebounds must be met.&lt;/p&gt;

&lt;p&gt;引用&lt;/p&gt;

&lt;p&gt;chubby的介绍论文 http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/chubby-osdi06.pdf&lt;/p&gt;

&lt;p&gt;chubby的实现论文，Paxos Made Live - An Engineering Perspective http://www.cs.ucla.edu/~kohler/class/08w-dsi/chandra07paxos.pdf&lt;/p&gt;

&lt;p&gt;在分布式系统设计领域，Paxos可谓是最重要一致性的算法。Google的大牛们称&lt;/p&gt;

&lt;p&gt;All working protocols for asynchronous consensus we have so far encountered have Paxos at their core.&lt;/p&gt;

&lt;p&gt;可见此算法的地位。网络上讨论此算法的文章多如牛毛，但大多数让人看了之后仍然是一头雾水，就连维基百科中，对此算法的描述亦有含糊和错误之处。但实际上，此算法的核心思想还是比较简单的，只是大多数文章的分析脱离了实际应用，或是陷入大量实现细节以致掩盖了算法的核心。本文将先给出Paxos算法的设计目的，和算法流程，再反过来分析算法的原理。&lt;/p&gt;

&lt;p&gt;Paxos算法实现的是分布式系统多个结点之上数据的一致性，这个算法有如下特性&lt;/p&gt;

&lt;p&gt;1.基于消息传递，允许消息传输的丢失，重复，乱序，但是不允许消息被攥改&lt;/p&gt;

&lt;p&gt;2.在结点数少于半数失效的情况下仍然能正常的工作，结点失效可以在任何时候发生而不影响算法正常执行。&lt;/p&gt;

&lt;p&gt;下面是Basic Paxos算法，注意，这个算法只具有在多个冲突请求中选出一个的功能，并不具备序列化多个请求依次执行的功能。&lt;/p&gt;

&lt;p&gt;Paxos算法包含三个角色Proposor，Acceptor，Learner。&lt;/p&gt;

&lt;p&gt;实现的时候采用一组固定数目Server，每个Server同时担任上述三个角色，多个Client将自己的请求值Value_i随机发给一个Server处理，然后这一组Server经过协商后得出统一的一个值Chosen_Value，这个值必须被每个Server学习到，同时回复给所有发起请求的Client。&lt;/p&gt;

&lt;p&gt;具体算法流程如下，为避免歧义，关键字眼Propose,Proposal,Accept,Value,Choose等保留英文原文。&lt;/p&gt;

&lt;p&gt;阶段1a—Prepare（预定Proposal序号）&lt;/p&gt;

&lt;p&gt;每个Proposor 拿到某个Client的请求Value_i后，在此阶段还不能发起Proposal，只能发送一个Proposal序号N，将序号发送给所有Acceptor（即所有Server包括自己），整个系统中所有Proposal的序号不能重复而且每个Proposor自己用到的序号必须是递增的，通常的做法是，假设K台Server协同运行Paxos算法，那么Server_i（i=0…K-1)用的Proposal序号初始值为i，以后每次要产生新序号时递增K，这样保证了所有Server的Proposal序号不重复。&lt;/p&gt;

&lt;p&gt;阶段1b—Respond with Promise&lt;/p&gt;

&lt;p&gt;每个Acceptor收到Proposal序号后，先检查之前是否Repond序号更高的Proposal，若没有，那么就给出Response，这个Response带有自己已经Accept的序号最高的Proposal（若还没Accept任何Proposal，回复null），同时，Promise自己不再Accept低于接收序号的Proposal。否则，拒绝Respond。&lt;/p&gt;

&lt;p&gt;阶段2a—发起Proposal，请求Accept&lt;/p&gt;

&lt;p&gt;Proposal如果得到了来自超过半数的Acceptor的Response，那么就有资格向Acceptor发起Proposal&amp;lt;N,value&amp;gt;。其中，N是阶段1a中发送的序号，value是收到的Response中序号最大的Proposal的Value，若收到的Response全部是null，那么Value自定义，可直接选一个Client请求的Value_i&lt;/p&gt;

&lt;p&gt;阶段2b–Accept Proposal&lt;/p&gt;

&lt;p&gt;检查收到的Proposal的序号是否违反阶段1b的Promise，若不违反，则Accept收到的Proposal。&lt;/p&gt;

&lt;p&gt;所有Acceptor Accept的Proposal要不断通知所有Learner，或者Learner主动去查询，一旦Learner确认Proposal已经被超过半数的Acceptor Accept，那么表示这个Proposal 的Value 被 Chosen，Learner就可以学习这个Proposal的Value，同时在自己Server上就可以不再受理Proposor的请求。&lt;/p&gt;

&lt;p&gt;这个算法能达到什么效果呢，只要保证超过半数的Server维持正常工作，同时连接工作Server的网络正常（网络允许消息丢失，重复，乱序），就一定能保证，&lt;/p&gt;

&lt;p&gt;P2a: 在将来某一时刻，自从某个Proposal被多数派Acceptor Accept后，之后Accept的Proposal Value一定和这个Proposal Value相同。&lt;/p&gt;

&lt;p&gt;这就是整个算法的关键，保证了这一点，剩下的Learn Value过程就简单了，无需再为消息丢失，Server宕机而担心，例如，假设5台Server编号0~4，Server0，Server1，Server2已经Accept Proposal 100，然后Server0,Server1学习到Proposal 100，刚学习完成Server0,Server1就都宕机了，但这时候，Server2 Server3和Server4由于没有学习到Chosen value，因此还要继续提出Proposal，然后呢，根据这个神奇的算法，最后能使得Server3 Server4将来Accept的值一定是之前选出来过的Proposal 100的Value。&lt;/p&gt;

&lt;p&gt;看到这里，大家应该能够隐隐猜到，在这个过程中，Server2之前Accept Proposal 100的Value起了关键作用，下面，我们就来严格证明上述红色字体表示的算法关键点：&lt;/p&gt;

&lt;p&gt;首先回顾前边两阶段协议的几个关键点：&lt;/p&gt;

&lt;p&gt;1.发起Proposal前要先获得多数派Acceptor中Accept过的序号最大的Proposal Value。若Value为null才能采用自己的Value。&lt;/p&gt;

&lt;p&gt;2.阶段1b Promise自己不再Accept低于接收序号的Proposal。&lt;/p&gt;

&lt;p&gt;3.Propsal被超半数的Acceptor Accept才能被认定为Chosen Value从而被Learner学习。&lt;/p&gt;

&lt;p&gt;这几个约束条件共同作用，达到了上述P2a要求的效果，Paxos算法提出者Leslie Lamport是怎么构造出来的呢，事实上很简单：&lt;/p&gt;

&lt;p&gt;首先，把P2a加强为如下条件：&lt;/p&gt;

&lt;p&gt;P2b:自从某个Proposal被多数派Acceptor Accept后，之后Proposor提出的Proposal Value一定和这个Proposal Value相同。&lt;/p&gt;

&lt;p&gt;显而易见，由P2b可以推出P2a，那么怎么满足P2b呢，实际上，只要满足如下条件：&lt;/p&gt;

&lt;p&gt;P2c:发起的Proposal的Value为任意一个多数派Acceptor集合中Accept过的序号最大的Proposal Value。若这个Acceptor集合中没有Accept过Proposal才能采用自己的Value。&lt;/p&gt;

&lt;p&gt;如何从P2c推出P2b呢，利用数学归纳法可以轻易做出证明：假设在某一时刻一个超半数Acceptor集合C共同Accept了某个Proposal K，由于集合C和任意一个多数派Acceptor集合S必有一个共同成员，那么，在这个时刻之后，任意一个多数派Acceptor集合S 中Accept过的最大序号的Proposal只可能是Proposal K或序号比Proposal K更大的Proposal，假设为Proposal K2。同理，Proposal K2的Value等于Proposal K或Proposal K3的Value，而K&amp;lt;K3&amp;lt;K2，递推下去，最终推出根据P2c定出的Value必然是Proposal K的Value。&lt;/p&gt;

&lt;p&gt;我们可以看到，P2c条件基本就是上述两阶段协议的关键点1，但是还有一个问题，这个P2c条件要求找出这个“最大序号Value”和提出Proposal必须是一个原子操作，这实际上是难以实现的，所以，上述两阶段协议用了一个巧妙的方法避开了这个问题，这就是上述关键点2 Promise所起的作用了。在Acceptor respond“最大序号Value”的时候，Promise不再Accept低于收到序号的Proposal，这样“找出这个‘最大序号Value’”和“提出Proposal”之间就不可能插入新的被Accept的序号，从而避免P2c条件被破坏。&lt;/p&gt;

&lt;p&gt;到这里为止，基本的Paxos算法就已经透彻分析完了，但是，现在这个算法是使用多个Proposal，会造成活锁问题，需要引入leader来优化，而且，这个算法还只能实现在多个冲突Value中选举一个Value的功能，至于序列化多个Value实现状态机，就需要multi-paxos算法。
Google Chubby是一个分布式锁服务，能存储小文件。GFS和big table用它来进行分布式协作、储存元数据。Chubby是一致性算法Paxos很好的一个实例。相对于Leslie Lamport老先生等大牛学术上的描述，Chubby给出了一个更贴近实战的描述。我这里将最要注意力集中在paxos上。这里的讨论不考虑拜占庭失效问题。&lt;/p&gt;

&lt;p&gt;Chubby&lt;/p&gt;

&lt;p&gt;集群由若干独立的replica构成，replica在结构和能力上相互对等
replica用paxos来保持log的一致性
replica都有可能离线，然后重新上线。重新上线后，需要保持与其它节点数据的一致
replica的结构&lt;/p&gt;

&lt;p&gt;replica有个2个大件：容错的数据库 和 容错的日志。日志部分，每个replica都存有一份本地数据。&lt;/p&gt;

&lt;p&gt;replica提交一次value的过程&lt;/p&gt;

&lt;p&gt;chubby中的paxos&lt;/p&gt;

&lt;p&gt;大体步骤是这样的：&lt;/p&gt;

&lt;p&gt;选举一个replica成为coordinator
coordinator选择一个value，广播给所有的replica，这个消息称为accept。replica收到消息后，可以选择同意或者拒绝，并将决定告诉coordinator
当coordinator收到多数replica的同意确认后，就认为一致性达到了，并向相关replica发送commit消息
coordinator失效&lt;/p&gt;

&lt;p&gt;因为coordinator可能失效，所以paxos允许同时有多个coordinator。多个replica可以在同时决定称为coordinator并在任意时间执行算法。但是在系统，我们还是要尽量避免coordinator的轮转，因为这样会延缓达到一致。这种灵活的选举制度意味着同时会有不同的replica决定成为coodinator并提交不同的value。cubby就用了paxos中的2种机制来解决：1)给coodinator分配序号 2)限定coodinator能选择的value。&lt;/p&gt;

&lt;p&gt;具体这样来实现：&lt;/p&gt;

&lt;p&gt;每个replica都记录了它知道的coodinator中最大的编号。这样，它就通过发送过来的accept请求带的编号，拒绝旧coodinator的请求。&lt;/p&gt;

&lt;p&gt;当一个replica想成为coordinator的时候，它给自己分配一个唯一的序号。chubby的作者举例了一个序号生成的方法：&lt;/p&gt;

&lt;p&gt;有n个replica，每个有一个id，0 &amp;lt;= id &amp;lt;= n - 1。每个replica的序号是 N * n + id，N的初始值是0。
然后将序号广播给其它replica，这称之为propose消息。
其它replica收到propose消息，并将自己之前知道的最大序号返回，这称之为promise消息。如果propose消息中的序号是最大的，replica会承诺不再同意旧coodinator发送的accept消息
如果多数的replica回复的最大的序号小于生成的序号，replica就成为一个coodinator。
paxos算法强迫新的coodinator必须选择和前任相同的value。在2中的promise消息中，包含了每个节点上一次接收到的消息。如果没有收到任何promise消息中包含的value，coordinator可以自行选择value。&lt;/p&gt;

&lt;p&gt;有一个常见的优化，选择一个replica作为coordinator之后，就长期不变了，暂称之为master。&lt;/p&gt;

&lt;p&gt;对集群不断执行paxos算法，就能达到一致性。在Chubby中，提交一次value到log就触发一次paxos执行。&lt;/p&gt;

&lt;p&gt;磁盘失效&lt;/p&gt;

&lt;p&gt;chubby在文件中加入校检合，来探测文件损坏。
新replica节点在启动的时候，会在GFS存一个标记，表示自己的是新的。当replica启动时，发现自己磁盘是空的，它就去检测GFS中的marker。这样来区分新replica和磁盘不可访问的情况
Master租约&lt;/p&gt;

&lt;p&gt;当有多Master同时存在的时候，会出现master间数据不一致的情况。这样在master上的读，就可能读到脏数据。chubby的解决方法是只要有一个master拥有租约，其它的replica就不能成功地提交value。chubby的replica在租约的有效期内，默认拒绝其它replica的请求。master间断性地提交空的”心跳“value到Paxos集群来刷新租约。&lt;/p&gt;

&lt;p&gt;Paxos算法在工程实现时，会遇到非常多的问题，工程实现中很多细节算法并不涉及，同时如何达到较好的性能和稳定性也是一个挑战。Google的分布式锁服务chubby底层就是以Paxos算法作为基础的，这给我们提供了一个很好的范例，展示了如何填补Paxos基本算法在工程实现中的空白之处。&lt;/p&gt;

&lt;p&gt;Chubby是以5台独立的机器组成一个cell来提供一个可靠的锁服务，5台机中只要不超过两台出错都不影响服务运行。Chubby的基本架构大致分为三层：&lt;/p&gt;

&lt;p&gt;最底层是 log replication，通过Paxos算法保证5台机的log完全一致，同时具备容错性
log层之上就是Key-Value类型的数据存储层，通过下层的log来保证一致性和容错性
存储层之上再实现Chubby提供的锁服务和小文件存储服务
示意图如下&lt;/p&gt;

&lt;p&gt;先从Log层说起。&lt;/p&gt;

&lt;p&gt;每台机器的数据存储状态可看做一个状态机，只要给定相同的输入序列，状态机就能保证一致的变化，这就是 state machine replication。所以呢，这里的Log层目的就是实现一致的log replication。&lt;/p&gt;

&lt;p&gt;但是Paxos算法只能从多个不同proposal value中确定一个一致的value，而这里log需要确定的是无限多个value（提供不间断服务，log无限增长），因此，每个value的确定需要一个Paxos instance。多个instance之间不相干，可以并行进行。当然每个instance也需要一个唯一的instance编号，instance编号按序分配并顺序写入log。把Paxos每个两阶段提交过程Prepare-&amp;gt;Promise-&amp;gt;Propose-&amp;gt;Accept称作一个round，每个Paxos instance内又可能经过多个round才达成一致。这就是Multi-Paxos算法。&lt;/p&gt;

&lt;p&gt;上述算法存在大量的优化空间：&lt;/p&gt;

&lt;p&gt;多个Proposor的活锁问题会严重影响效率，导致每个instance可能要多个round才能达成一致。
在每个replica上，多个instance的Prepare-&amp;gt;Promise阶段可以合并成一个。
因此必须选举一个master作为唯一的proposor。master宕机后其它机器自动再次选举。Paxos算法能够容忍master的“不安全状态”。也就是说，在master切换之时，允许出现短暂的多个master共存，Paxos算法可以保证replica log一致性。
先考虑第二点，如何合并多个instance的Prepare-&amp;gt;Promise阶段。原本，多个instance之间是完全独立的，每个instance自己决定每一个round序号，保证在instance内部不重复即可，但现在为了合并Prepare-&amp;gt;Promise阶段，多个instance公用一套序号分配，具体做法如下：&lt;/p&gt;

&lt;p&gt;当某个replica通过选举获得master资格后，用新分配的编号N广播一个Prepare消息，这个Prepare消息被所有未达成一致的instance和将来还未开始的instance共用。
当Acceptor接收到Prepare后，现在必须对多个instance同时做出回应，这可以封装在一个数据包中，假设最多允许K个instance同时选举，那么：
当前至多有K个未达成一致的instance，将这些未决的instance各自最新accept的value（若没有用null代替）封装进一个数据包，作为Promise消息返回
同时，标记这些未决instance和所有未来instance的highestPromisedNum为N，如果N比它们原先的值大的话。这样，这些未决instance和所有未来instance都不能再accept编号小于N的Proposal。
然后master就可以对所有未决instance和所有未来instance分别执行Propose-&amp;gt;Accept阶段，始终使用编号N，如果这个master保持稳定的话，就再也不需要Prepare-&amp;gt;Promise了。但是，一旦发现acceptor返回了一个reject消息，说明另一个master启动，用更大的编号M&amp;gt;N发送了Prepare消息，这时自己就要分配新的编号(必须比M更大)再次进行Prepare-&amp;gt;Promise阶段。
上述改进的算法，在master稳定的时候，只需要用同一个编号依次执行每个instance的Promise-&amp;gt;Accept阶段，每个instance在收到多数派的Accept后，就可以将value写入本地log并广播commit消息，其它replica收到commit消息就可将value写入log。若因为宕机或者网络原因错过了commit消息，可以主动向其它replica查询。在多个master共存的时候，也能保证多个replica的一致性。同时，只要维持多数派机器正常运行，其它机器在任意时刻宕机，都能保证已经commit的value的安全性。&lt;/p&gt;

&lt;p&gt;这里面还有一个小的可以改进的地方。如果允许并行执行多个instance，master切换之时，新的master收到的Promise消息可能包含不连续的未决instance，即出现“gap”，state machine replication执行的时候必须按顺序执行log，遇到gap就必须等待gap处的instance达成一致value才能继续执行。为了缩短卡在gap处的时间尽快执行后续log指令，在Promise阶段对gap处提交no-operation指令，最后执行log指令时碰到no-op直接跳过。&lt;/p&gt;

&lt;p&gt;对state machine replication的读操作，如果要保证读到最新的数据，必须也为读操作建立一个Paxos instance，序列化写入log，这样对大量的读操作性能就不高。因此，我们需要一个“安全”的选举算法，保证任意时候不出现多个master，这样，就可以对非master机器禁止commit操作，然后将读操作全部集中到master上，这样就能保证读操作始终读到最新的数据。如何实现这个“安全”的选举算法，请点击这里。&lt;/p&gt;

&lt;p&gt;实现了一致的log replication，就可以在上层实现一个一致的state machine replication，这就是前边图中的fault-tolerant DB层。DB层在内存中的数据结构这里不做讨论，这里就大致说下snapshot+replay log的实现，这是比较简单的一个问题。&lt;/p&gt;

&lt;p&gt;在宕机重启以后，为了恢复state machine 状态，需要将已有的log重新执行一遍，但是如果log积累了很多，那么恢复的时间就非常长，因此需要定期对state machine做一个snapshot存入磁盘，然后就可以将snapshot点之前的log删去。为了避免snapshot阻塞state machine的更新操作，可以建立一个shadow state machine，平常执行log时分别在state machine和shadow上执行，在开始snapshot后，冻结shadow，但不影响原state machine执行，snapshot完成后，再让shadow追赶上最新的log。在新的snapshot完成后才能删除旧的snapshot，这样snapshot执行一半时宕机也不影响恢复。如果state machine占用空间非常大，那么这种简单的整体snapshot方式可能开销就比较大，可以使用更好的办法，这个问题放到别的文章里讨论。&lt;/p&gt;

&lt;p&gt;replica宕机后的恢复，分为两种情况，一种是磁盘未损坏，盘上snapshot+log可以恢复到之前某个时间的状态，然后向别的replica索取宕机后缺失的部分，一种磁盘损坏用空盘代替，这时就需要从别的replica索取整个状态，但处理方法是类似的，如果缺的比较少，可能只需要传输近期的log就够了，如果宕机太久或者需要整个重建，那就要传输最近的snapshot+log。&lt;/p&gt;

&lt;p&gt;replica宕机重启之后，为了安全起见，暂时不能立即开始参与Paxos instance，需要等待观测到K个Paxos instance成功完成之后，K是允许并发的instance数目。这样就能保证新分配的编号（比观测到的都大）不和自己以前发过的重复。前边提到的一致读操作，也要等到这个时刻到来以后才允许开始。&lt;/p&gt;

&lt;p&gt;log是否需要实时commit进磁盘？只要任意时刻保证多数派的机器正常运行，那么宕机后未flush到磁盘的一小部分log也可以从正常的replica中获取，因此不需要实时flush log。这可以极大的提高写盘效率。&lt;/p&gt;

&lt;p&gt;在Fault-tolerant DB层之上，就可以比较容易的构建一个分布式锁服务–Chubby，当然需要讨论的问题还有很多，如Chubby中client cache一致性，session状态恢复，keep-Alive机制等等，且听下回分解。&lt;/p&gt;

</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/04/chubby.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/04/chubby.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
  </channel>
</rss>
