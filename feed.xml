<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 27 Jan 2018 14:25:33 +0800</pubDate>
    <lastBuildDate>Sat, 27 Jan 2018 14:25:33 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>导入第三方依赖到shell</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;import SparkContext
这是spark下面已经有这个jar包的存在了
spark-shell下面包含所有的spark和java的依赖
但是对于第三代jar包，需要先将第三方依赖（jar包）导入到spark-shell下面才行
spark-shell –jars /home/wangtuntun/下载/nscala-time_2.10-2.12.0.jar
如果需要导入多个依赖，之间用逗号隔开
前提要配置spark-shell到环境变量&lt;/p&gt;
</description>
        <pubDate>Sat, 27 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/27/spark_jar.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/27/spark_jar.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>zero copy</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;许多web应用都会向用户提供大量的静态内容，这意味着有很多data从硬盘读出之后，会原封不动的通过socket传输给用户。这种操作看起来可能不会怎么消耗CPU，但是实际上它是低效的：kernal把数据从disk读出来，然后把它传输给user级的application，然后application再次把同样的内容再传回给处于kernal级的socket。这种场景下，application实际上只是作为一种低效的中间介质，用来把disk file的data传给socket。&lt;/p&gt;

&lt;p&gt;data每次穿过user-kernel boundary，都会被copy，这会消耗cpu，并且占用RAM的带宽。幸运的是，你可以用一种叫做Zero-Copy的技术来去掉这些无谓的copy。应用程序用zero copy来请求kernel直接把disk的data传输给socket，而不是通过应用程序传输。Zero copy大大提高了应用程序的性能，并且减少了kernel和user模式的上下文切换。&lt;/p&gt;

&lt;p&gt;Java的libaries在linux和unix中支持zero copy，一个关键的api是java.nio.channel.FileChannel的transferTo()方法。我们可以用transferTo()来把bytes直接从调用它的channel传输到另一个writable byte channel，中间不会使data经过应用程序。本文首先描述传统的copy是怎样坑爹的，然后再展示zero-copy技术在性能上是多么的给力以及为什么给力。&lt;/p&gt;

&lt;p&gt;Date transfer: The traditional approach&lt;/p&gt;

&lt;p&gt;考虑一下这个场景，通过网络把一个文件传输给另一个程序。这个操作的核心代码就是下面的两个函数：&lt;/p&gt;

&lt;p&gt;Listing 1. Copying bytes from a file to a socket&lt;/p&gt;

&lt;p&gt;File.read(fileDesc, buf, len);
Socket.send(socket, buf, len);
尽管看起来很简单，但是在OS的内部，这个copy操作要经历四次user mode和kernel mode之间的上下文切换，甚至连数据都被拷贝了四次&lt;/p&gt;

&lt;p&gt;read() 引入了一次从user mode到kernel mode的上下文切换。实际上调用了sys_read() 来从文件中读取data。第一次copy由DMA完成，将文件内容从disk读出，存储在kernel的buffer中。
然后data被copy到user buffer中，此时read()成功返回。这是触发了第二次context switch: 从kernel到user。至此，数据存储在user的buffer中。
send() socket call 带来了第三次context switch，这次是从user mode到kernel mode。同时，也发生了第三次copy：把data放到了kernel adress space中。当然，这次的kernel buffer和第一步的buffer是不同的两个buffer。
最终 send() system call 返回了，同时也造成了第四次context switch。同时第四次copy发生，DMA将data从kernel buffer拷贝到protocol engine中。第四次copy是独立而且异步的。
使用kernel buffer做中介(而不是直接把data传到user buffer中)看起来比较低效(多了一次copy)。然而实际上kernel buffer是用来提高性能的。在进行读操作的时候，kernel buffer起到了预读cache的作用。当写请求的data size比kernel buffer的size小的时候，这能够显著的提升性能。在进行写操作时，kernel buffer的存在可以使得写请求完全异步。&lt;/p&gt;

&lt;p&gt;悲剧的是，当请求的data size远大于kernel buffer size的时候，这个方法本身变成了性能的瓶颈。因为data需要在disk，kernel buffer，user buffer之间拷贝很多次(每次写满整个buffer)。&lt;/p&gt;

&lt;p&gt;而Zero copy正是通过消除这些多余的data copy来提升性能。&lt;/p&gt;

&lt;p&gt;Data Transfer：The Zero Copy Approach&lt;/p&gt;

&lt;p&gt;如果重新检查一遍traditional approach，你会注意到实际上第二次和第三次copy是毫无意义的。应用程序仅仅缓存了一下data就原封不动的把它发回给socket buffer。实际上，data应该直接在read buffer和socket buffer之间传输。transferTo()方法正是做了这样的操作。Listing 2是transferTo()的函数原型：&lt;/p&gt;

&lt;p&gt;public void transferTo(long position, long count, WritableByteChannel target);
transferTo()方法把data从file channel传输到指定的writable byte channel。它需要底层的操作系统支持zero copy。在UNIX和各种Linux中，会执行List 3中的系统调用sendfile()，该命令把data从一个文件描述符传输到另一个文件描述符(Linux中万物皆文件)：&lt;/p&gt;

&lt;p&gt;#include &amp;lt;sys/socket.h&amp;gt;
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
在List 1中的file.read()和socket.send()可以用一句transferTo()替代，如List 4：&lt;/p&gt;

&lt;p&gt;transferTo(position, count, writableChannel);&lt;/p&gt;

&lt;p&gt;在像Listing 4那样使用transferTo()之后，整个过程如下：&lt;/p&gt;

&lt;p&gt;transferTo()方法使得文件内容被DMA engine直接copy到一个read buffer中。然后数据被kernel再次拷贝到和output socket相关联的那个kernel buffer中去。
第三次拷贝由DMA engine完成，它把kernel buffer中的data拷贝到protocol engine中。
这是一个很明显的进步：我们把context switch的次数从4次减少到了2次，同时也把data copy的次数从4次降低到了3次(而且其中只有一次占用了CPU，另外两次由DMA完成)。但是，要做到zero copy，这还差得远。如果网卡支持 gather operation，我们可以通过kernel进一步减少数据的拷贝操作。在2.4及以上版本的linux内核中，开发者修改了socket buffer descriptor来适应这一需求。这个方法不仅减少了context switch，还消除了和CPU有关的数据拷贝。user层面的使用方法没有变，但是内部原理却发生了变化：&lt;/p&gt;

&lt;p&gt;transferTo()方法使得文件内容被copy到了kernel buffer，这一动作由DMA engine完成。
没有data被copy到socket buffer。取而代之的是socket buffer被追加了一些descriptor的信息，包括data的位置和长度。然后DMA engine直接把data从kernel buffer传输到protocol engine，这样就消除了唯一的一次需要占用CPU的拷贝操作。
Figure 5描述了新的transferTo()方法中的data copy:&lt;/p&gt;

&lt;p&gt;Kafka在提高效率方面做了很大努力。Kafka的一个主要使用场景是处理网站活动日志，吞吐量是非常大的，每个页面都会产生好多次写操作。读方面，假设每个消息只被消费一次，读的量的也是很大的，Kafka也尽量使读的操作更轻量化。&lt;/p&gt;

&lt;p&gt;我们之前讨论了磁盘的性能问题，线性读写的情况下影响磁盘性能问题大约有两个方面：太多的琐碎的I/O操作和太多的字节拷贝。I/O问题发生在客户端和服务端之间，也发生在服务端内部的持久化的操作中。
消息集（message set）
为了避免这些问题，Kafka建立了“消息集（message set）”的概念，将消息组织到一起，作为处理的单位。以消息集为单位处理消息，比以单个的消息为单位处理，会提升不少性能。Producer把消息集一块发送给服务端，而不是一条条的发送；服务端把消息集一次性的追加到日志文件中，这样减少了琐碎的I/O操作。consumer也可以一次性的请求一个消息集。
另外一个性能优化是在字节拷贝方面。在低负载的情况下这不是问题，但是在高负载的情况下它的影响还是很大的。为了避免这个问题，Kafka使用了标准的二进制消息格式，这个格式可以在producer,broker和producer之间共享而无需做任何改动。
zero copy
Broker维护的消息日志仅仅是一些目录文件，消息集以固定队的格式写入到日志文件中，这个格式producer和consumer是共享的，这使得Kafka可以一个很重要的点进行优化：消息在网络上的传递。现代的unix操作系统提供了高性能的将数据从页面缓存发送到socket的系统函数，在linux中，这个函数是sendfile.
为了更好的理解sendfile的好处，我们先来看下一般将数据从文件发送到socket的数据流向：&lt;/p&gt;

&lt;p&gt;操作系统把数据从文件拷贝内核中的页缓存中
应用程序从页缓存从把数据拷贝自己的内存缓存中
应用程序将数据写入到内核中socket缓存中
操作系统把数据从socket缓存中拷贝到网卡接口缓存，从这里发送到网络上。&lt;/p&gt;

&lt;p&gt;这显然是低效率的，有4次拷贝和2次系统调用。Sendfile通过直接将数据从页面缓存发送网卡接口缓存，避免了重复拷贝，大大的优化了性能。
在一个多consumers的场景里，数据仅仅被拷贝到页面缓存一次而不是每次消费消息的时候都重复的进行拷贝。这使得消息以近乎网络带宽的速率发送出去。这样在磁盘层面你几乎看不到任何的读操作，因为数据都是从页面缓存中直接发送到网络上去了。&lt;/p&gt;

</description>
        <pubDate>Fri, 26 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2018/01/26/zero_copy.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2018/01/26/zero_copy.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>进程在后台运行原理</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;nohup/setsid/&amp;amp;
场景：
如果只是临时有一个命令需要长时间运行，什么方法能最简便的保证它在后台稳定运行呢？&lt;/p&gt;

&lt;p&gt;hangup 名称的来由
在 Unix 的早期版本中，每个终端都会通过 modem 和系统通讯。当用户 logout 时，modem 就会挂断（hang up）电话。 同理，当 modem 断开连接时，就会给终端发送 hangup 信号来通知其关闭所有子进程。&lt;/p&gt;

&lt;p&gt;解决方法：
我们知道，当用户注销（logout）或者网络断开时，终端会收到 HUP（hangup）信号从而关闭其所有子进程。因此，我们的解决办法就有两种途径：要么让进程忽略 HUP 信号，要么让进程运行在新的会话里从而成为不属于此终端的子进程。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;nohup&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;nohup 无疑是我们首先想到的办法。顾名思义，nohup 的用途就是让提交的命令忽略 hangup 信号。让我们先来看一下 nohup 的帮助信息：
NOHUP(1)                        User Commands                        NOHUP(1)&lt;/p&gt;

&lt;p&gt;NAME
       nohup - run a command immune to hangups, with output to a non-tty&lt;/p&gt;

&lt;p&gt;SYNOPSIS
       nohup COMMAND [ARG]…
       nohup OPTION&lt;/p&gt;

&lt;p&gt;DESCRIPTION
       Run COMMAND, ignoring hangup signals.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   --help display this help and exit
 
   --version
          output version information and exit 可见，nohup 的使用是十分方便的，只需在要处理的命令前加上 nohup 即可，标准输出和标准错误缺省会被重定向到 nohup.out 文件中。一般我们可在结尾加上&quot;&amp;amp;&quot;来将命令同时放入后台运行，也可用&quot;&amp;gt;filename 2&amp;gt;&amp;amp;1&quot;来更改缺省的重定向文件名。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;nohup 示例
[root@pvcent107 ~]# nohup ping www.ibm.com &amp;amp;
[1] 3059
nohup: appending output to `nohup.out’
[root@pvcent107 ~]# ps -ef |grep 3059
root      3059   984  0 21:06 pts/3    00:00:00 ping www.ibm.com
root      3067   984  0 21:06 pts/3    00:00:00 grep 3059
[root@pvcent107 ~]#
2。setsid&lt;/p&gt;

&lt;p&gt;nohup 无疑能通过忽略 HUP 信号来使我们的进程避免中途被中断，但如果我们换个角度思考，如果我们的进程不属于接受 HUP 信号的终端的子进程，那么自然也就不会受到 HUP 信号的影响了。setsid 就能帮助我们做到这一点。让我们先来看一下 setsid 的帮助信息：
SETSID(8)                 Linux Programmer’s Manual                 SETSID(8)&lt;/p&gt;

&lt;p&gt;NAME
       setsid - run a program in a new session&lt;/p&gt;

&lt;p&gt;SYNOPSIS
       setsid program [ arg … ]&lt;/p&gt;

&lt;p&gt;DESCRIPTION
       setsid runs a program in a new session.
可见 setsid 的使用也是非常方便的，也只需在要处理的命令前加上 setsid 即可。&lt;/p&gt;

&lt;p&gt;setsid 示例
[root@pvcent107 ~]# setsid ping www.ibm.com
[root@pvcent107 ~]# ps -ef |grep www.ibm.com
root     31094     1  0 07:28 ?        00:00:00 ping www.ibm.com
root     31102 29217  0 07:29 pts/4    00:00:00 grep www.ibm.com
[root@pvcent107 ~]#
值得注意的是，上例中我们的进程 ID(PID)为31094，而它的父 ID（PPID）为1（即为 init 进程 ID），并不是当前终端的进程 ID。请将此例与nohup 例中的父 ID 做比较。&lt;/p&gt;

&lt;p&gt;3。&amp;amp;&lt;/p&gt;

&lt;p&gt;这里还有一个关于 subshell 的小技巧。我们知道，将一个或多个命名包含在“()”中就能让这些命令在子 shell 中运行中，从而扩展出很多有趣的功能，我们现在要讨论的就是其中之一。&lt;/p&gt;

&lt;p&gt;当我们将”&amp;amp;”也放入“()”内之后，我们就会发现所提交的作业并不在作业列表中，也就是说，是无法通过jobs来查看的。让我们来看看为什么这样就能躲过 HUP 信号的影响吧。&lt;/p&gt;

&lt;p&gt;subshell 示例
[root@pvcent107 ~]# (ping www.ibm.com &amp;amp;)
[root@pvcent107 ~]# ps -ef |grep www.ibm.com
root     16270     1  0 14:13 pts/4    00:00:00 ping www.ibm.com
root     16278 15362  0 14:13 pts/4    00:00:00 grep www.ibm.com
[root@pvcent107 ~]#
从上例中可以看出，新提交的进程的父 ID（PPID）为1（init 进程的 PID），并不是当前终端的进程 ID。因此并不属于当前终端的子进程，从而也就不会受到当前终端的 HUP 信号的影响了。&lt;/p&gt;

&lt;p&gt;disown
场景：
我们已经知道，如果事先在命令前加上 nohup 或者 setsid 就可以避免 HUP 信号的影响。但是如果我们未加任何处理就已经提交了命令，该如何补救才能让它避免 HUP 信号的影响呢？&lt;/p&gt;

&lt;p&gt;解决方法：
这时想加 nohup 或者 setsid 已经为时已晚，只能通过作业调度和 disown 来解决这个问题了。让我们来看一下 disown 的帮助信息：
disown [-ar] [-h] [jobspec …]
    Without options, each jobspec is  removed  from  the  table  of
    active  jobs.   If  the -h option is given, each jobspec is not
    removed from the table, but is marked so  that  SIGHUP  is  not
    sent  to the job if the shell receives a SIGHUP.  If no jobspec
    is present, and neither the -a nor the -r option  is  supplied,
    the  current  job  is  used.  If no jobspec is supplied, the -a
    option means to remove or mark all jobs; the -r option  without
    a  jobspec  argument  restricts operation to running jobs.  The
    return value is 0 unless a jobspec does  not  specify  a  valid
    job.
可以看出，我们可以用如下方式来达成我们的目的。&lt;/p&gt;

&lt;p&gt;灵活运用 CTRL-z
在我们的日常工作中，我们可以用 CTRL-z 来将当前进程挂起到后台暂停运行，执行一些别的操作，然后再用 fg 来将挂起的进程重新放回前台（也可用 bg 来将挂起的进程放在后台）继续运行。这样我们就可以在一个终端内灵活切换运行多个任务，这一点在调试代码时尤为有用。因为将代码编辑器挂起到后台再重新放回时，光标定位仍然停留在上次挂起时的位置，避免了重新定位的麻烦。&lt;/p&gt;

&lt;p&gt;用disown -h jobspec来使某个作业忽略HUP信号。
用disown -ah 来使所有的作业都忽略HUP信号。
用disown -rh 来使正在运行的作业忽略HUP信号。
需要注意的是，当使用过 disown 之后，会将把目标作业从作业列表中移除，我们将不能再使用jobs来查看它，但是依然能够用ps -ef查找到它。&lt;/p&gt;

&lt;p&gt;但是还有一个问题，这种方法的操作对象是作业，如果我们在运行命令时在结尾加了”&amp;amp;”来使它成为一个作业并在后台运行，那么就万事大吉了，我们可以通过jobs命令来得到所有作业的列表。但是如果并没有把当前命令作为作业来运行，如何才能得到它的作业号呢？答案就是用 CTRL-z（按住Ctrl键的同时按住z键）了！&lt;/p&gt;

&lt;p&gt;CTRL-z 的用途就是将当前进程挂起（Suspend），然后我们就可以用jobs命令来查询它的作业号，再用bg jobspec来将它放入后台并继续运行。需要注意的是，如果挂起会影响当前进程的运行结果，请慎用此方法。&lt;/p&gt;

&lt;p&gt;disown 示例1（如果提交命令时已经用“&amp;amp;”将命令放入后台运行，则可以直接使用“disown”）
[root@pvcent107 build]# cp -r testLargeFile largeFile &amp;amp;
[1] 4825
[root@pvcent107 build]# jobs
[1]+  Running                 cp -i -r testLargeFile largeFile &amp;amp;
[root@pvcent107 build]# disown -h %1
[root@pvcent107 build]# ps -ef |grep largeFile
root      4825   968  1 09:46 pts/4    00:00:00 cp -i -r testLargeFile largeFile
root      4853   968  0 09:46 pts/4    00:00:00 grep largeFile
[root@pvcent107 build]# logout
disown 示例2（如果提交命令时未使用“&amp;amp;”将命令放入后台运行，可使用 CTRL-z 和“bg”将其放入后台，再使用“disown”）
[root@pvcent107 build]# cp -r testLargeFile largeFile2&lt;/p&gt;

&lt;p&gt;[1]+  Stopped                 cp -i -r testLargeFile largeFile2
[root@pvcent107 build]# bg %1
[1]+ cp -i -r testLargeFile largeFile2 &amp;amp;
[root@pvcent107 build]# jobs
[1]+  Running                 cp -i -r testLargeFile largeFile2 &amp;amp;
[root@pvcent107 build]# disown -h %1
[root@pvcent107 build]# ps -ef |grep largeFile2
root      5790  5577  1 10:04 pts/3    00:00:00 cp -i -r testLargeFile largeFile2
root      5824  5577  0 10:05 pts/3    00:00:00 grep largeFile2
[root@pvcent107 build]#
screen
场景：
我们已经知道了如何让进程免受 HUP 信号的影响，但是如果有大量这种命令需要在稳定的后台里运行，如何避免对每条命令都做这样的操作呢？&lt;/p&gt;

&lt;p&gt;解决方法：
此时最方便的方法就是 screen 了。简单的说，screen 提供了 ANSI/VT100 的终端模拟器，使它能够在一个真实终端下运行多个全屏的伪终端。screen 的参数很多，具有很强大的功能，我们在此仅介绍其常用功能以及简要分析一下为什么使用 screen 能够避免 HUP 信号的影响。我们先看一下 screen 的帮助信息：
SCREEN(1)                                                           SCREEN(1)&lt;/p&gt;

&lt;p&gt;NAME
       screen - screen manager with VT100/ANSI terminal emulation&lt;/p&gt;

&lt;p&gt;SYNOPSIS
       screen [ -options ] [ cmd [ args ] ]
       screen -r [[pid.]tty[.host]]
       screen -r sessionowner/[[pid.]tty[.host]]&lt;/p&gt;

&lt;p&gt;DESCRIPTION
       Screen  is  a  full-screen  window manager that multiplexes a physical
       terminal between several  processes  (typically  interactive  shells).
       Each  virtual  terminal provides the functions of a DEC VT100 terminal
       and, in addition, several control functions from the  ISO  6429  (ECMA
       48,  ANSI  X3.64)  and ISO 2022 standards (e.g. insert/delete line and
       support for multiple character sets).  There is a  scrollback  history
       buffer  for  each virtual terminal and a copy-and-paste mechanism that
       allows moving text regions between windows.
使用 screen 很方便，有以下几个常用选项：&lt;/p&gt;

&lt;p&gt;用screen -dmS session name来建立一个处于断开模式下的会话（并指定其会话名）。
用screen -list 来列出所有会话。
用screen -r session name来重新连接指定会话。
用快捷键CTRL-a d 来暂时断开当前会话。
screen 示例
[root@pvcent107 ~]# screen -dmS Urumchi
[root@pvcent107 ~]# screen -list
There is a screen on:
        12842.Urumchi   (Detached)
1 Socket in /tmp/screens/S-root.&lt;/p&gt;

&lt;p&gt;[root@pvcent107 ~]# screen -r Urumchi
当我们用“-r”连接到 screen 会话后，我们就可以在这个伪终端里面为所欲为，再也不用担心 HUP 信号会对我们的进程造成影响，也不用给每个命令前都加上“nohup”或者“setsid”了。这是为什么呢？让我来看一下下面两个例子吧。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;未使用 screen 时新进程的进程树
[root@pvcent107 ~]# ping www.google.com &amp;amp;
[1] 9499
[root@pvcent107 ~]# pstree -H 9499
init─┬─Xvnc
  ├─acpid
  ├─atd
  ├─2*[sendmail] 
  ├─sshd─┬─sshd───bash───pstree
  │       └─sshd───bash───ping
我们可以看出，未使用 screen 时我们所处的 bash 是 sshd 的子进程，当 ssh 断开连接时，HUP 信号自然会影响到它下面的所有子进程（包括我们新建立的 ping 进程）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用了 screen 后新进程的进程树
[root@pvcent107 ~]# screen -r Urumchi
[root@pvcent107 ~]# ping www.ibm.com &amp;amp;
[1] 9488
[root@pvcent107 ~]# pstree -H 9488
init─┬─Xvnc
  ├─acpid
  ├─atd
  ├─screen───bash───ping
  ├─2*[sendmail]
而使用了 screen 后就不同了，此时 bash 是 screen 的子进程，而 screen 是 init（PID为1）的子进程。那么当 ssh 断开连接时，HUP 信号自然不会影响到 screen 下面的子进程了。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;生产环境下，除了我们ssh登录上去，然后手动跑的那部分以外，其他都是自动运行的，这些大部分都应该是后台执行的。如何才能后台执行呢？&lt;/p&gt;

&lt;p&gt;nohup ./XXX &amp;amp;
由系统的其他daemon进程启动。这样的话，你的程序是它的子进程，跟终端没关系。退出终端也不会导致进程退出。如写在crontab里。
写成daemon程序，可以手动执行，退出终端时程序不退出。
如何选择呢？ 
（1）首先，清理过期日志这一类需求，可以写一个死循环一直运行，也可以写在crontab里，每次执行完就退出（如果每分钟一次可以满足的话）； 
（2）crontab的需要接受最多1分钟的时延，如果实时性要求更高一些，那么就需要考虑写个死循环了，这个程序可以由crontab来start和restart，只有在挂了重启时才会出现1分钟时延； 
（3）服务不能中断的（nginx、redis、apache，所有在线服务），一般都是daemon程序。但理论上用（2）似乎也可以；当然这两者细节上有很多区别。&lt;/p&gt;

&lt;p&gt;是nohup的关键代码了，首先向系统注册一个SIGHUP信号，处理方式是忽略，系统默认的方式kill进程，接着execvp函数执行参数后面的进程。&lt;/p&gt;

&lt;p&gt;为什么这样可以做到把代码放到后台运行呢，不让进程不因SIGHUP信号而终止呢，得弄清楚什么时候系统会向进程发送SINHUP信号呢，其实在posix标准里，linux也遵守这个标准，当用户在连接上linux终端后，当当前会话终端中断时，系统会向当前会话的前台进程组发送一个SIGHUP信号，系统默认对这个信号的处理方式是KILL该进程，如果进程忽视这个信号，没有被kill的话，接着系统会再次向该进程发送一个SIGCONT信号，nohup的作用就是让程序忽视SIGHUP信号，避免终端连接中断时，进程被kill，同时改变进程标准输入输出。&lt;/p&gt;

&lt;p&gt;其实有没有必要一定用nohup方式去启动后台进程呢，我觉得没有必要，当一个会话的终端中断后，系统只会给当前会话的前台进程组发送SIGHUP信号，如果启动进程时，让进程在后台去执行，也是执行的时候后面加一个&amp;amp;，也可以自己去重定向输入输出，会话的终端中断时，后台的进程组是不会收到SIGHUP信号，也就不会被系统kill掉，当然也可以在代码里注册SIGHUP该信号，防止进程被kill。
你可能会遇到nohup命令问题，这里将介绍nohup命令问题的解决方法,Linux本身是这个操作系统的核心部分，也就是操作系统的内核。内核 是完成那些最基本操作的程序，它负责其他程序（如文本编辑器程序）的启动与终止、内存申请处理硬盘访问、网络连接管理等方面的工作。Unix/Linux 下一般想让某个程序在后台运行，很多都是使用 &amp;amp; 在程序结尾来让程序自动运行。&lt;/p&gt;

&lt;p&gt;比如我们要运行mysql在后台：/usr/local/mysql/bin/mysqld_safe –user=mysql &amp;amp;但是我们很多程序并不象mysqld一样可以做成守护进程，可能我们的程序只是普通程序而已，一般这种程序即使使用 &amp;amp; 结尾，如果终端关闭，那么程序也会被关闭。为了能够后台运行，我们需要使用nohup命令，比如我们有个start.sh需要在后台运行，并且希望在后台 能够一直运行，那么就使用nohup：&lt;/p&gt;

&lt;p&gt;nohup /root/start.sh &amp;amp;&lt;/p&gt;

&lt;p&gt;在shell中回车后提示：
[~]$ appending output to nohup.out原程序的的标准输出被自动改向到当前目录下的nohup.out文件，起到了log的作用。但是有时候在这一步会有问题，当把终端关闭 后，进程会自动被关闭，察看nohup.out可以看到在关闭终端瞬间服务自动关闭。咨询红旗Linux工程师后，他也不得其解，在我的终端上执行后，他 启动的进程竟然在关闭终端后依然运行。&lt;/p&gt;

&lt;p&gt;在第二遍给我演示时，我才发现我和他操作终端时的一个细节不同：他是在当shell中提示了nohup成功后还需要按终端上键盘任意键退回到 shell输入命令窗口，然后通过在shell中输入exit来退出终端；而我是每次在nohup执行成功后直接点关闭程序按钮关闭终端.。所以这时候会 断掉该命令所对应的session，导致nohup对应的进程被通知需要一起shutdown。这个细节有人和我一样没注意到，所以在这儿记录一下了。&lt;/p&gt;

&lt;p&gt;附：nohup命令参考nohup命令
用途：不挂断地运行命令。
语法：nohup Command [ Arg … ] [　&amp;amp; ]
描 述：nohup命令运行由 Command 参数和任何相关的 Arg 参数指定的命令，忽略所有挂断（SIGHUP）信号。在注销后使用 nohup 命令运行后台中的程序。要运行后台中的 nohup命令，添加 &amp;amp; （ 表示”and”的符号）到命令的尾部。&lt;/p&gt;

&lt;p&gt;无论是否将 nohup命令的输出重定向到终端，输出都将附加到当前目录的 nohup.out 文件中。如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME/nohup.out 文件中。如果没有文件能创建或打开以用于追加，那么 Command 参数指定的命令不可调用。如果标准错误是一个终端，那么把指定的命令写给标准错误的所有输出作为标准输出重定向到相同的文件描述符。&lt;/p&gt;

&lt;p&gt;退出状态：该命令返回下列出口值：126 可以查找但不能调用 Command 参数指定的命令。127 nohup 命令发生错误或不能查找由 Command 参数指定的命令。否则，nohup命令的退出状态是 Command 参数指定nohup命令的退出状态。&lt;/p&gt;

&lt;p&gt;nohup命令及其输出文件&lt;/p&gt;

&lt;p&gt;nohup命令：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以使用nohup命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思( n ohang up)。&lt;/p&gt;

&lt;p&gt;该命令的一般形式为：nohup command &amp;amp;使用nohup命令提交作业
如果使用nohup命令提交作业，那么在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中，除非另外指定了输出文件：
nohup command &amp;gt; myout.file 2&amp;gt;&amp;amp;1 &amp;amp;在上面的例子中，输出被重定向到myout.file文件中。使用 jobs 查看任务。使用 fg %n　关闭。&lt;/p&gt;

&lt;p&gt;另外有两个常用的ftp工具ncftpget和ncftpput，可以实现后台的ftp上传和下载，这样就可以利用这些命令在后台上传和下载文件了。&lt;/p&gt;

&lt;p&gt;unix中进程组织结构为session包含一个前台进程组及一个或多个后台进程组，一个进程组包含多个进程。&lt;/p&gt;

&lt;p&gt;一个session可能会有一个session首进程，而一个session首进程可能会有一个控制终端。&lt;/p&gt;

&lt;p&gt;一个进程组可能会有一个进程组首进程。&lt;/p&gt;

&lt;p&gt;进程组首进程的进程与该进程组ID相等。&lt;/p&gt;

&lt;p&gt;这儿是可能会有，在一定情况之下是没有的。&lt;/p&gt;

&lt;p&gt;与终端交互的进程是前台进程，否则便是后台进程。&lt;/p&gt;

&lt;p&gt;SIGHUP会在以下3种情况下被发送给相应的进程：&lt;/p&gt;

&lt;p&gt;1、终端关闭时，该信号被发送到首进程以及作为job提交的进程（即用 &amp;amp; 符号提交的进程）&lt;/p&gt;

&lt;p&gt;2、session首进程退出时，该信号被发送到该session中的前台进程组中的每一个进程&lt;/p&gt;

&lt;p&gt;3、若父进程退出导致进程组成为孤儿进程组，且该进程组中有进程处于停止状态（收到SIGSTOP或SIGTSTP信号），该信号会被发送到该进程组中的每一个进程。&lt;/p&gt;

&lt;p&gt;系统对信号的默认处理是终止收到该信号的进程。&lt;/p&gt;

&lt;p&gt;所以若程序中没有捕捉该信号，当收到该信号时，进程就会退出。&lt;/p&gt;

&lt;p&gt;下面观察几种因终端关闭导致进程退出的情况，在这儿进程退出是因为收到了SIGHUP信号。&lt;/p&gt;

&lt;p&gt;login shell是session首进程。&lt;/p&gt;

&lt;p&gt;编译后的执行文件为sigtest&lt;/p&gt;

&lt;p&gt;1、命令： sigtest front &amp;gt; tt.txt&lt;/p&gt;

&lt;p&gt;操作： 关闭终端&lt;/p&gt;

&lt;p&gt;结果： tt文件的内容为front: sighup received&lt;/p&gt;

&lt;p&gt;原因：&lt;/p&gt;

&lt;p&gt;sigtest是前台进程，终端关闭后，根据上面提到的第1种情况， loginshell作为session首进程，会收到SIGHUP信号然后退出，&lt;/p&gt;

&lt;p&gt;根据第2种情况，sigtest作为前台进程，会收到login shell发出的SIGHUP信号。&lt;/p&gt;

&lt;p&gt;2、命令：&lt;/p&gt;

&lt;p&gt;sigtest back &amp;gt; tt.txt &amp;amp;&lt;/p&gt;

&lt;p&gt;操作：&lt;/p&gt;

&lt;p&gt;关闭终端&lt;/p&gt;

&lt;p&gt;结果：&lt;/p&gt;

&lt;p&gt;tt文件的内容为back: sighup received&lt;/p&gt;

&lt;p&gt;原因：&lt;/p&gt;

&lt;p&gt;sigtest是提交的job，根据上面提到的第1种情况，sigtest会收到SIGHUP信号&lt;/p&gt;

&lt;p&gt;3、写一个shell，内容为&lt;/p&gt;

&lt;p&gt;[cpp] view plain copy
sigtest &amp;amp;&lt;br /&gt;
执行该shell&lt;/p&gt;

&lt;p&gt;操作： 关闭终端&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;结果： ps -ef&lt;/td&gt;
      &lt;td&gt;grep sigtest会看到该进程还在，tt文件为空&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;原因：&lt;/p&gt;

&lt;p&gt;执行该shell时，sigtest作为job提交，然后该shell退出，致使sigtest变成了孤儿进程，不再是当前session的job了，&lt;/p&gt;

&lt;p&gt;因此sigtest即不是session首进程也不是job，不会收到SIGHUP&lt;/p&gt;

&lt;p&gt;同时孤儿进程属于后台进程，因此loginshell退出后不会发送SIGHUP给sigtest，因为它只将该信号发送给前台进程。&lt;/p&gt;

&lt;p&gt;第3条说过若进程组变成孤儿进程组的时候，若有进程处于停止状态，也会收到SIGHUP信号，但sigtest没有处于停止状态，所以不会收到SIGHUP信号&lt;/p&gt;

&lt;p&gt;4、nohup sigtest &amp;gt; tt&lt;/p&gt;

&lt;p&gt;操作：&lt;/p&gt;

&lt;p&gt;关闭终端&lt;/p&gt;

&lt;p&gt;结果： tt文件为空&lt;/p&gt;

&lt;p&gt;原因：nohup可以防止进程收到SIGHUP信号&lt;/p&gt;

&lt;p&gt;至此，我们就清楚了何种情况下终端关闭后进程会退出，何种情况下不会退出。&lt;/p&gt;

&lt;p&gt;要想终端关闭后进程不退出有以下几种方法，均为通过shell的方式：&lt;/p&gt;

&lt;p&gt;1、 编写shell，内容如下&lt;/p&gt;

&lt;p&gt;[cpp] view plain copy
trap “” SIGHUP  #该句的作用是屏蔽SIGHUP信号，trap可以屏蔽很多信号&lt;/p&gt;

&lt;p&gt;sigtest&lt;/p&gt;

&lt;p&gt;2、nohup sigtest可以直接在命令行执行，&lt;/p&gt;

&lt;p&gt;若想做完该操作后继续别的操作， 可以执行&lt;/p&gt;

&lt;p&gt;[cpp] view plain copy
nohup sigtest &amp;amp;&lt;/p&gt;

&lt;p&gt;3、 编写shell，内容如下&lt;/p&gt;

&lt;p&gt;[cpp] view plain copy
sigtest &amp;amp;&lt;/p&gt;

&lt;p&gt;其实任何将进程变为孤儿进程的方式都可以，包括fork后父进程马上退出&lt;/p&gt;

</description>
        <pubDate>Wed, 24 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2018/01/24/nohup.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2018/01/24/nohup.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>mongodb</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;一、概念：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  分片（sharding）是指将数据库拆分，将其分散在不同的机器上的过程。将数据分散到不同的机器上，不需要功能强大的服务器就可以存储更多的数据和处理更大的负载。基本思想就是将集合切成小块，这些块分散到若干片里，每个片只负责总数据的一部分，最后通过一个均衡器来对各个分片进行均衡（数据迁移）。通过一个名为mongos的路由进程进行操作，mongos知道数据和片的对应关系（通过配置服务器）。大部分使用场景都是解决磁盘空间的问题，对于写入有可能会变差（+++里面的说明+++），查询则尽量避免跨分片查询。使用分片的时机：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1，机器的磁盘不够用了。使用分片解决磁盘空间的问题。
2，单个mongod已经不能满足写数据的性能要求。通过分片让写压力分散到各个分片上面，使用分片服务器自身的资源。
3，想把大量数据放到内存里提高性能。和上面一样，通过分片使用分片服务器自身的资源。
二、部署安装： 前提是安装了mongodb（本文用3.0测试）&lt;/p&gt;

&lt;p&gt;在搭建分片之前，先了解下分片中各个角色的作用。&lt;/p&gt;

&lt;p&gt;① 配置服务器。是一个独立的mongod进程，保存集群和分片的元数据，即各分片包含了哪些数据的信息。最先开始建立，启用日志功能。像启动普通的mongod一样启动配置服务器，指定configsvr选项。不需要太多的空间和资源，配置服务器的1KB空间相当于真是数据的200MB。保存的只是数据的分布表。当服务不可用，则变成只读，无法分块、迁移数据。
② 路由服务器。即mongos，起到一个路由的功能，供程序连接。本身不保存数据，在启动时从配置服务器加载集群信息，开启mongos进程需要知道配置服务器的地址，指定configdb选项。
③ 分片服务器。是一个独立普通的mongod进程，保存数据信息。可以是一个副本集也可以是单独的一台服务器。
部署环境：3台机子&lt;/p&gt;

&lt;p&gt;A：配置(3)、路由1、分片1；&lt;/p&gt;

&lt;p&gt;B：分片2，路由2；&lt;/p&gt;

&lt;p&gt;C：分片3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  在部署之前先明白片键的意义，一个好的片键对分片至关重要。片键必须是一个索引，数据根据这个片键进行拆分分散。通过sh.shardCollection加会自动创建索引。一个自增的片键对写入和数据均匀分布就不是很好，因为自增的片键总会在一个分片上写入，后续达到某个阀值可能会写到别的分片。但是按照片键查询会非常高效。随机片键对数据的均匀分布效果很好。注意尽量避免在多个分片上进行查询。在所有分片上查询，mongos会对结果进行归并排序。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动上面这些服务，因为在后台运行，所以用配置文件启动，配置文件说明。&lt;/p&gt;

&lt;p&gt;1）配置服务器的启动。(A上开启3个，Port：20000、21000、22000)&lt;/p&gt;

&lt;p&gt;配置服务器是一个普通的mongod进程，所以只需要新开一个实例即可。配置服务器必须开启1个或则3个，开启2个则会报错：&lt;/p&gt;

&lt;p&gt;BadValue need either 1 or 3 configdbs
因为要放到后台用用配置文件启动，需要修改配置文件：&lt;/p&gt;

&lt;p&gt;/etc/mongod_20000.conf&lt;/p&gt;

&lt;p&gt;复制代码
#数据目录
dbpath=/usr/local/config/
#日志文件
logpath=/var/log/mongodb/mongodb_config.log
#日志追加
logappend=true
#端口
port = 20000
#最大连接数
maxConns = 50
pidfilepath = /var/run/mongo_20000.pid
#日志,redo log
journal = true
#刷写提交机制
journalCommitInterval = 200
#守护进程模式
fork = true
#刷写数据到日志的频率
syncdelay = 60
#storageEngine = wiredTiger
#操作日志,单位M
oplogSize = 1000
#命名空间的文件大小,默认16M，最大2G。
nssize = 16
noauth = true
unixSocketPrefix = /tmp
configsvr = true
复制代码
/etc/mongod_21000.conf&lt;/p&gt;

&lt;p&gt;复制代码
数据目录
dbpath=/usr/local/config1/
#日志文件
logpath=/var/log/mongodb/mongodb_config1.log
#日志追加
logappend=true
#端口
port = 21000
#最大连接数
maxConns = 50
pidfilepath = /var/run/mongo_21000.pid
#日志,redo log
journal = true
#刷写提交机制
journalCommitInterval = 200
#守护进程模式
fork = true
#刷写数据到日志的频率
syncdelay = 60
#storageEngine = wiredTiger
#操作日志,单位M
oplogSize = 1000
#命名空间的文件大小,默认16M，最大2G。
nssize = 16
noauth = true
unixSocketPrefix = /tmp
configsvr = true
复制代码
开启配置服务器：&lt;/p&gt;

&lt;p&gt;复制代码
root@mongo1:~# mongod -f /etc/mongod_20000.conf 
about to fork child process, waiting until server is ready for connections.
forked process: 8545
child process started successfully, parent exiting&lt;/p&gt;

&lt;p&gt;root@mongo1:~# mongod -f /etc/mongod_21000.conf 
about to fork child process, waiting until server is ready for connections.
forked process: 8595
child process started successfully, parent exiting
复制代码
同理再起一个22000端口的配置服务器。&lt;/p&gt;

&lt;p&gt;View Code
2）路由服务器的启动。(A、B上各开启1个，Port：30000)&lt;/p&gt;

&lt;p&gt;路由服务器不保存数据，把日志记录一下即可。&lt;/p&gt;

&lt;p&gt;复制代码&lt;/p&gt;
&lt;h1 id=&quot;mongos&quot;&gt;mongos&lt;/h1&gt;

&lt;p&gt;#日志文件
logpath=/var/log/mongodb/mongodb_route.log
#日志追加
logappend=true
#端口
port = 30000
#最大连接数
maxConns = 100
#绑定地址
#bind_ip=192.168.200.*,…,&lt;/p&gt;

&lt;p&gt;pidfilepath = /var/run/mongo_30000.pid&lt;/p&gt;

&lt;p&gt;configdb=192.168.200.A:20000,192.168.200.A:21000,192.168.200.A:22000  #必须是1个或则3个配置 。
#configdb=127.0.0.1:20000  #报错
#守护进程模式 fork = true
复制代码
其中最重要的参数是configdb，不能在其后面带的配置服务器的地址写成localhost或则127.0.0.1，需要设置成其他分片也能访问的地址，即192.168.200.A:20000/21000/22000。否则在addshard的时候会报错：&lt;/p&gt;

&lt;p&gt;{
“ok” : 0,
“errmsg” : “can’t use localhost as a shard since all shards need to communicate. either use all shards and configdbs in localhost or all in actual IPs  host: 172.16.5.104:20000 isLocalHost:0”
}
开启mongos：&lt;/p&gt;

&lt;p&gt;root@mongo1:~# mongos -f /etc/mongod_30000.conf 
2015-07-10T14:42:58.741+0800 W SHARDING running with 1 config server should be done only for testing purposes and is not recommended for production
about to fork child process, waiting until server is ready for connections.
forked process: 8965
child process started successfully, parent exiting
3）分片服务器的启动：&lt;/p&gt;

&lt;p&gt;就是一个普通的mongod进程：&lt;/p&gt;

&lt;p&gt;root@mongo1:~# mongod -f /etc/mongod_40000.conf 
note: noprealloc may hurt performance in many applications
about to fork child process, waiting until server is ready for connections.
forked process: 9020
child process started successfully, parent exiting
A服务器上面的服务开启完毕&lt;/p&gt;

&lt;p&gt;root@mongo1:~# ps -ef | grep mongo
root      9020     1  0 14:47 ?        00:00:06 mongod -f /etc/mongod_40000.conf
root      9990     1  0 15:14 ?        00:00:02 mongod -f /etc/mongod_20000.conf
root     10004     1  0 15:14 ?        00:00:01 mongod -f /etc/mongod_21000.conf
root     10076     1  0 15:20 ?        00:00:00 mongod -f /etc/mongod_22000.conf
root     10096     1  0 15:20 ?        00:00:00 mongos -f /etc/mongod_30000.conf
按照上面的方法再到B上开启分片服务和路由服务(配置文件一样)，以及在C上开启分片服务。到此分片的配置服务器、路由服务器、分片服务器都已经部署完成。&lt;/p&gt;

&lt;p&gt;三、配置分片：下面的操作都是在mongodb的命令行里执行&lt;/p&gt;

&lt;p&gt;1）添加分片：sh.addShard(“IP:Port”)&lt;/p&gt;

&lt;p&gt;登陆路由服务器mongos 操作：&lt;/p&gt;

&lt;p&gt;root@mongo1:~# mongo –port=30000
MongoDB shell version: 3.0.4
connecting to: 127.0.0.1:30000/test
mongos&amp;gt; 
添加分片：&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; sh.status()    #查看集群的信息
— Sharding Status — 
  sharding version: {
    “_id” : 1,
    “minCompatibleVersion” : 5,
    “currentVersion” : 6,
    “clusterId” : ObjectId(“559f72470f93270ba60b26c6”)
}
  shards:
  balancer:
    Currently enabled:  yes
    Currently running:  no
    Failed balancer rounds in last 5 attempts:  0
    Migration Results for the last 24 hours: 
        No recent migrations
  databases:
    {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; sh.addShard(“192.168.200.A:40000”) #添加分片
{ “shardAdded” : “shard0000”, “ok” : 1 }
mongos&amp;gt; sh.addShard(“192.168.200.B:40000”) #添加分片
{ “shardAdded” : “shard0001”, “ok” : 1 }
mongos&amp;gt; sh.addShard(“192.168.200.C:40000”) #添加分片
{ “shardAdded” : “shard0002”, “ok” : 1 }&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; sh.status()    #查看集群信息
— Sharding Status — 
  sharding version: {
    “_id” : 1,
    “minCompatibleVersion” : 5,
    “currentVersion” : 6,
    “clusterId” : ObjectId(“559f72470f93270ba60b26c6”)
}
  shards:  #分片信息
    {  “_id” : “shard0000”,  “host” : “192.168.200.A:40000” }
    {  “_id” : “shard0001”,  “host” : “192.168.200.B:40000” }
    {  “_id” : “shard0002”,  “host” : “192.168.200.C:40000” }
  balancer:
    Currently enabled:  yes
    Currently running:  no
    Failed balancer rounds in last 5 attempts:  0
    Migration Results for the last 24 hours: 
        No recent migrations
  databases:
    {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }
复制代码
2）开启分片功能：sh.enableSharding(“库名”)、sh.shardCollection(“库名.集合名”,{“key”:1})&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; sh.enableSharding(“dba”)  #首先对数据库启用分片
{ “ok” : 1 }
mongos&amp;gt; sh.status()               #查看分片信息
— Sharding Status —…
…
  databases:
    {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }
    {  “_id” : “test”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “dba”,  “partitioned” : true,  “primary” : “shard0000” }&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; sh.shardCollection(“dba.account”,{“name”:1})    #再对集合进行分片，name字段是片键。片键的选择：利于分块、分散写请求、查询数据。
{ “collectionsharded” : “dba.account”, “ok” : 1 }
mongos&amp;gt; sh.status()
— Sharding Status —…
  shards:
    {  “_id” : “shard0000”,  “host” : “192.168.200.51:40000” }
    {  “_id” : “shard0001”,  “host” : “192.168.200.52:40000” }
    {  “_id” : “shard0002”,  “host” : “192.168.200.53:40000” }
…
  databases:
    {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }
    {  “_id” : “test”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “dba”,  “partitioned” : true,  “primary” : “shard0000” }  #库
        dba.account
            shard key: { “name” : 1 }                                    #集合
            chunks:
                shard0000    1
            { “name” : { “$minKey” : 1 } } –» { “name” : { “$maxKey” : 1 } } on : shard0000 Timestamp(1, 0) 
复制代码
上面加粗部分表示分片信息已经配置完成。要是出现：&lt;/p&gt;

&lt;p&gt;too many chunks to print, use verbose if you want to force print
想要看到详细的信息则需要执行：&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; sh.status({“verbose”:1})
或则
mongos&amp;gt; db.printShardingStatus(“vvvv”)
或则
mongos&amp;gt; printShardingStatus(db.getSisterDB(“config”),1)
四、测试 ：对dba库的account集合进行测试，随机写入，查看是否分散到3个分片中。&lt;/p&gt;

&lt;p&gt;判断是否为shard：db.runCommand({isdbgrid:1})&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; db.runCommand({isdbgrid:1})
{ “isdbgrid” : 1, “hostname” : “mongo3c”, “ok” : 1 }
通过一个python脚本进行随机写入：分别向A、B 2个mongos各写入10万条记录。&lt;/p&gt;

&lt;p&gt;View Code
查看是否分片：db.collection.stats()&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; db.account.stats() #查看集合的分布情况
…
…
    “shards” : {
        “shard0000” : {
            “ns” : “dba.account”,
            “count” : 89710,
            “size” : 10047520,
…
…
        “shard0001” : {
            “ns” : “dba.account”,
            “count” : 19273,
            “size” : 2158576,
…
…
        “shard0002” : {
            “ns” : “dba.account”,
            “count” : 91017,
            “size” : 10193904,
…
…
复制代码
上面加粗部分为集合的基本信息，可以看到分片成功，各个分片都有数据(count)。到此MongoDB分片集群搭建成功。&lt;/p&gt;

&lt;p&gt;++++++++++++++++++++++++++++++++++++++++++++++++&lt;/p&gt;

&lt;p&gt;感兴趣的同学可以看下面这个比较有趣的现象：&lt;/p&gt;

&lt;p&gt;复制代码
#在写之前分片的基本信息：
mongos&amp;gt; sh.status()
— Sharding Status — 
…
…
  databases:
    {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }
    {  “_id” : “test”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “dba”,  “partitioned” : true,  “primary” : “shard0000” }
        dba.account
            shard key: { “name” : 1 }
            chunks:
                shard0000    1
            { “name” : { “$minKey” : 1 } } –» { “name” : { “$maxKey” : 1 } } on : shard0000 Timestamp(1, 0)   #可以看到这里片键的写入，都是写在shard0000里面的。&lt;/p&gt;

&lt;p&gt;#在写期间的分片基本信息：
mongos&amp;gt; sh.status()
— Sharding Status — 
…
…
  databases:
    {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }
    {  “_id” : “test”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “dba”,  “partitioned” : true,  “primary” : “shard0000” }
        dba.account
            shard key: { “name” : 1 }
            chunks:          #数据块分布
                shard0000    1
                shard0001    1
                shard0002    1
            { “name” : { “$minKey” : 1 } } –» { “name” : “5yyfY8mmR5HyhGJ” } on : shard0001 Timestamp(2, 0) 
            { “name” : “5yyfY8mmR5HyhGJ” } –» { “name” : “woQAv99Pq1FVoMX” } on : shard0002 Timestamp(3, 0) 
            { “name” : “woQAv99Pq1FVoMX” } –» { “name” : { “$maxKey” : 1 } } on : shard0000 Timestamp(3, 1)   #可以看到片键写入的基本分布&lt;/p&gt;

&lt;p&gt;#在写完成后的基本信息：
mongos&amp;gt; sh.status()
— Sharding Status — 
…
…
  databases:
    {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }
    {  “_id” : “test”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “dba”,  “partitioned” : true,  “primary” : “shard0000” }
        dba.account
            shard key: { “name” : 1 }
            chunks:          #数据块分布
                shard0000    2
                shard0001    1
                shard0002    2
            { “name” : { “$minKey” : 1 } } –» { “name” : “5yyfY8mmR5HyhGJ” } on : shard0001 Timestamp(2, 0) 
            { “name” : “5yyfY8mmR5HyhGJ” } –» { “name” : “UavMbMlfszZOFrz” } on : shard0000 Timestamp(4, 0) 
            { “name” : “UavMbMlfszZOFrz” } –» { “name” : “t9LyVSNXDmf6esP” } on : shard0002 Timestamp(4, 1) 
            { “name” : “t9LyVSNXDmf6esP” } –» { “name” : “woQAv99Pq1FVoMX” } on : shard0002 Timestamp(3, 4) 
            { “name” : “woQAv99Pq1FVoMX” } –» { “name” : { “$maxKey” : 1 } } on : shard0000 Timestamp(3, 1)  #最后片键写入的分布
复制代码
上面加粗的信息对比上看到，本来在每个分片上都只有一个块，最后在shard0000、shard0002上有2个块，被拆分了。shard0001不变。这是因为mongos在收到写请求的时候，会检查当前块的拆分阀值点。到达该阀值的时候，会向分片发起一个拆分的请求。例子中shard0000和shard0002里的块被拆分了。分片内的数据进行了迁移（有一定的消耗），最后通过一个均衡器来对数据进行转移分配。所以在写入途中要是看到一个分片中集合的数量变小也是正常的。&lt;/p&gt;

&lt;p&gt;balancer:  #均衡器
    Currently enabled:  yes
    Currently running:  yes   #正在转移
        Balancer lock taken at Fri Jul 10 2015 22:57:27 GMT+0800 (CST) by mongo2:30000:1436540125:1804289383:Balancer:846930886
均衡器：均衡器负责数据迁移，周期性的检查分片是否存在不均衡，如果不存在则会开始块的迁移，config.locks集合里的state表示均衡器是否找正在运行，0表示非活动状态，2表示正在均衡。均衡迁移数据的过程会增加系统的负载：目标分片必须查询源分片的所有文档，将文档插入目标分片中，再清除源分片的数据。可以关闭均衡器（不建议）：关闭会导致各分片数据分布不均衡，磁盘空间得不到有效的利用。&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; sh.setBalancerState(false)  #关闭自动均衡器，手动均衡，打开：sh.setBalancerState(true)
mongos&amp;gt; db.settings.find()          #查看均衡器状态
{ “_id” : “balancer”, “stopped” : true }
可以为均衡器设置一个均衡时间窗口：activeWindow&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; db.settings.update({“_id”:”balancer”},{“$set”:{“activeWindow”:{“start”:”08:00”,”stop”:”02:00”}}},true)
WriteResult({ “nMatched” : 1, “nUpserted” : 0, “nModified” : 1 })
mongos&amp;gt; db.settings.find({“_id”:”balancer”})
{ “_id” : “balancer”, “stopped” : false, “activeWindow” : { “start” : “08:00”, “stop” : “02:00” } }
上面说明：均衡只会在早上8点到凌晨2点进行均衡操作。均衡器是以块的数量作为迁移指标，而非数据大小，块的大小默认是64M，可以修改:(config.settings)&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; db.settings.find()
{ “_id” : “chunksize”, “value” : 64 }
mongos&amp;gt; db.settings.save({“_id”:”chunksize”,”value”:32})
WriteResult({ “nMatched” : 1, “nUpserted” : 0, “nModified” : 1 })
mongos&amp;gt; db.settings.find()
{ “_id” : “chunksize”, “value” : 32 }
上面把块的默认大小改成了32M，除了通过均衡器自动迁移外，还可以手动迁移数据：sh.moveChunk(“db.collection”,{块地址},”新片名称”)&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; db.chunks.find({“&lt;em&gt;id” : “abc.account-name&lt;/em&gt;&quot;wPeFnJEvendSTbH&quot;”}).pretty() #先到config.chunks上任意找一个块
{
    “&lt;em&gt;id” : “abc.account-name&lt;/em&gt;&quot;wPeFnJEvendSTbH&quot;”,
    “lastmod” : Timestamp(3, 1),
    “lastmodEpoch” : ObjectId(“55a52ff1fdd9a605a0371327”),
    “ns” : “abc.account”,
    “min” : {
        “name” : “wPeFnJEvendSTbH”              #被移动的块
    },
    “max” : {
        “name” : { “$maxKey” : 1 }
    },
    “shard” : “shard0000”                       #原先所在的分片
}
mongos&amp;gt; sh.moveChunk(“abc.account”,{“name” : “wPeFnJEvendSTbH”},”mablevi”)  #把abc.account集合中包含name(片键)为”“的快迁移到mablevi分片中
{ “millis” : 6800, “ok” : 1 }
mongos&amp;gt; db.chunks.find({“&lt;em&gt;id” : “abc.account-name&lt;/em&gt;&quot;wPeFnJEvendSTbH&quot;”}).pretty()&lt;br /&gt;
{
    “&lt;em&gt;id” : “abc.account-name&lt;/em&gt;&quot;wPeFnJEvendSTbH&quot;”,
    “lastmod” : Timestamp(5, 0),
    “lastmodEpoch” : ObjectId(“55a52ff1fdd9a605a0371327”),
    “ns” : “abc.account”,
    “min” : {
        “name” : “wPeFnJEvendSTbH”
    },
    “max” : {
        “name” : { “$maxKey” : 1 }
    },
    “shard” : “mablevi”                        #已被迁移到新片
}
复制代码
上面是手动移动数据的操作，数据被移动。 要是块超出了64M限制【原因是片键没选好(日期、状态值等)，导致一个块无限增大】，则无法进行自动均衡，无法分块。有2个办法：1是加大块的大小(setting)，2是拆分sh.splitAt()（推荐）。&lt;/p&gt;

&lt;p&gt;所以要是遇到分片写入比单点写入慢就是因为分片路由服务（mongos）需要维护元数据、数据迁移、路由开销等。&lt;/p&gt;

&lt;p&gt;++++++++++++++++++++++++++++++++++++++++++++++++&lt;/p&gt;

&lt;p&gt;五、高可用：Sharding+Replset&lt;/p&gt;

&lt;p&gt;上面的分片都是单点的，要是一个分片坏了，则数据会丢失，利用之前减少的副本集，能否把副本集加入到分片中？下面就来说明下。&lt;/p&gt;

&lt;p&gt;1）添加副本集分片服务器（mmm副本集名称）：这里测试就只对一个分片加副本集，要实现完全的高可用就需要对所有分片加副本集，避免单点故障&lt;/p&gt;

&lt;p&gt;一个普通的副本集：&lt;/p&gt;

&lt;p&gt;View Code
现在需要把这个副本集加入到分片中：&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; sh.addShard(“mmm/192.168.200.25:27017,192.168.200.245:27017,192.168.200.245:37017”) #加入副本集分片
{ “shardAdded” : “mmm”, “ok” : 1 }&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; sh.status()
— Sharding Status — 
…
…
shards:
    {  “_id” : “mmm”,  “host” : “mmm/192.168.200.245:27017,192.168.200.245:37017,192.168.200.25:27017” }
    {  “_id” : “shard0000”,  “host” : “192.168.200.51:40000” }
    {  “_id” : “shard0001”,  “host” : “192.168.200.52:40000” }
    {  “_id” : “shard0002”,  “host” : “192.168.200.53:40000” }
  balancer:
    Currently enabled:  yes
    Currently running:  no
    Failed balancer rounds in last 5 attempts:  0
    Migration Results for the last 24 hours: 
        4 : Success
  databases:
    {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }
    {  “_id” : “test”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “dba”,  “partitioned” : true,  “primary” : “shard0000” }
        dba.account
            shard key: { “name” : 1 }
            chunks:
                mmm    1
                shard0000    1
                shard0001    1
                shard0002    2
            { “name” : { “$minKey” : 1 } } –» { “name” : “5yyfY8mmR5HyhGJ” } on : shard0001 Timestamp(2, 0) 
            { “name” : “5yyfY8mmR5HyhGJ” } –» { “name” : “UavMbMlfszZOFrz” } on : mmm Timestamp(5, 0) 
            { “name” : “UavMbMlfszZOFrz” } –» { “name” : “t9LyVSNXDmf6esP” } on : shard0002 Timestamp(4, 1) 
            { “name” : “t9LyVSNXDmf6esP” } –» { “name” : “woQAv99Pq1FVoMX” } on : shard0002 Timestamp(3, 4) 
            { “name” : “woQAv99Pq1FVoMX” } –» { “name” : { “$maxKey” : 1 } } on : shard0000 Timestamp(5, 1) 
    {  “_id” : “abc”,  “partitioned” : false,  “primary” : “shard0000” }   #未设置分片
复制代码
上面加粗部分表示副本集分片已经成功加入，并且新加入的分片会分到已有的分片数据。&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; db.account.stats()
…
…
    “shards” : {
        “mmm” : {
            “ns” : “dba.account”,
            “count” : 7723,        #后加入的分片得到了数据
            “size” : 741408,
            “avgObjSize” : 96,
            “storageSize” : 2793472,
            “numExtents” : 5,
            “nindexes” : 2,
            “lastExtentSize” : 2097152,
            “paddingFactor” : 1,
            “systemFlags” : 1,
            “userFlags” : 0,
            “totalIndexSize” : 719488,
            “indexSizes” : {
                “&lt;em&gt;id&lt;/em&gt;” : 343392,
                “name_1” : 376096
            },
            “ok” : 1
        },
…
…
复制代码
2）继续用python脚本写数据，填充到副本集中&lt;/p&gt;

&lt;p&gt;由于之前的副本集是比较老的版本（2.4），所以在写入副本集分片的时候报错：&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; db.account.insert({“name”:”UavMbMlfsz1OFrz”})
WriteResult({
    “nInserted” : 0,
    “writeError” : {
        “code” : 83,
        “errmsg” : “write results unavailable from 192.168.200.25:27017 :: caused by :: Location28563 cannot send batch write operation to server 192.168.200.25:27017 (192.168.200.25)”
    }
})
复制代码
太混蛋了，错误提示不太人性化，搞了半天。所以说版本一致性还是很重要的。现在重新开了一个副本集：&lt;/p&gt;

&lt;p&gt;View Code
把之前的副本集分片删除了，如何删除见下面3）。&lt;/p&gt;

&lt;p&gt;新的副本集加入分片中：&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; sh.addShard(“mablevi/192.168.200.53:50000,192.168.200.53:50001,192.168.200.53:50002”)
{ “shardAdded” : “mablevi”, “ok” : 1 }&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; sh.status()
— Sharding Status — 
…
…
  shards:
    {  “_id” : “mablevi”,  “host” : “mablevi/192.168.200.53:50000,192.168.200.53:50001,192.168.200.53:50002” }
    {  “_id” : “shard0000”,  “host” : “192.168.200.51:40000” }
    {  “_id” : “shard0001”,  “host” : “192.168.200.52:40000” }
    {  “_id” : “shard0002”,  “host” : “192.168.200.53:40000” }
…
…
        dba.account
            shard key: { “name” : 1 }
            chunks:
                mablevi    1
                shard0000    1
                shard0001    1
                shard0002    2
            { “name” : { “$minKey” : 1 } } –» { “name” : “5yyfY8mmR5HyhGJ” } on : shard0001 Timestamp(2, 0) 
            { “name” : “5yyfY8mmR5HyhGJ” } –» { “name” : “UavMbMlfszZOFrz” } on : mablevi Timestamp(9, 0) #新加入的分片得到数据
            { “name” : “UavMbMlfszZOFrz” } –» { “name” : “t9LyVSNXDmf6esP” } on : shard0002 Timestamp(4, 1) 
            { “name” : “t9LyVSNXDmf6esP” } –» { “name” : “woQAv99Pq1FVoMX” } on : shard0002 Timestamp(3, 4) 
            { “name” : “woQAv99Pq1FVoMX” } –» { “name” : { “$maxKey” : 1 } } on : shard0000 Timestamp(9, 1) 
    {  “_id” : “abc”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “mablevi”,  “partitioned” : false,  “primary” : “shard0001” }
复制代码
继续用python写入操作：&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; db.account.stats()
{
…
…
    “shards” : {
        “mablevi” : {
            “ns” : “dba.account”,
            “count” : 47240,
            “size” : 5290880,
…
…
复制代码
副本集的分片被写入了47240条记录。此时把副本集分片的Primary shutdown掉，再查看：&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; db.account.stats()
{
    “sharded” : true,
    “code” : 13639,
    “ok” : 0,
    “errmsg” : “exception: can’t connect to new replica set master [192.168.200.53:50000], err: couldn’t connect to server 192.168.200.53:50000 (192.168.200.53), connection attempt failed”  #由于副本集的Primary被shutdown之后，选举新主还是要几秒的时间，期间数据不能访问，导致分片数据也不能访问
}
mongos&amp;gt; db.account.stats()
…
…
    “shards” : {
        “mablevi” : {
            “ns” : “dba.account”,
            “count” : 47240,       #副本集新主选举完毕之后，分片数据访问正常。数据没有丢失，高可用得到了实现。
            “size” : 5290880,
…
…
复制代码
要是让副本集分片只剩下一台（Secondary），则分片会报错：&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; db.account.stats()
{
    “sharded” : true,
    “code” : 10009,
    “ok” : 0,
    “errmsg” : “exception: ReplicaSetMonitor no master found for set: mablevi” #数据不能访问
}
复制代码
3）删除分片： db.runCommand({“removeshard”:”mmm”})&lt;/p&gt;

&lt;p&gt;要是觉得分片太多了，想删除，则：&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; use admin   #需要到admin下面删除
switched to db admin
mongos&amp;gt; db.runCommand({“removeshard”:”mmm”})
{
    “msg” : “draining started successfully”,
    “state” : “started”,   #开始删除，数据正在转移
    “shard” : “mmm”,
    “ok” : 1
}
mongos&amp;gt; sh.status()
— Sharding Status —…
…
  shards:
    {  “_id” : “mmm”,  “host” : “mmm/192.168.200.245:27017,192.168.200.245:37017,192.168.200.25:27017”,  “draining” : true }  #删除的分片数据移动到其他分片
    {  “_id” : “shard0000”,  “host” : “192.168.200.51:40000” }
    {  “_id” : “shard0001”,  “host” : “192.168.200.52:40000” }
    {  “_id” : “shard0002”,  “host” : “192.168.200.53:40000” }
…
…
  databases:
    {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }
    {  “_id” : “test”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “dba”,  “partitioned” : true,  “primary” : “shard0000” }
        dba.account
            shard key: { “name” : 1 }
            chunks:
                shard0000    2
                shard0001    1
                shard0002    2
            { “name” : { “$minKey” : 1 } } –» { “name” : “5yyfY8mmR5HyhGJ” } on : shard0001 Timestamp(2, 0) 
            { “name” : “5yyfY8mmR5HyhGJ” } –» { “name” : “UavMbMlfszZOFrz” } on : shard0000 Timestamp(8, 0) 
            { “name” : “UavMbMlfszZOFrz” } –» { “name” : “t9LyVSNXDmf6esP” } on : shard0002 Timestamp(4, 1) #这里已经没有了被删除分片信息
            { “name” : “t9LyVSNXDmf6esP” } –» { “name” : “woQAv99Pq1FVoMX” } on : shard0002 Timestamp(3, 4) 
            { “name” : “woQAv99Pq1FVoMX” } –» { “name” : { “$maxKey” : 1 } } on : shard0000 Timestamp(7, 1) 
    {  “_id” : “abc”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “mablevi”,  “partitioned” : false,  “primary” : “shard0001” }&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; db.runCommand({“removeshard”:”mmm”})   #再次执行，直到执行成功，要是原来分片的数据比较大，这里比较费时，要是一个主分片则需要执行movePrimary
{
    “msg” : “removeshard completed successfully”,
    “state” : “completed”,  #完成删除
    “shard” : “mmm”,
    “ok” : 1
}
mongos&amp;gt; sh.status()
— Sharding Status —…
  shards:   #分片消失
    {  “_id” : “shard0000”,  “host” : “192.168.200.51:40000” }
    {  “_id” : “shard0001”,  “host” : “192.168.200.52:40000” }
    {  “_id” : “shard0002”,  “host” : “192.168.200.53:40000” }
…
…
            { “name” : { “$minKey” : 1 } } –» { “name” : “5yyfY8mmR5HyhGJ” } on : shard0001 Timestamp(2, 0) 
            { “name” : “5yyfY8mmR5HyhGJ” } –» { “name” : “UavMbMlfszZOFrz” } on : shard0000 Timestamp(8, 0) 
            { “name” : “UavMbMlfszZOFrz” } –» { “name” : “t9LyVSNXDmf6esP” } on : shard0002 Timestamp(4, 1) #已经没有了被删除分片的信息
            { “name” : “t9LyVSNXDmf6esP” } –» { “name” : “woQAv99Pq1FVoMX” } on : shard0002 Timestamp(3, 4) 
            { “name” : “woQAv99Pq1FVoMX” } –» { “name” : { “$maxKey” : 1 } } on : shard0000 Timestamp(7, 1) 
    {  “_id” : “abc”,  “partitioned” : false,  “primary” : “shard0000” }
    {  “_id” : “mablevi”,  “partitioned” : false,  “primary” : “shard0001” }
复制代码
分片被删除之后，数据被移到其他分片中，不会丢失。要是想让主分片进行转移则(movePrimary):&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; db.adminCommand({“movePrimary”:”test”,”to”:”shard0001”}) #把test的主分片从shard0000迁移到shard0001 
刷新下配置服务器：db.adminCommand({“flushRouterConfig”:1})&lt;/p&gt;

&lt;p&gt;db.adminCommand({“flushRouterConfig”:1})
最后来查看下分片成员：db.runCommand({ listshards : 1 })&lt;/p&gt;

&lt;p&gt;复制代码
mongos&amp;gt; use admin  #需要进入admin才能执行
switched to db admin
mongos&amp;gt; db.runCommand({ listshards : 1 })
{
    “shards” : [
        {
            “_id” : “shard0000”,
            “host” : “192.168.200.51:40000”
        },
        {
            “_id” : “shard0001”,
            “host” : “192.168.200.52:40000”
        },
        {
            “_id” : “shard0002”,
            “host” : “192.168.200.53:40000”
        },
        {
            “_id” : “mablevi”,
            “host” : “mablevi/192.168.200.53:50000,192.168.200.53:50001,192.168.200.53:50002”
        }
    ],
    “ok” : 1
}
复制代码
到此已经把MongoDB分片原理、搭建、应用大致已经介绍完。&lt;/p&gt;

&lt;p&gt;六、认证分配&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  上面的所有操作都是在无账号密码下进行的，这样是不安全的，那如何使用账号密码呢？和副本级一样，需要添加KeyFile参数，但是针对上面的三个角色（config、mongos、mongod）账号密码怎么添加呢？官网上已经做了说明：http://docs.mongodb.org/manual/tutorial/enable-authentication-in-sharded-cluster/。下面就对有账号密码认证分片进行相关设置说明。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先要创建账号(Root角色)和生成一个KeyFile文件，其中mongos 不需要创建账号。&lt;/p&gt;

&lt;p&gt;openssl rand -base64 741 &amp;gt; mongodb-keyfile
chmod 600 mongodb-keyfile
其实这个文件也可以直接用明文，只要保证各个地方指定的文件是同一个就可以了。&lt;/p&gt;

&lt;p&gt;1）mongd： 首先在mongod角色的分片成员上生成key file文件，特别注意的是有副本级的分片，再把这个文件分别复制到其他角色的服务器上。再添加参数：&lt;/p&gt;

&lt;p&gt;auth = true
keyFile = /usr/local/mongodb-keyfile
2）Config上添加参数：&lt;/p&gt;

&lt;p&gt;auth = true
keyFile = /usr/local/mongodb-keyfile
3）mongos上添加参数，因为mongos本来就是从config里加载数据的，所以只需要添加keyfile文件即可，不需要找上面createUser。&lt;/p&gt;

&lt;p&gt;keyFile = /usr/local/mongodb-keyfile
最后重启各个服务，再进入mongos里查看：&lt;/p&gt;

&lt;p&gt;复制代码
root@mongo1:/usr/local# mongo –port=30000
MongoDB shell version: 3.0.4
connecting to: 127.0.0.1:30000/test
mongos&amp;gt; sh.status()      #没有认证，没有权限报错。
2015-07-14T23:42:11.800+0800 E QUERY    Error: error: { “$err” : “not authorized for query on config.version”, “code” : 13 }
    at Error (&lt;anonymous&gt;)
    at DBQuery.next (src/mongo/shell/query.js:259:15)
    at DBCollection.findOne (src/mongo/shell/collection.js:189:22)
    at printShardingStatus (src/mongo/shell/shardingtest.js:659:55)
    at Function.sh.status (src/mongo/shell/utils_sh.js:60:5)
    at (shell):1:4 at src/mongo/shell/query.js:259
mongos&amp;gt; use admin
switched to db admin
mongos&amp;gt; db.auth('dba','dba')   #认证
1
mongos&amp;gt; sh.status()            #有权限
--- Sharding Status --- 
  sharding version: {
    &quot;_id&quot; : 1,
    &quot;minCompatibleVersion&quot; : 5,
    &quot;currentVersion&quot; : 6,
    &quot;clusterId&quot; : ObjectId(&quot;55a51ef18bd517d4acec5ef9&quot;)
}
  shards:
    {  &quot;_id&quot; : &quot;mablevi&quot;,  &quot;host&quot; : &quot;mablevi/192.168.200.53:50000,192.168.200.53:50001,192.168.200.53:50002&quot; }
    {  &quot;_id&quot; : &quot;shard0000&quot;,  &quot;host&quot; : &quot;192.168.200.51:40000&quot; }
    {  &quot;_id&quot; : &quot;shard0001&quot;,  &quot;host&quot; : &quot;192.168.200.52:40000&quot; }
    {  &quot;_id&quot; : &quot;shard0002&quot;,  &quot;host&quot; : &quot;192.168.200.53:40000&quot; }
  balancer:
  ...
  ...
  databases:
    {  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
    {  &quot;_id&quot; : &quot;test&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;shard0000&quot; }
    {  &quot;_id&quot; : &quot;dba&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;shard0000&quot; }
        dba.account
            shard key: { &quot;name&quot; : 1 }
            chunks:
                mablevi    1
                shard0000    1
                shard0001    2
                shard0002    1
            { &quot;name&quot; : { &quot;$minKey&quot; : 1 } } --&amp;gt;&amp;gt; { &quot;name&quot; : &quot;9XXqCaBhfhPIXLq&quot; } on : mablevi Timestamp(2, 0) 
            { &quot;name&quot; : &quot;9XXqCaBhfhPIXLq&quot; } --&amp;gt;&amp;gt; { &quot;name&quot; : &quot;RWINvgjYYQmbZds&quot; } on : shard0002 Timestamp(4, 0) 
            { &quot;name&quot; : &quot;RWINvgjYYQmbZds&quot; } --&amp;gt;&amp;gt; { &quot;name&quot; : &quot;jSPRBNH8rvnzblG&quot; } on : shard0001 Timestamp(4, 1) 
            { &quot;name&quot; : &quot;jSPRBNH8rvnzblG&quot; } --&amp;gt;&amp;gt; { &quot;name&quot; : &quot;okmjUUZuuKgftDC&quot; } on : shard0001 Timestamp(3, 4) 
            { &quot;name&quot; : &quot;okmjUUZuuKgftDC&quot; } --&amp;gt;&amp;gt; { &quot;name&quot; : { &quot;$maxKey&quot; : 1 } } on : shard0000 Timestamp(3, 1) 
复制代码
七、分片备份、还原&lt;/anonymous&gt;&lt;/p&gt;

&lt;p&gt;因为分片机制里面会有平衡器来迁移数据，所以各个分片里的数据很可能会移动，所以在备份分片时需要做：&lt;/p&gt;

&lt;p&gt;①：先停止平衡器的工作，并检查没有chunk move动作，保证dump的时候没有进行数据迁移。&lt;/p&gt;

&lt;p&gt;mongos&amp;gt; sh.stopBalancer()
②：锁定数据库，保证数据没有写入：在各个分片上和配置服务器上执行。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;db.fsyncLock()
{
    “info” : “now locked against writes, use db.fsyncUnlock() to unlock”,
    “seeAlso” : “http://dochub.mongodb.org/core/fsynccommand”,
    “ok” : 1
}
③：执行备份操作，备份各个分片服务器和配置服务器。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;mongodump -udba -p12345 -d dba_test –authenticationDatabase admin -o backup/
④：解锁数据库，备份完成之后在分片和配置服务器上解锁数据库，允许修改。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;db.fsyncUnlock()
{ “ok” : 1, “info” : “unlock completed” }
当数据库出现问题，需要还原的时候，需要还原各个分片和配置服务器，并且重启MongoDB实例。还原数据库需要做：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;①：还原各个分片和配置服务器。&lt;/p&gt;

&lt;p&gt;mongorestore –host=127.0.0.1 –port=27017 -udba -p12345 -d dba_test –authenticationDatabase admin –drop backup/dba_test
②：重启各个实例&lt;/p&gt;

&lt;p&gt;为什么需要索引？
当你抱怨MongoDB集合查询效率低的时候，可能你就需要考虑使用索引了，为了方便后续介绍，先科普下MongoDB里的索引机制（同样适用于其他的数据库比如mysql）。&lt;/p&gt;

&lt;p&gt;mongo-9552:PRIMARY&amp;gt; db.person.find()
{ “_id” : ObjectId(“571b5da31b0d530a03b3ce82”), “name” : “jack”, “age” : 19 }
{ “_id” : ObjectId(“571b5dae1b0d530a03b3ce83”), “name” : “rose”, “age” : 20 }
{ “_id” : ObjectId(“571b5db81b0d530a03b3ce84”), “name” : “jack”, “age” : 18 }
{ “_id” : ObjectId(“571b5dc21b0d530a03b3ce85”), “name” : “tony”, “age” : 21 }
{ “_id” : ObjectId(“571b5dc21b0d530a03b3ce86”), “name” : “adam”, “age” : 18 }
当你往某各个集合插入多个文档后，每个文档在经过底层的存储引擎持久化后，会有一个位置信息，通过这个位置信息，就能从存储引擎里读出该文档。比如mmapv1引擎里，位置信息是『文件id + 文件内offset 』， 在wiredtiger存储引擎（一个KV存储引擎）里，位置信息是wiredtiger在存储文档时生成的一个key，通过这个key能访问到对应的文档；为方便介绍，统一用pos(position的缩写)来代表位置信息。&lt;/p&gt;

&lt;p&gt;比如上面的例子里，person集合里包含插入了4个文档，假设其存储后位置信息如下(为方便描述，文档省去_id字段)&lt;/p&gt;

&lt;p&gt;位置信息	文档
pos1	{“name” : “jack”, “age” : 19 }
pos2	{“name” : “rose”, “age” : 20 }
pos3	{“name” : “jack”, “age” : 18 }
pos4	{“name” : “tony”, “age” : 21}
pos5	{“name” : “adam”, “age” : 18}
假设现在有个查询 db.person.find( {age: 18} ), 查询所有年龄为18岁的人，这时需要遍历所有的文档（『全表扫描』），根据位置信息读出文档，对比age字段是否为18。当然如果只有4个文档，全表扫描的开销并不大，但如果集合文档数量到百万、甚至千万上亿的时候，对集合进行全表扫描开销是非常大的，一个查询耗费数十秒甚至几分钟都有可能。&lt;/p&gt;

&lt;p&gt;如果想加速 db.person.find( {age: 18} ），就可以考虑对person表的age字段建立索引。&lt;/p&gt;

&lt;p&gt;db.person.createIndex( {age: 1} )  // 按age字段创建升序索引
建立索引后，MongoDB会额外存储一份按age字段升序排序的索引数据，索引结构类似如下，索引通常采用类似btree的结构持久化存储，以保证从索引里快速（O(logN)的时间复杂度）找出某个age值对应的位置信息，然后根据位置信息就能读取出对应的文档。&lt;/p&gt;

&lt;p&gt;AGE	位置信息
18	pos3
18	pos5
19	pos1
20	pos2
21	pos4
简单的说，索引就是将文档按照某个（或某些）字段顺序组织起来，以便能根据该字段高效的查询。有了索引，至少能优化如下场景的效率：&lt;/p&gt;

&lt;p&gt;查询，比如查询年龄为18的所有人
更新/删除，将年龄为18的所有人的信息更新或删除，因为更新或删除时，需要根据条件先查询出所有符合条件的文档，所以本质上还是在优化查询
排序，将所有人的信息按年龄排序，如果没有索引，需要全表扫描文档，然后再对扫描的结果进行排序
众所周知，MongoDB默认会为插入的文档生成_id字段（如果应用本身没有指定该字段），_id是文档唯一的标识，为了保证能根据文档id快递查询文档，MongoDB默认会为集合创建_id字段的索引。&lt;/p&gt;

&lt;p&gt;mongo-9552:PRIMARY&amp;gt; db.person.getIndexes() // 查询集合的索引信息
[
    {
        “ns” : “test.person”,  // 集合名
        “v” : 1,               // 索引版本
        “key” : {              // 索引的字段及排序方向
            “&lt;em&gt;id” : 1           // 根据_id字段升序索引
        },
        “name” : “_id&lt;/em&gt;”        // 索引的名称
    }
]
MongoDB索引类型
MongoDB支持多种类型的索引，包括单字段索引、复合索引、多key索引、文本索引等，每种类型的索引有不同的使用场合。&lt;/p&gt;

&lt;p&gt;单字段索引 （Single Field Index）
    db.person.createIndex( {age: 1} ) 
上述语句针对age创建了单字段索引，其能加速对age字段的各种查询请求，是最常见的索引形式，MongoDB默认创建的id索引也是这种类型。&lt;/p&gt;

&lt;p&gt;{age: 1} 代表升序索引，也可以通过{age: -1}来指定降序索引，对于单字段索引，升序/降序效果是一样的。&lt;/p&gt;

&lt;p&gt;复合索引 (Compound Index)
复合索引是Single Field Index的升级版本，它针对多个字段联合创建索引，先按第一个字段排序，第一个字段相同的文档按第二个字段排序，依次类推，如下针对age, name这2个字段创建一个复合索引。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.person.createIndex( {age: 1, name: 1} )  上述索引对应的数据组织类似下表，与{age: 1}索引不同的时，当age字段相同时，在根据name字段进行排序，所以pos5对应的文档排在pos3之前。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;AGE,NAME	位置信息
18,adam	pos5
18,jack	pos3
19,jack	pos1
20,rose	pos2
21,tony	pos4
复合索引能满足的查询场景比单字段索引更丰富，不光能满足多个字段组合起来的查询，比如db.person.find( {age： 18， name: “jack”} )，也能满足所以能匹配符合索引前缀的查询，这里{age: 1}即为{age: 1, name: 1}的前缀，所以类似db.person.find( {age： 18} )的查询也能通过该索引来加速；但db.person.find( {name: “jack”} )则无法使用该复合索引。如果经常需要根据『name字段』以及『name和age字段组合』来查询，则应该创建如下的复合索引&lt;/p&gt;

&lt;p&gt;db.person.createIndex( {name: 1, age: 1} ) 
除了查询的需求能够影响索引的顺序，字段的值分布也是一个重要的考量因素，即使person集合所有的查询都是『name和age字段组合』（指定特定的name和age），字段的顺序也是有影响的。&lt;/p&gt;

&lt;p&gt;age字段的取值很有限，即拥有相同age字段的文档会有很多；而name字段的取值则丰富很多，拥有相同name字段的文档很少；显然先按name字段查找，再在相同name的文档里查找age字段更为高效。&lt;/p&gt;

&lt;p&gt;多key索引 （Multikey Index）
当索引的字段为数组时，创建出的索引称为多key索引，多key索引会为数组的每个元素建立一条索引，比如person表加入一个habbit字段（数组）用于描述兴趣爱好，需要查询有相同兴趣爱好的人就可以利用habbit字段的多key索引。&lt;/p&gt;

&lt;p&gt;{“name” : “jack”, “age” : 19, habbit: [“football, runnning”]}
db.person.createIndex( {habbit: 1} )  // 自动创建多key索引
db.person.find( {habbit: “football”} )
其他类型索引
哈希索引（Hashed Index）是指按照某个字段的hash值来建立索引，目前主要用于MongoDB Sharded Cluster的Hash分片，hash索引只能满足字段完全匹配的查询，不能满足范围查询等。&lt;/p&gt;

&lt;p&gt;地理位置索引（Geospatial Index）能很好的解决O2O的应用场景，比如『查找附近的美食』、『查找某个区域内的车站』等。&lt;/p&gt;

&lt;p&gt;文本索引（Text Index）能解决快速文本查找的需求，比如有一个博客文章集合，需要根据博客的内容来快速查找，则可以针对博客内容建立文本索引。&lt;/p&gt;

&lt;p&gt;索引额外属性
MongoDB除了支持多种不同类型的索引，还能对索引定制一些特殊的属性。&lt;/p&gt;

&lt;p&gt;唯一索引 (unique index)：保证索引对应的字段不会出现相同的值，比如_id索引就是唯一索引
TTL索引：可以针对某个时间字段，指定文档的过期时间（经过指定时间后过期 或 在某个时间点过期）
部分索引 (partial index): 只针对符合某个特定条件的文档建立索引，3.2版本才支持该特性
稀疏索引(sparse index): 只针对存在索引字段的文档建立索引，可看做是部分索引的一种特殊情况
索引优化
db profiling
MongoDB支持对DB的请求进行profiling，目前支持3种级别的profiling。&lt;/p&gt;

&lt;p&gt;0： 不开启profiling
1： 将处理时间超过某个阈值(默认100ms)的请求都记录到DB下的system.profile集合 （类似于mysql、redis的slowlog）
2： 将所有的请求都记录到DB下的system.profile集合（生产环境慎用）
通常，生产环境建议使用1级别的profiling，并根据自身需求配置合理的阈值，用于监测慢请求的情况，并及时的做索引优化。&lt;/p&gt;

&lt;p&gt;如果能在集合创建的时候就能『根据业务查询需求决定应该创建哪些索引』，当然是最佳的选择；但由于业务需求多变，要根据实际情况不断的进行优化。索引并不是越多越好，集合的索引太多，会影响写入、更新的性能，每次写入都需要更新所有索引的数据；所以你system.profile里的慢请求可能是索引建立的不够导致，也可能是索引过多导致。&lt;/p&gt;

&lt;p&gt;查询计划
索引已经建立了，但查询还是很慢怎么破？这时就得深入的分析下索引的使用情况了，可通过查看下详细的查询计划来决定如何优化。通过执行计划可以看出如下问题&lt;/p&gt;

&lt;p&gt;根据某个/些字段查询，但没有建立索引
根据某个/些字段查询，但建立了多个索引，执行查询时没有使用预期的索引。
建立索引前，db.person.find( {age： 18} )必须执行COLLSCAN，即全表扫描。&lt;/p&gt;

&lt;p&gt;mongo-9552:PRIMARY&amp;gt; db.person.find({age: 18}).explain()
{
    “queryPlanner” : {
        “plannerVersion” : 1,
        “namespace” : “test.person”,
        “indexFilterSet” : false,
        “parsedQuery” : {
            “age” : {
                “$eq” : 18
            }
        },
        “winningPlan” : {
            “stage” : “COLLSCAN”,
            “filter” : {
                “age” : {
                    “$eq” : 18
                }
            },
            “direction” : “forward”
        },
        “rejectedPlans” : [ ]
    },
    “serverInfo” : {
        “host” : “localhost”,
        “port” : 9552,
        “version” : “3.2.3”,
        “gitVersion” : “b326ba837cf6f49d65c2f85e1b70f6f31ece7937”
    },
    “ok” : 1
}
建立索引后，通过查询计划可以看出，先进行[IXSCAN]((https://docs.mongodb.org/manual/reference/explain-results/#queryplanner)(从索引中查找)，然后FETCH，读取出满足条件的文档。&lt;/p&gt;

&lt;p&gt;mongo-9552:PRIMARY&amp;gt; db.person.find({age: 18}).explain()
{
    “queryPlanner” : {
        “plannerVersion” : 1,
        “namespace” : “test.person”,
        “indexFilterSet” : false,
        “parsedQuery” : {
            “age” : {
                “$eq” : 18
            }
        },
        “winningPlan” : {
            “stage” : “FETCH”,
            “inputStage” : {
                “stage” : “IXSCAN”,
                “keyPattern” : {
                    “age” : 1
                },
                “indexName” : “age_1”,
                “isMultiKey” : false,
                “isUnique” : false,
                “isSparse” : false,
                “isPartial” : false,
                “indexVersion” : 1,
                “direction” : “forward”,
                “indexBounds” : {
                    “age” : [
                        “[18.0, 18.0]”
                    ]
                }
            }
        },
        “rejectedPlans” : [ ]
    },
    “serverInfo” : {
        “host” : “localhost”,
        “port” : 9552,
        “version” : “3.2.3”,
        “gitVersion” : “b326ba837cf6f49d65c2f85e1b70f6f31ece7937”
    },
    “ok” : 1
}&lt;/p&gt;

&lt;p&gt;一、存储引擎（Storage）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mongodb 3.0默认存储引擎为MMAPV1，还有一个新引擎wiredTiger可选，或许可以提高一定的性能。

mongodb中有多个databases，每个database可以创建多个collections，collection是底层数据分区（partition）的单位，每个collection都有多个底层的数据文件组成。（参见下文data files存储原理）

 

wiredTiger引擎：3.0新增引擎，官方宣称在read、insert和复杂的update下具有更高的性能。所以后续版本，我们建议使用wiredTiger。所有的write请求都基于“文档级别”的lock，因此多个客户端可以同时更新一个colleciton中的不同文档，这种更细颗粒度的lock，可以支撑更高的读写负载和并发量。因为对于production环境，更多的CPU可以有效提升wireTiger的性能，因为它是的IO是多线程的。wiredTiger不像MMAPV1引擎那样尽可能的耗尽内存，它可以通过在配置文件中指定“cacheSizeGB”参数设定引擎使用的内存量，此内存用于缓存工作集数据（索引、namespace，未提交的write，query缓冲等）。

journal就是一个预写事务日志，来确保数据的持久性，wiredTiger每隔60秒（默认）或者待写入的数据达到2G时，mongodb将对journal文件提交一个checkpoint（检测点，将内存中的数据变更flush到磁盘中的数据文件中，并做一个标记点，表示此前的数据表示已经持久存储在了数据文件中，此后的数据变更存在于内存和journal日志）。对于write操作，首先被持久写入journal，然后在内存中保存变更数据，条件满足后提交一个新的检测点，即检测点之前的数据只是在journal中持久存储，但并没有在mongodb的数据文件中持久化，延迟持久化可以提升磁盘效率，如果在提交checkpoint之前，mongodb异常退出，此后再次启动可以根据journal日志恢复数据。journal日志默认每个100毫秒同步磁盘一次，每100M数据生成一个新的journal文件，journal默认使用了snappy压缩，检测点创建后，此前的journal日志即可清除。mongod可以禁用journal，这在一定程度上可以降低它带来的开支；对于单点mongod，关闭journal可能会在异常关闭时丢失checkpoint之间的数据（那些尚未提交到磁盘数据文件的数据）；对于replica set架构，持久性的保证稍高，但仍然不能保证绝对的安全（比如replica set中所有节点几乎同时退出时）。



 

MMAPv1引擎：mongodb原生的存储引擎，比较简单，直接使用系统级的内存映射文件机制（memory mapped files），一直是mongodb的默认存储引擎，对于insert、read和in-place update（update不导致文档的size变大）性能较高；不过MMAPV1在lock的并发级别上，支持到collection级别，所以对于同一个collection同时只能有一个write操作执行，这一点相对于wiredTiger而言，在write并发性上就稍弱一些。对于production环境而言，较大的内存可以使此引擎更加高效，有效减少“page fault”频率，但是因为其并发级别的限制，多核CPU并不能使其受益。此引擎将不会使用到swap空间，但是对于wiredTiger而言需要一定的swap空间。（核心：对于大文件MAP操作，比较忌讳的就是在文件的中间修改数据，而且导致文件长度增长，这会涉及到索引引用的大面积调整）

 

为了确保数据的安全性，mongodb将所有的变更操作写入journal并间歇性的持久到磁盘上，对于实际数据文件将延迟写入，和wiredTiger一样journal也是用于数据恢复。所有的记录在磁盘上连续存储，当一个document尺寸变大时，mongodb需要重新分配一个新的记录（旧的record标记删除，新的记record在文件尾部重新分配空间），这意味着mongodb同时还需要更新此文档的索引（指向新的record的offset），与in-place update相比，将消耗更多的时间和存储开支。由此可见，如果你的mongodb的使用场景中有大量的这种update，那么或许MMAPv1引擎并不太适合，同时也反映出如果document没有索引，是无法保证document在read中的顺序（即自然顺序）。3.0之后，mongodb默认采用“Power of 2 Sized Allocations”，所以每个document对应的record将有实际数据和一些padding组成，这padding可以允许document的尺寸在update时适度的增长，以最小化重新分配record的可能性。此外重新分配空间，也会导致磁盘碎片（旧的record空间）。

 

Power of 2 Sized Allocations：默认情况下，MMAPv1中空间分配使用此策略，每个document的size是2的次幂，比如32、64、128、256...2MB，如果文档尺寸大于2MB，则空间为2MB的倍数（2M,4M,6M等）。这种策略有2种优势，首先那些删除或者update变大而产生的磁盘碎片空间（尺寸变大，意味着开辟新空间存储此document，旧的空间被mark为deleted）可以被其他insert重用，再者padding可以允许文档尺寸有限度的增长，而无需每次update变大都重新分配空间。此外，mongodb还提供了一个可选的“No padding Allocation”策略（即按照实际数据尺寸分配空间），如果你确信数据绝大多数情况下都是insert、in-place update，极少的delete，此策略将可以有效的节约磁盘空间，看起来数据更加紧凑，磁盘利用率也更高。

 

备注：mongodb 3.2+之后，默认的存储引擎为“wiredTiger”，大量优化了存储性能，建议升级到3.2+版本。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;二、Capped Collections：一种特殊的collection，其尺寸大小是固定值，类似于一个可循环使用的buffer，如果空间被填满之后，新的插入将会覆盖最旧的文档，我们通常不会对Capped进行删除或者update操作，所以这种类型的collection能够支撑较高的write和read，通常情况下我们不需要对这种collection构建索引，因为insert是append（insert的数据保存是严格有序的）、read是iterator方式，几乎没有随机读；在replica set模式下，其oplog就是使用这种colleciton实现的。    Capped Collection的设计目的就是用来保存“最近的”一定尺寸的document。&lt;/p&gt;

&lt;p&gt;Java代码  收藏代码
db.createCollection(“capped_collections”,new CreateCollectionOptions()&lt;br /&gt;
                .capped(true)&lt;br /&gt;
                .maxDocuments(6552350)&lt;br /&gt;
                .usePowerOf2Sizes(false).autoIndex(true));//不会涉及到更新，所以可以不用power of 2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Capped Collection在语义上，类似于“FIFO”队列，而且是有界队列。适用于数据缓存，消息类型的存储。Capped支持update，但是我们通常不建议，如果更新导致document的尺寸变大，操作将会失败，只能使用in-place update，而且还需要建立合适的索引。在capped中使用remove操作是允许的。autoIndex属性表示默认对_id字段建立索引，我们推荐这么做。在上文中我们提到了Tailable Cursor，就是为Capped而设计的，效果类似于“tail -f ”。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;三、数据模型（Data Model）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;上文已经描述过，mongodb是一个模式自由的NOSQL，不像其他RDBMS一样需要预先定义Schema而且所有的数据都“整齐划一”，mongodb的document是BSON格式，松散的，原则上说任何一个Colleciton都可以保存任意结构的document，甚至它们的格式千差万别，不过从应用角度考虑，包括业务数据分类和查询优化机制等，我们仍然建议每个colleciton中的document数据结构应该比较接近。

对于有些update，比如对array新增元素等，会导致document尺寸的增加，无论任何存储系统包括MYSQL、Hbase等，对于这种情况都需要额外的考虑，这归结于磁盘空间的分配是连续的（连续意味着读取性能将更高，存储文件空间通常是预分配固定尺寸，我们需要尽可能的利用磁盘IO的这种优势）。对于MMAPV1引擎，如果文档尺寸超过了原分配的空间（上文提到Power of 2 Allocate），mongodb将会重新分配新的空间来保存整个文档（旧文档空间回收，可以被后续的insert重用）。

 

document模型的设计与存储，需要兼顾应用的实际需要，否则可能会影响性能。mongodb支持内嵌document，即document中一个字段的值也是一个document，可以形成类似于RDBMS中的“one-to-one”、“one-to-many”，只需要对reference作为一个内嵌文档保存即可。这种情况就需要考虑mongodb存储引擎的机制了，如果你的内嵌文档（即reference文档）尺寸是动态的，比如一个user可以有多个card，因为card数量无法预估，这就会导致document的尺寸可能不断增加以至于超过“Power of 2 Allocate”，从而触发空间重新分配，带来性能开销，这种情况下，我们需要将内嵌文档单独保存到一个额外的collection中，作为一个或者多个document存储，比如把card列表保存在card collection中。“one-to-one”的情况也需要个别考虑，如果reference文档尺寸较小，可以内嵌，如果尺寸较大，建议单独存储。此外内嵌文档还有个优点就是write的原子性，如果使用reference的话，就无法保证了。

 

索引：提高查询性能，默认情况下_id字段会被创建唯一索引；因为索引不仅需要占用大量内存而且也会占用磁盘，所以我们需要建立有限个索引，而且最好不要建立重复索引；每个索引需要8KB的空间，同时update、insert操作会导致索引的调整，会稍微影响write的性能，索引只能使read操作收益，所以读写比高的应用可以考虑建立索引。

 

大集合拆分：比如一个用于存储log的collection，log分为有两种“dev”、“debug”，结果大致为{&quot;log&quot;:&quot;dev&quot;,&quot;content&quot;:&quot;....&quot;},{&quot;log&quot;:&quot;debug&quot;,&quot;content&quot;:&quot;.....&quot;}。这两种日志的document个数比较接近，对于查询时，即使给log字段建立索引，这个索引也不是高效的，所以可以考虑将它们分别放在2个Collection中，比如：log_dev和log_debug。

 

数据生命周期管理：mongodb提供了expire机制，即可以指定文档保存的时长，过期后自动删除，即TTL特性，这个特性在很多场合将是非常有用的，比如“验证码保留15分钟有效期”、“消息保存7天”等等，mongodb会启动一个后台线程来删除那些过期的document。需要对一个日期字段创建“TTL索引”，比如插入一个文档：{&quot;check_code&quot;:&quot;101010&quot;,$currentDate:{&quot;created&quot;:true}}}，其中created字段默认值为系统时间Date；然后我们对created字段建立TTL索引：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java代码  收藏代码
collection.createIndex(new Document(“created”,1),new IndexOptions().expireAfter(15L,TimeUnit.MILLISECONDS));//15分钟&lt;br /&gt;
    我们向collection中insert文档时，created的时间为系统当前时间，其中在creatd字段上建立了“TTL”索引，索引TTL为15分钟，mongodb后台线程将会扫描并检测每条document的（created时间 + 15分钟）与当前时间比较，如果发现过期，则删除索引条目（连带删除document）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;某些情况下，我们可能需要实现“在某个指定的时刻过期”，我们只需要将上述文档和索引变通改造即可，即created指定为“目标时间”，expiredAfter指定为0。 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;四、架构模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Replica set：复制集，mongodb的架构方式之一 ，通常是三个对等的节点构成一个“复制集”集群，有“primary”和secondary等多中角色（稍后详细介绍），其中primary负责读写请求，secondary可以负责读请求，这有配置决定，其中secondary紧跟primary并应用write操作；如果primay失效，则集群进行“多数派”选举，选举出新的primary，即failover机制，即HA架构。复制集解决了单点故障问题，也是mongodb垂直扩展的最小部署单位，当然sharding cluster中每个shard节点也可以使用Replica set提高数据可用性。

 

Sharding cluster：分片集群，数据水平扩展的手段之一；replica set这种架构的缺点就是“集群数据容量”受限于单个节点的磁盘大小，如果数据量不断增加，对它进行扩容将时非常苦难的事情，所以我们需要采用Sharding模式来解决这个问题。将整个collection的数据将根据sharding key被sharding到多个mongod节点上，即每个节点持有collection的一部分数据，这个集群持有全部数据，原则上sharding可以支撑数TB的数据。

 

系统配置：1）建议mongodb部署在linux系统上，较高版本，选择合适的底层文件系统（ext4），开启合适的swap空间  2）无论是MMAPV1或者wiredTiger引擎，较大的内存总能带来直接收益。3）对数据存储文件关闭“atime”（文件每次access都会更改这个时间值，表示文件最近被访问的时间），可以提升文件访问效率。 4）ulimit参数调整，这个在基于网络IO或者磁盘IO操作的应用中，通常都会调整，上调系统允许打开的文件个数（ulimit -n 65535）。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;五、数据文件存储原理（Data Files storage，MMAPV1引擎）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、Data Files

mongodb的数据将会保存在底层文件系统中，比如我们dbpath设定为“/data/db”目录，我们创建一个database为“test”，collection为“sample”，然后在此collection中插入数条documents。我们查看dbpath下生成的文件列表：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java代码  收藏代码&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;ls -lh&lt;br /&gt;
-rw——-  1 mongo  mongo    16M 11  6 17:24 test.0&lt;br /&gt;
-rw——-  1 mongo  mongo    32M 11  6 17:24 test.1&lt;br /&gt;
-rw——-  1 mongo  mongo    64M 11  6 17:24 test.2&lt;br /&gt;
-rw——-  1 mongo  mongo   128M 11  6 17:24 test.3&lt;br /&gt;
-rw——-  1 mongo  mongo   256M 11  6 17:24 test.4&lt;br /&gt;
-rw——-  1 mongo  mongo   512M 11  6 17:24 test.5&lt;br /&gt;
-rw——-  1 mongo  mongo   512M 11  6 17:24 test.6&lt;br /&gt;
-rw——-  1 mongo  mongo    16M 11  6 17:24 test.ns&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;可以看到test这个数据库目前已经有6个数据文件（data files），每个文件以“database”的名字 + 序列数字组成，序列号从0开始，逐个递增，数据文件从16M开始，每次扩张一倍（16M、32M、64M、128M...），在默认情况下单个data file的最大尺寸为2G，如果设置了smallFiles属性（配置文件中）则最大限定为512M；mongodb中每个database最多支持16000个数据文件，即约32T，如果设置了smallFiles则单个database的最大数据量为8T。如果你的database中的数据文件很多，可以使用directoryPerDB配置项将每个db的数据文件放置在各自的目录中。当最后一个data file有数据写入后，mongodb将会立即预分配下一个data file，可以通过“--nopreallocate”启动命令参数来关闭此选项。

 

一个database中所有的collections以及索引信息会分散存储在多个数据文件中，即mongodb并没有像SQL数据库那样，每个表的数据、索引分别存储；数据分块的单位为extent（范围，区域），即一个data file中有多个extents组成，extent中可以保存collection数据或者indexes数据，一个extent只能保存同一个collection数据，不同的collections数据分布在不同的extents中，indexes数据也保存在各自的extents中；最终，一个collection有一个或者多个extents构成，最小size为8K，最大可以为2G，依次增大；它们分散在多个data files中。对于一个data file而言，可能包含多个collection的数据，即有多个不同collections的extents、index extents混合构成。每个extent包含多条documents（或者index entries），每个extent的大小可能不相等，但一个extent不会跨越2个data files。

 



 

有人肯定疑问：一个collection中有哪些extents，这种信息mongodb存在哪里？在每个database的namespace文件中，比如test.ns文件中，每个collection只保存了第一个extent的位置信息，并不保存所有的extents列表，但每个extent都维护者一个链表关系，即每个extent都在其header信息中记录了此extent的上一个、下一个extent的位置信息，这样当对此collection进行scan操作时（比如全表扫描），可以提供很大的便利性。

 

我们可以通过db.stats()指令查看当前database中extents的信息：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java代码  收藏代码&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;use test&lt;br /&gt;
switched to db test&lt;br /&gt;
db.stats();&lt;br /&gt;
{&lt;br /&gt;
    “db” : “test”,&lt;br /&gt;
    “collections” : 3,  ##collection的个数&lt;br /&gt;
    “objects” : 1000006, ##documents总条数&lt;br /&gt;
    “avgObjSize” : 495.9974400153599, ##record的平均大小，单位byte&lt;br /&gt;
    “dataSize” : 496000416, ##document所占空间的总量&lt;br /&gt;
    “storageSize” : 629649408, ##&lt;br /&gt;
    “numExtents” : 18,  ##extents个数&lt;br /&gt;
    “indexes” : 2,&lt;br /&gt;
    “indexSize” : 108282944,&lt;br /&gt;
    “fileSize” : 1006632960,&lt;br /&gt;
    “nsSizeMB” : 16, ##namespace文件大小&lt;br /&gt;
    “extentFreeList” : {   ##尚未使用（已分配尚未使用、已删除但尚未被重用）的extent列表&lt;br /&gt;
        “num” : 0,&lt;br /&gt;
        “totalSize” : 0&lt;br /&gt;
    },&lt;br /&gt;
    “dataFileVersion” : {&lt;br /&gt;
        “major” : 4,&lt;br /&gt;
        “minor” : 22&lt;br /&gt;
    },&lt;br /&gt;
    “ok” : 1&lt;br /&gt;
}&lt;br /&gt;
    列表信息中有几个字段简单介绍一下：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1） dataSize：documents所占的空间总量，mongodb将会为每个document分配一定空间用于保存数据，每个document所占空间包括“文档实际大小” + “padding”，对于MMAPV1引擎，mongodb默认采用了“Power of 2 Sized Allocations”策略，这也意味着通常会有padding，不过如果你的document不会被update（或者update为in-place方式，不会导致文档尺寸变大），可以在在createCollection是指定noPadding属性为true，这样dataSize的大小就是documents实际大小；当documents被删除后，将导致dataSize减小；不过如果在原有document的空间内（包括其padding空间）update（或者replace），则不会导致dataSize的变大，因为mongodb并没有分配任何新的document空间。

2）storageSize：所有collection的documents占用总空间，包括那些已经删除的documents所占的空间，为存储documents的extents所占空间总和。文档的删除或者收缩不会导致storageSize变小。

3）indexSize：所用collection的索引数据的大小，为存储indexes的extents所占空间的总和。

4）fileSize：为底层所有data files的大小总和，但不包括namespace文件。为storageSize、indexSize、以及一些尚未使用的空间等等。当删除database、collections时会导致此值变小。

 

此外，如果你想查看一个collection中extents的分配情况，可以使用db.&amp;lt;collection名称&amp;gt;.stats()，结构与上述类似；如果你希望更细致的了解collection中extents的全部信息，则可以使用db.&amp;lt;collection名称&amp;gt;.validate()，此方法接收一个boolean值，表示是否查看明细，这个指令会scan全部的data files，因此比较耗时：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java代码  收藏代码&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;db.sample.validate(true);&lt;br /&gt;
{&lt;br /&gt;
    “ns” : “test.sample”,&lt;br /&gt;
    “datasize” : 496000000,&lt;br /&gt;
    “nrecords” : 1000000,&lt;br /&gt;
    “lastExtentSize” : 168742912,&lt;br /&gt;
    “firstExtent” : “0:5000 ns:test.sample”,&lt;br /&gt;
    “lastExtent” : “3:a05f000 ns:test.sample”,&lt;br /&gt;
    “extentCount” : 16,&lt;br /&gt;
    “extents” : [&lt;br /&gt;
        {&lt;br /&gt;
            “loc” : “0:5000”,&lt;br /&gt;
            “xnext” : “0:49000”,&lt;br /&gt;
            “xprev” : “null”,&lt;br /&gt;
            “nsdiag” : “test.sample”,&lt;br /&gt;
            “size” : 8192,&lt;br /&gt;
            “firstRecord” : “0:50b0”,&lt;br /&gt;
            “lastRecord” : “0:6cb0”&lt;br /&gt;
        },&lt;br /&gt;
        …&lt;br /&gt;
        ]&lt;br /&gt;
        …&lt;br /&gt;
}&lt;br /&gt;
    可以看到extents在逻辑上是链表形式，以及每个extent的数据量、以及所在data file的offset位置。具体参见【validate方法】&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;从上文中我们已经得知，删除document会导致磁盘碎片，有些update也会导致磁盘碎片，比如update导致文档尺寸变大，进而超过原来分配的空间；当有新的insert操作时，mongodb会检测现有的extents中是否合适的碎片空间可以被重用，如果有，则重用这些fragment，否则分配新的存储空间。磁盘碎片，对write操作有一定的性能影响，而且会导致磁盘空间浪费；如果你需要删除某个collection中大部分数据，则可以考虑将有效数据先转存到新的collection，然后直接drop()原有的collection。或者使用db.runCommand({compact: '&amp;lt;collection&amp;gt;'})。

如果你的database已经运行一段时间，数据已经有很大的磁盘碎片（storageSize与dataSize比较），可以通过mongodump将指定database的所有数据导出，然后将原有的db删除，再通过mongorestore指令将数据重新导入。（同compact，这种操作需要停机维护）

 

mongod中还有2个默认的database，系统级的，“admin”和“local”；它们的存储原理同上，其中“admin”用于存储“用户授权信息”，比如每个database中用户的role、权限等；“local”即为本地数据库，我们常说的oplog（replication架构中使用，类似与binlog）即保存在此数据库中。

 

2、Namespace文件

对于namespace文件，比如“test.ns”文件，默认大小为16M，此文件中主要用于保存“collection”、index的命名信息，比如collection的“属性”信息、每个索引的属性类型等，如果你的database中需要存储大量的collection（比如每一小时生成一个collection，在数据分析应用中），那么我们可以通过配置文件“nsSize”选项来指定。参见【mongodb配置文件】

 

3、journal文件

journal日志为mongodb提供了数据保障能力，它本质上与mysql binlog没有太大区别，用于当mongodb异常crash后，重启时进行数据恢复；这归结于mongodb的数据持久写入磁盘是滞后的。默认情况下，“journal”特性是开启的，特别在production环境中，我们没有理由来关闭它。（除非，数据丢失对应用而言，是无关紧要的）

 

一个mongodb实例中所有的databases共享journal文件。

 

对于write操作而言，首先写入journal日志，然后将数据在内存中修改（mmap），此后后台线程间歇性的将内存中变更的数据flush到底层的data files中，时间间隔为60秒（参见配置项“syncPeriodSecs”）；write操作在journal文件中是有序的，为了提升性能，write将会首先写入journal日志的内存buffer中，当buffer数据达到100M或者每隔100毫秒，buffer中的数据将会flush到磁盘中的journal文件中；如果mongodb异常退出，将可能导致最多100M数据或者最近100ms内的数据丢失，flush磁盘的时间间隔有配置项“commitIntervalMs”决定，默认为100毫秒。mongodb之所以不能对每个write都将journal同步磁盘，这也是对性能的考虑，mysql的binlog也采用了类似的权衡方式。开启journal日志功能，将会导致write性能有所降低，可能降低5~30%，因为它直接加剧了磁盘的写入负载，我们可以将journal日志单独放置在其他磁盘驱动器中来提高写入并发能力（与data files分别使用不同的磁盘驱动器）。

 

如果你希望数据尽可能的不丢失，可以考虑：1）减小commitIntervalMs的值 2）每个write指定“write concern”中指定“j”参数为true  3）最佳手段就是采用“replica set”架构模式，通过数据备份方式解决，同时还需要在“write concern”中指定“w”选项，且保障级别不低于“majority”。【参见mongodb复制集】最终我们需要在“写入性能”和“数据一致性”两个方面权衡，即CAP理论。

 

根据write并发量，journal日志文件为1G，如果指定了smallFiles配置项，则最大为128M，和data files一样journal文件也采用了“preallocated”方式，journal日志保存在dbpath下“journal”子目录中，一般会有三个journal文件，每个journal文件格式类似于“j._&amp;lt;序列数字&amp;gt;”。并不是每次buffer flush都生成一个新的journal日志，而是当前journal文件即将满时会预创建一个新的文件，journal文件中保存了write操作的记录，每条记录中包含write操作内容之外，还包含一个“lsn”（last sequence number），表示此记录的ID；此外我们会发现在journal目录下，还有一个“lsn”文件，这个文件非常小，只保存了一个数字，当write变更的数据被flush到磁盘中的data files后，也意味着这些数据已经持久化了，那么它们在“异常恢复”时也不需要了，那么其对应的journal日志将可以删除，“lsn”文件中记录的就是write持久化的最后一个journal记录的ID，此ID之前的write操作已经被持久写入data files，此ID之前的journal在“异常恢复”时则不需要关注；如果某个journal文件中最大 ID小于“lsn”，则此journal可以被删除或者重用。
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Wed, 24 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/24/mongodb.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/24/mongodb.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>倒排索引</title>
        <description>&lt;p&gt;倒排索引源于实际应用中需要根据属性的值来查找记录。这种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。由于不是由记录来确定属性值，而是由属性值来确定记录的位置，因而称为倒排索引(inverted index)。带有倒排索引的文件我们称为倒排索引文件，简称倒排文件(inverted file)。
在关系数据库系统里，索引[1]  是检索数据最有效率的方式,。但对于搜索引擎，它并不能满足其特殊要求：
1）海量数据：搜索引擎面对的是海量数据，像Google，百度这样大型的商业搜索引擎索引都是亿级甚至百亿级的网页数量 ，面对如此海量数据 ,使得数据库系统很难有效的管理。
2）数据操作简单：搜索引擎使用的数据操作简单 ,一般而言 ,只需要增、 删、 改、 查几个功能 ,而且数据都有特定的格式 ,可以针对这些应用设计出简单高效的应用程序。而一般的数据库系统则支持大而全的功能 ,同时损失了速度和空间。最后 ,搜索引擎面临大量的用户检索需求 ,这要求搜索引擎在检索程序的设计上要分秒必争 ,尽可能的将大运算量的工作在索引建立时完成 ,使检索运算尽量的少。一般的数据库系统很难承受如此大量的用户请求 ,而且在检索响应时间和检索并发度上都不及我们专门设计的索引系统。
倒排列表
倒排列表用来记录有哪些文档包含了某个单词。一般在文档集合里会有很多文档包含某个单词，每个文档会记录文档编号（DocID），单词在这个文档中出现的次数（TF）及单词在文档中哪些位置出现过等信息，这样与一个文档相关的信息被称做倒排索引项（Posting），包含这个单词的一系列倒排索引项形成了列表结构，这就是某个单词对应的倒排列表。右图是倒排列表的示意图，在文档集合中出现过的所有单词及其对应的倒排列表组成了倒排索引。
在实际的搜索引擎系统中，并不存储倒排索引项中的实际文档编号，而是代之以文档编号差值（D-Gap）。文档编号差值是倒排列表中相邻的两个倒排索引项文档编号的差值，一般在索引构建过程中，可以保证倒排列表中后面出现的文档编号大于之前出现的文档编号，所以文档编号差值总是大于0的整数。如图2所示的例子中，原始的 3个文档编号分别是187、196和199，通过编号差值计算，在实际存储的时候就转化成了：187、9、3。
之所以要对文档编号进行差值计算，主要原因是为了更好地对数据进行压缩，原始文档编号一般都是大数值，通过差值计算，就有效地将大数值转换为了小数值，而这有助于增加数据的压缩率。
倒排索引
倒排索引[2]  （英语：Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。
　　
倒排索引
倒排索引
倒排索引[2]  有两种不同的反向索引形式：
　　一条记录的水平反向索引（或者反向档案索引）包含每个引用单词的文档的列表。
　　一个单词的水平反向索引（或者完全反向索引）又包含每个单词在一个文档中的位置。
　　后者的形式提供了更多的兼容性（比如短语搜索），但是需要更多的时间和空间来创建。
　　现代搜索引擎的索引[3]  都是基于倒排索引。相比“签名文件”、“后缀树”等索引结构，“倒排索引”是实现单词到文档映射关系的最佳实现方式和最有效的索引结构.
构建方法编辑
简单法
索引的构建[4]  相当于从正排表到倒排表的建立过程。当我们分析完网页时 ,得到的是以网页为主码的索引表。当索引建立完成后 ,应得到倒排表 ,具体流程如图所示：
流程描述如下：
1）将文档分析成单词term标记，
2）使用hash去重单词term
　　3）对单词生成倒排列表
　　倒排列表就是文档编号DocID，没有包含其他的信息（如词频，单词位置等），这就是简单的索引。
　　这个简单索引功能可以用于小数据，例如索引几千个文档。然而它有两点限制：
　　1）需要有足够的内存来存储倒排表，对于搜索引擎来说， 都是G级别数据，特别是当规模不断扩大时 ,我们根本不可能提供这么多的内存。
　　2）算法是顺序执行，不便于并行处理。
合并法
归并法[4]  ,即每次将内存中数据写入磁盘时，包括词典在内的所有中间结果信息都被写入磁盘，这样内存所有内容都可以被清空，后续建立索引可以使用全部的定额内存。
归并索引
归并索引
如图 归并示意图：
合并流程：
1）页面分析，生成临时倒排数据索引A，B，当临时倒排数据索引A，B占满内存后，将内存索引A，B写入临时文件生成临时倒排文件，
　　2) 对生成的多个临时倒排文件 ,执行多路归并 ,输出得到最终的倒排文件 ( inverted file)。
合并流程
合并流程
索引创建过程中的页面分析 ,特别是中文分词为主要时间开销。算法的第二步相对很快。这样创建算法的优化集中在中文分词效率上。
更新策略编辑
更新策略有四种[2]  ：完全重建、再合并策略、原地更新策略以及混合策略。
完全重建策略：当新增文档到达一定数量，将新增文档和原先的老文档整合，然后利用静态索引创建方法对所有文档重建索引，新索引建立完成后老索引会被遗弃。此法代价高，但是主流商业搜索引擎一般是采用此方式来维护索引的更新（这句话是书中原话）
再合并策略：当新增文档进入系统，解析文档，之后更新内存中维护的临时索引，文档中出现的每个单词，在其倒排表列表末尾追加倒排表列表项；一旦临时索引将指定内存消耗光，即进行一次索引合并，这里需要倒排文件里的倒排列表存放顺序已经按照索引单词字典顺序由低到高排序，这样直接顺序扫描合并即可。其缺点是：因为要生成新的倒排索引文件，所以对老索引中的很多单词，尽管其在倒排列表并未发生任何变化，也需要将其从老索引中取出来并写入新索引中，这样对磁盘消耗是没必要的。
原地更新策略：试图改进再合并策略，在原地合并倒排表，这需要提前分配一定的空间给未来插入，如果提前分配的空间不够了需要迁移。实际显示，其索引更新的效率比再合并策略要低。
混合策略：出发点是能够结合不同索引更新策略的长处，将不同索引更新策略混合，以形成更高效的方法。&lt;/p&gt;

&lt;p&gt;在搜索引擎中每个文件都对应一个文件ID，文件内容被表示为一系列关键词的集合（实际上在搜索引擎索引库中，关键词也已经转换为关键词ID）。例如“文档1”经过分词，提取了20个关键词，每个关键词都会记录它在文档中的出现次数和出现位置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 得到正向索引的结构如下：

   “文档1”的ID &amp;gt; 单词1：出现次数，出现位置列表；单词2：出现次数，出现位置列表；…………。

   “文档2”的ID &amp;gt; 此文档出现的关键词列表。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　一般是通过key，去找value。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  当用户在主页上搜索关键词“华为手机”时，假设只存在正向索引（forward index），那么就需要扫描索引库中的所有文档，找出所有包含关键词“华为手机”的文档，再根据打分模型进行打分，排出名次后呈现给用户。因为互联网上收录在搜索引擎中的文档的数目是个天文数字，这样的索引结构根本无法满足实时返回排名结果的要求。

   所以，搜索引擎会将正向索引重新构建为倒排索引，即把文件ID对应到关键词的映射转换为关键词到文件ID的映射，每个关键词都对应着一系列的文件，这些文件中都出现这个关键词。

   得到倒排索引的结构如下：

   “关键词1”：“文档1”的ID，“文档2”的ID，…………。

   “关键词2”：带有此关键词的文档ID列表。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　从词的关键字，去找文档。&lt;/p&gt;

&lt;p&gt;1.单词——文档矩阵
      单词-文档矩阵是表达两者之间所具有的一种包含关系的概念模型，图1展示了其含义。图3-1的每列代表一个文档，每行代表一个单词，打对勾的位置代表包含关系。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 从纵向即文档这个维度来看，每列代表文档包含了哪些单词，比如文档1包含了词汇1和词汇4，而不包含其它单词。从横向即单词这个维度来看，每行代表了哪些文档包含了某个单词。比如对于词汇1来说，文档1和文档4中出现过单词1，而其它文档不包含词汇1。矩阵中其它的行列也可作此种解读。

搜索引擎的索引其实就是实现“单词-文档矩阵”的具体数据结构。可以有不同的方式来实现上述概念模型，比如“倒排索引”、“签名文件”、“后缀树”等方式。但是各项实验数据表明，“倒排索引”是实现单词到文档映射关系的最佳实现方式，所以本博文主要介绍“倒排索引”的技术细节。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.倒排索引基本概念
       文档(Document)：一般搜索引擎的处理对象是互联网网页，而文档这个概念要更宽泛些，代表以文本形式存在的存储对象，相比网页来说，涵盖更多种形式，比如Word，PDF，html，XML等不同格式的文件都可以称之为文档。再比如一封邮件，一条短信，一条微博也可以称之为文档。在本书后续内容，很多情况下会使用文档来表征文本信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   文档集合(Document Collection)：由若干文档构成的集合称之为文档集合。比如海量的互联网网页或者说大量的电子邮件都是文档集合的具体例子。

   文档编号(Document ID)：在搜索引擎内部，会将文档集合内每个文档赋予一个唯一的内部编号，以此编号来作为这个文档的唯一标识，这样方便内部处理，每个文档的内部编号即称之为“文档编号”，后文有时会用DocID来便捷地代表文档编号。

   单词编号(Word ID)：与文档编号类似，搜索引擎内部以唯一的编号来表征某个单词，单词编号可以作为某个单词的唯一表征。

   倒排索引(Inverted Index)：倒排索引是实现“单词-文档矩阵”的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。

   单词词典(Lexicon)：搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。

   倒排列表(PostingList)：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。

   倒排文件(Inverted File)：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件即被称之为倒排文件，倒排文件是存储倒排索引的物理文件。

 关于这些概念之间的关系，通过图2可以比较清晰的看出来。 3.倒排索引简单实例
  倒排索引从逻辑结构和基本思路上来讲非常简单。下面我们通过具体实例来进行说明，使得读者能够对倒排索引有一个宏观而直接的感受。

  假设文档集合包含五个文档，每个文档内容如图3所示，在图中最左端一栏是每个文档对应的文档编号。我们的任务就是对这个文档集合建立倒排索引。 　　中文和英文等语言不同，单词之间没有明确分隔符号，所以首先要用分词系统将文档自动切分成单词序列。这样每个文档就转换为由单词序列构成的数据流，为了系统后续处理方便，需要对每个不同的单词赋予唯一的单词编号，同时记录下哪些文档包含这个单词，在如此处理结束后，我们可以得到最简单的倒排索引（参考图3-4）。在图4中，“单词ID”一栏记录了每个单词的单词编号，第二栏是对应的单词，第三栏即每个单词对应的倒排列表。比如单词“谷歌”，其单词编号为1，倒排列表为{1,2,3,4,5}，说明文档集合中每个文档都包含了这个单词。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　　　　　　　　　　　　　&lt;/p&gt;

&lt;p&gt;　　　　　　　　　　　　　　　　　　　　　　　　　　　　图4   简单的倒排索引&lt;/p&gt;

&lt;p&gt;　　之所以说图4所示倒排索引是最简单的，是因为这个索引系统只记载了哪些文档包含某个单词，而事实上，索引系统还可以记录除此之外的更多信息。图5是一个相对复杂些的倒排索引，与图4的基本索引系统比，在单词对应的倒排列表中不仅记录了文档编号，还记载了单词频率信息（TF），即这个单词在某个文档中的出现次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是很重要的一个计算因子，所以将其记录在倒排列表中，以方便后续排序时进行分值计算。在图5的例子里，单词“创始人”的单词编号为7，对应的倒排列表内容为：（3:1），其中的3代表文档编号为3的文档包含这个单词，数字1代表词频信息，即这个单词在3号文档中只出现过1次，其它单词对应的倒排列表所代表含义与此相同。&lt;/p&gt;

&lt;p&gt;　　　　　　　　　　　　　　&lt;/p&gt;

&lt;p&gt;　　　　　　　　　　　　　　　　　　　　　　　　　　　　图 5 带有单词频率信息的倒排索引&lt;/p&gt;

&lt;p&gt;　　实用的倒排索引还可以记载更多的信息，图6所示索引系统除了记录文档编号和单词频率信息外，额外记载了两类信息，即每个单词对应的“文档频率信息”（对应图6的第三栏）以及在倒排列表中记录单词在某个文档出现的位置信息。&lt;/p&gt;

&lt;p&gt;　　　　　　　　　　　　　　　　　　&lt;/p&gt;

&lt;p&gt;　　　　　　　　　　　　　　　　　　　　　　图6   带有单词频率、文档频率和出现位置信息的倒排索引&lt;/p&gt;

&lt;p&gt;“文档频率信息”代表了在文档集合中有多少个文档包含某个单词，之所以要记录这个信息，其原因与单词频率信息一样，这个信息在搜索结果排序计算中是非常重要的一个因子。而单词在某个文档中出现的位置信息并非索引系统一定要记录的，在实际的索引系统里可以包含，也可以选择不包含这个信息，之所以如此，因为这个信息对于搜索系统来说并非必需的，位置信息只有在支持“短语查询”的时候才能够派上用场。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 以单词“拉斯”为例，其单词编号为8，文档频率为2，代表整个文档集合中有两个文档包含这个单词，对应的倒排列表为：{(3;1;&amp;lt;4&amp;gt;)，(5;1;&amp;lt;4&amp;gt;)},其含义为在文档3和文档5出现过这个单词，单词频率都为1，单词“拉斯”在两个文档中的出现位置都是4，即文档中第四个单词是“拉斯”。

 图6所示倒排索引已经是一个非常完备的索引系统，实际搜索系统的索引结构基本如此，区别无非是采取哪些具体的数据结构来实现上述逻辑结构。

 有了这个索引系统，搜索引擎可以很方便地响应用户的查询，比如用户输入查询词“Facebook”，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息、文档频率信息即可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程，具体实现方案本书第五章会做详细描述。
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;单词词典
　　单词词典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里查询，就能够获得相应的倒排列表，并以此作为后续排序的基础。
    对于一个规模很大的文档集合来说，可能包含几十万甚至上百万的不同单词，能否快速定位某个单词，这直接影响搜索时的响应速度，所以需要高效的数据结构来对单词词典进行构建和查找，常用的数据结构包括哈希加链表结构和树形词典结构。
4.1   哈希加链表
    图7是这种词典结构的示意图。这种词典结构主要由两个部分构成：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; 主体部分是哈希表，每个哈希表项保存一个指针，指针指向冲突链表，在冲突链表里，相同哈希值的单词形成链表结构。之所以会有冲突链表，是因为两个不同单词获得相同的哈希值，如果是这样，在哈希方法里被称做是一次冲突，可以将相同哈希值的单词存储在链表里，以供后续查找。
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;　　　　　　　　　　　　　　　　　　　　　　&lt;/p&gt;

&lt;p&gt;　　在建立索引的过程中，词典结构也会相应地被构建出来。比如在解析一个新文档的时候，对于某个在文档中出现的单词T，首先利用哈希函数获得其哈希值，之后根据哈希值对应的哈希表项读取其中保存的指针，就找到了对应的冲突链表。如果冲突链表里已经存在这个单词，说明单词在之前解析的文档里已经出现过。如果在冲突链表里没有发现这个单词，说明该单词是首次碰到，则将其加入冲突链表里。通过这种方式，当文档集合内所有文档解析完毕时，相应的词典结构也就建立起来了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   在响应用户查询请求时，其过程与建立词典类似，不同点在于即使词典里没出现过某个单词，也不会添加到词典内。以图7为例，假设用户输入的查询请求为单词3，对这个单词进行哈希，定位到哈希表内的2号槽，从其保留的指针可以获得冲突链表，依次将单词3和冲突链表内的单词比较，发现单词3在冲突链表内，于是找到这个单词，之后可以读出这个单词对应的倒排列表来进行后续的工作，如果没有找到这个单词，说明文档集合内没有任何文档包含单词，则搜索结果为空。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.2   树形结构
       B树（或者B+树）是另外一种高效查找结构，图8是一个 B树结构示意图。B树与哈希方式查找不同，需要字典项能够按照大小排序（数字或者字符序），而哈希方式则无须数据满足此项要求。
       B树形成了层级查找结构，中间节点用于指出一定顺序范围的词典项目存储在哪个子树中，起到根据词典项比较大小进行导航的作用，最底层的叶子节点存储单词的地址信息，根据这个地址就可以提取出单词字符串。&lt;/p&gt;

&lt;p&gt;单词ID：记录每个单词的单词编号；
单词：对应的单词；
文档频率：代表文档集合中有多少个文档包含某个单词
倒排列表：包含单词ID及其他必要信息
DocId：单词出现的文档id
TF：单词在某个文档中出现的次数
POS：单词在文档中出现的位置
     以单词“加盟”为例，其单词编号为6，文档频率为3，代表整个文档集合中有三个文档包含这个单词，对应的倒排列表为{(2;1;&amp;lt;4&amp;gt;),(3;1;&amp;lt;7&amp;gt;),(5;1;&amp;lt;5&amp;gt;)}，含义是在文档2，3，5出现过这个单词，在每个文档的出现过1次，单词“加盟”在第一个文档的POS是4，即文档的第四个单词是“加盟”，其他的类似。
这个倒排索引已经是一个非常完备的索引系统，实际搜索系统的索引结构基本如此。
Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。&lt;/p&gt;
</description>
        <pubDate>Wed, 24 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/24/inverted_file.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/24/inverted_file.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>npm registry</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;镜像使用方法（三种办法任意一种都能解决问题，建议使用第三种，将配置写死，下次用的时候配置还在）:&lt;/p&gt;

&lt;p&gt;1.通过config命令&lt;/p&gt;

&lt;p&gt;npm config set registry https://registry.npm.taobao.org 
npm info underscore （如果上面配置正确这个命令会有字符串response）
2.命令行指定&lt;/p&gt;

&lt;p&gt;npm –registry https://registry.npm.taobao.org info underscore 
3.编辑 ~/.npmrc 加入下面内容&lt;/p&gt;

&lt;p&gt;registry = https://registry.npm.taobao.org
搜索镜像: https://npm.taobao.org&lt;/p&gt;

&lt;p&gt;建立或使用镜像,参考: https://github.com/cnpm/cnpmjs.org&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/16/npm.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/16/npm.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>制作地图</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;参考：
https://bost.ocks.org/mike/map/
用 D3 和 TopoJSON做一个地图&lt;/p&gt;

&lt;p&gt;1，安装转换工具
brew install gdal
npm install -g topojson@1
检查安装结果
which ogr2ogr
which topojson
2，获取数据
http://www.naturalearthdata.com/
国家：http://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-details/
省：http://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-1-states-provinces/
城市：http://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-populated-places/
除了还有50m和110m数据
3，数据转换
参考：http://calmhawk.iteye.com/blog/2026798
ogr2ogr   -f GeoJSON   -where “ADM0_A3 IN (‘CHN’,’HKG’,’MAC’,’TWN’)”   subunits.json   ne_10m_admin_0_map_subunits/ne_10m_admin_0_map_subunits.shp
ogr2ogr   -f GeoJSON   -where “ISO_A2 = ‘CN’ AND SCALERANK &amp;lt; 8”   places.json   ne_10m_populated_places/ne_10m_populated_places.shp
ogr2ogr   -f GeoJSON   -where “ADM0_A3 = ‘CHN’” cn_province.json ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp
topojson   -o uk.json   –id-property SU_A3   –properties name=NAME   –   subunits.json   places.json cn_province.json&lt;/p&gt;

&lt;p&gt;参考：
https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3
http://blog.csdn.net/zhou452840622/article/details/49058395
中国编号
CHN	 China  TWN	Taiwan, Province of China&lt;br /&gt;
MAC	 Macao   HKG	Hong Kong
CHINA CN&lt;/p&gt;

&lt;p&gt;4，视图转换，大小缩放
参考：https://en.wikipedia.org/wiki/Albers_projection
中国数据：
var projection = d3.geo.albers()&lt;br /&gt;
    .scale(800)&lt;br /&gt;
    .translate([width / 2, height / 2])&lt;br /&gt;
    .rotate([-105, 0])&lt;br /&gt;
    .center([0, 36])&lt;br /&gt;
    .parallels([27, 45]);&lt;/p&gt;

&lt;p&gt;解释：
中间有一个关键的projection,投影.文中的albers projection中文名称叫等积圆锥投影,搜了一下参数,用如下code即可,简单原理就是&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;先放大图形&lt;/li&gt;
  &lt;li&gt;然后转换长宽到自己想要的大小&lt;/li&gt;
  &lt;li&gt;然后旋转使自己的区域所在经度居中,以0°为基准,西经为正数,东经为负数,参数文中中国正中为105°:&lt;/li&gt;
  &lt;li&gt;然后变换中心使自己的区域所在维度居中.&lt;/li&gt;
  &lt;li&gt;标称维度,参数文中为27°和45°.
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/map_cn.png&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;原文翻译：
作为一个公众性的选择，GeoCommons是一个分享地理数据集的平台。集成（integrated）搜索以及简介使得它容易被浏览。但是，尽管这里有很多有用的数据，你应该警惕没有被检验的来源，至少对于新闻业而言。对于直接获取数据来说，它比政府机构或者其他可引用来源来得容易。&lt;/p&gt;

&lt;p&gt;显而易见的，最方便的地理数据来源是Natural Earth，它是制图员Nathaniel Vaughn Kelso以及其他成员的倾力之作。Natural Earth 提供了大量的文化，物理以及栅格数据集。Shapefiles被漂亮的简化为不同的分辨率以便需要对应分辨率的应用使用。我们将使用1:1e7的Natural Earth数据集为这张地图的制作。&lt;/p&gt;

&lt;p&gt;Admin 0 -Details - map subunits
Populated Places&lt;/p&gt;

&lt;p&gt;第一个包括了国家的多边形(polygons)，而第二个则包括了著名地点的名字的位置。这两个文件囊括了整个世界，所以我们的第二部是筛选出我们所需要的数据子集。&lt;/p&gt;

&lt;h1 id=&quot;安装工具&quot;&gt;安装工具&lt;/h1&gt;

&lt;p&gt;地理数据文件对于手动清理和转换来说来说几乎总是工作量太大。幸运的是，存在一个充满活力的地理开源社区，其提供了很多强大而且免费的工具来进行数据的操纵与格式转换。&lt;/p&gt;

&lt;p&gt;最知名的综合工具是Geospatial Data Abstraction Library，经常被称为GDAL，它包括了OGR Simple Features Library以及Ogr2org 库，我们将用来操作Natural Earth 的shapefiles。这里有一些官方GDAL库覆盖了大多数平台，但是如果你在Mac，你应该使用Homebrew:&lt;/p&gt;

&lt;p&gt;brew install gdal&lt;/p&gt;

&lt;p&gt;接下来你将需要TopoJSON，这要求Node.js (你可以安装Node通过Homebrew，或者official installers也可以)，在安装Node之后，运行下面的命令安装TopoJSON：&lt;/p&gt;

&lt;p&gt;npm install -g topojson&lt;/p&gt;

&lt;p&gt;为了验证这两个安装成功了，尝试：&lt;/p&gt;

&lt;p&gt;which ogr2og2
which topojson&lt;/p&gt;

&lt;p&gt;它应该打印出 /usr/local/bin/ogr2ogr 以及 /usr/local/bin/topojson&lt;/p&gt;

&lt;h1 id=&quot;转换数据&quot;&gt;转换数据&lt;/h1&gt;

&lt;p&gt;现在我们准备好了，我们将合并两个shapefiles到一个单独的TopoJSON文件。我们首先筛选出shapefile，使得它只包含我们所需要的UK信息。然后我们将shapefiles先转换成中间件GeoJSON然后再生成TopoJSON。&lt;/p&gt;

&lt;p&gt;为什么出现两种JSON格式？事实上，它们是兄弟。TopoJSON是GeoJSON的一个拓扑编码扩展。通过对坐标进行固定精度编码，TopoJSON通常比GeoJSON小得多。我们的地图的GeoJSON有536KB之大，而TopoJSON只有80KB，一个85%的削减。（这个削减比例甚至在gzip压缩后仍然保持！）进一步的，在TopoJSON中的拓扑信息允许自动计算边界线以及其他令人感兴趣的应用，即使这消耗了更多的存储空间。&lt;/p&gt;

&lt;p&gt;将下载的ne_10m_admin_0_map_subunits.shp作为输入，使用ogr2ogr转换成subunits.json GeoJSON文件：&lt;/p&gt;

&lt;p&gt;ogr2ogr \
  -f GeoJSON \
  -where “ADM0_A3 IN (‘GBR’, ‘IRL’)” \
  subunits.json \
  ne_10m_admin_0_map_subunits.shp&lt;/p&gt;

&lt;p&gt;-where参数指示了筛选规则：只有ADMO_A3属性值为GBR与IRL的项才会被输出到GeoJSON中。在此，ADM0表示Admin-0，官方边界的最高等级，以及A3表示ISO 3166-1 alpha-5国家代码。尽管只画联合王国的地图，但我们需要所有的爱尔兰数据，另一方面，我们应当指出爱尔兰只是个地方并不是一个国名！&lt;/p&gt;

&lt;p&gt;接下来我们筛选出著名地点，其中place属性有点不同（也许设定的太随意了），所以我们使用ISO_A2取代之。而SCALERANK筛选将把标签限制在大城市级别。&lt;/p&gt;

&lt;p&gt;ogr2ogr \
  -f GeoJSON \
  -where “ISO_A2 = ‘GB’ AND SCALERANK &amp;lt; 8” \
  places.json \
  ne_10m_populated_places.shp&lt;/p&gt;

&lt;p&gt;最后我们组合subunits.json与places.json到一个单独的uk.json文件中。这一步包括一个最小化变换来固定原数据中的坐标，重命名NAME属性为name，以及将SU_A3属性变为对象id。&lt;/p&gt;

&lt;p&gt;topojson \
  -o uk.json \
  –id-property SU_A3 \
  –properties name=NAME \
  – \
  subunits.json \
  places.json&lt;/p&gt;

&lt;p&gt;尽管在这个地图中并不需要，但ogr2ogr有很多强有力的特性你也许用地上。-clipdst参数，作为例子，调整shapefile到一个bounding box，对于显示特性中的一小部分是有用的。如果你的shapefile使用网格坐标系统（如UTM），使用 -t_src EPSG:4326 可以将它转回经纬度系统。阅读ogr2ogr手册了解更多。&lt;/p&gt;

&lt;p&gt;#读取数据&lt;/p&gt;

&lt;p&gt;为了简洁的一瞥如何用命令行操纵地理数据，我们返回web开发、在我们之前转换出来的东西的基础上，我将假设你对HTML以及JavaScript非常熟悉，如果并不是，花一点时间阅读 Scott Murray’s introduction to D3.在uk.json所在的目录下，创建index.html以下属模板进行。&lt;/p&gt;

&lt;p&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/p&gt;
&lt;meta charset=&quot;utf-8&quot; /&gt;

&lt;style&gt;

/* CSS goes here. */

&lt;/style&gt;

&lt;body&gt;
&lt;script src=&quot;//d3js.org/d3.v3.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;//d3js.org/topojson.v1.min.js&quot;&gt;&lt;/script&gt;
&lt;script&gt;

/* JavaScript goes here. */

&lt;/script&gt;

然后，启动一个本地服务器来显示你的例子，我使用http-server，但是任何服务器都可以：

http-server -p 8008 &amp;amp;

如果你访问 http://localhost:8008,你应该看到一个光荣的空白页面：

很可能这并不是你希望的那样激动人心！但是我们可以很快改变这一切，在主script tag里（就是说JavaScript goes here那个地方），调用d3.json函数来加载TopoJSON文件：

d3.json(&quot;uk.json&quot;, function(error, uk) {
  if (error) return console.error(error);
  console.log(uk);
});

现在如果你看一下你的JavaScript命令行，你应该可以看到一个topology对象其代表联合王国的官方边界以及著名地点。


# 显示 Polygons

存在不同的方法来渲染二维几何对象在浏览器中，但是有两个主要标准是SG以及Canvas。D3 3.0两个都支持。我们将使用SVG对于这个例子，因为你可以对SVG通过CSS施加样式，并且声明样式是简单的。首先创建根SVG元素：

我建议这些在主脚本里进行而非在d3.json函数的参数回调函数中进行。这是因为d3.json是异步的：页面的剩余部分江北渲染当我们等待TopoJSON文件被下载。创建一个空的SVG根元素在页面加载的同时避免了在地理数据到达后又反过来去做这些事情。

我们还需要两个东西来渲染地理数据，一个projection(投影)以及一个path generator(路径产生器)。正如它的名字所暗示的，projection将球坐标系投影到笛卡尔坐标系上。这对于将其显示到2D的屏幕上是必要的，你可以跳过这一步如果你之后想以3D全息（holographic）投影显示它。path generator持有2D投影并且将它格式化为适当的SVG或者Canvas形态。

所以准备好做地图了！替换下面代码作为d3.json函数的回调函数像这样：

d3.json(&quot;uk.json&quot;, function(error, uk) {
  if (error) return console.error(error);

  svg.append(&quot;path&quot;)
      .datum(topojson.feature(uk, uk.objects.subunits))
      .attr(&quot;d&quot;, d3.geo.path().projection(d3.geo.mercator()));
});

你应该看到一个小的，黑的，熟悉的痕迹：

少量绘图者现在将宣布工作已经完满解决然后回家来上一杯啤酒。但是我们应做的比这个更多。无论如何，如果我解释那三行代码做了什么将是有帮助的。。。

回顾之前两个地位相近的JSON地理数据格式：GeoJSON以及TopoJSON。当我们的数据被以更有效率的TopoJSON储存，我们必须将它转回GeoJSON来进行显示。拆分出这一部让它更清晰：

var subunits = topojson.feature(uk, uk.objects.subunits);

类似的，我们可以提取投影的定义来使得代码更干净：

var projection = d3.geo.mercator()
    .scale(500)
    .translate([width / 2, height / 2]);

以及这样的path generator:

var path = d3.geo.path()
    .projection(projection);

将path元素绑定GeoJSON数据，然后使用selection.attr设置&quot;d&quot;属性给格式化过的path数据。

svg.append(&quot;path&quot;)
    .datum(subunits)
    .attr(&quot;d&quot;, path);

对于形成的这段结构化的代码，我们可以修改投影的设置来使其更适合联合王国。Albers equal-area conic projection是一个好的选择，并且控制所绘制区域为50N到60N。我们旋转经度4.4度并且设置中心到0W，55.4N，所以真正的中心是4.4W 55.4N，这个地方在苏格兰。

var projection = d3.geo.albers()
    .center([0, 55.4])
    .rotate([4.4, 0])
    .parallels([50, 60])
    .scale(6000)
    .translate([width / 2, height / 2]);

我们地图变成了这个样子


# 对Polygon施加样式

像我之前提到的那样，SVG的一个好处是我们可以作用CSS样式，我们可以对邦国进行染色通过赋予fill属性。无论如何，我们首先需要给定每个国家一个他们自己的path元素，而不是共享一个。如果不这样做，就没法分别染色。

在TopoJSON文件uk.json内部，Admin-0地图subunits被以feature collection(特征集)的方式表征。通过取出feature数组，我们可以计算data join（数据匹配，即数据多余时产生更多节点适应数据）以创建每个feature的path元素。

svg.selectAll(&quot;.subunit&quot;)
    .data(topojson.feature(uk, uk.objects.subunits).features)
  .enter().append(&quot;path&quot;)
    .attr(&quot;class&quot;, function(d) { return &quot;subunit &quot; + d.id; })
    .attr(&quot;d&quot;, path);

函数设定的&quot;class&quot;值基于ISO-3166 alpha-3国家码标准，这使得我们可以分别作用fill样式给每个国家：

.subunit.SCT { fill: #ddc; }
.subunit.WLS { fill: #cdd; }
.subunit.NIR { fill: #cdc; }
.subunit.ENG { fill: #dcd; }
.subunit.IRL { display: none; }

样式将爱尔兰完全隐藏了，不过之后我们在画边界的时候又会把它弄回来。下面是地图现在的样子：


# 显示边界

为了让polygon不至于连起来，我们需要一些线。这里有两种线，一种是英格兰，苏格兰，威尔士的边界，另一个爱尔兰的的海岸线。

我们将使用topojson.mesh来计算边界。这要求两个参数，topology以及constituent geometry object.一个可选的筛选器可以缩减返回的边界的集合，其持有的两个参数a和b表示边界两边的特征。如果是外部边界，如海岸线，则a和b是一样的。通过a===b和a!==b这样的表达式，我们可以给不同的边界以不同的渲染。

英格兰-苏格兰以及英格兰-威尔士边界是内部边界，我们可以排除爱尔兰和北爱尔兰边界通过id筛选：

svg.append(&quot;path&quot;)
    .datum(topojson.mesh(uk, uk.objects.subunits, function(a, b) { return a !== b &amp;amp;&amp;amp; a.id !== &quot;IRL&quot;; }))
    .attr(&quot;d&quot;, path)
    .attr(&quot;class&quot;, &quot;subunit-boundary&quot;);

这只留下了爱尔兰的外部边界

svg.append(&quot;path&quot;)
    .datum(topojson.mesh(uk, uk.objects.subunits, function(a, b) { return a === b &amp;amp;&amp;amp; a.id === &quot;IRL&quot;; }))
    .attr(&quot;d&quot;, path)
    .attr(&quot;class&quot;, &quot;subunit-boundary IRL&quot;);

增加一点样式

.subunit-boundary {
  fill: none;
  stroke: #777;
  stroke-dasharray: 2,2;
  stroke-linejoin: round;
}

.subunit-boundary.IRL {
  stroke: #aaa;
}


# 显示地点

就像国家polygon一样，著名地点也是一个feature集合，所以我们可以再次转换TopoJSON到GeoJSON并且使用d3.geo.path来进行渲染：

svg.append(&quot;path&quot;)
    .datum(topojson.feature(uk, uk.objects.places))
    .attr(&quot;d&quot;, path)
    .attr(&quot;class&quot;, &quot;place&quot;);

这将在每个城市所在地点画一个小圆，我们可以调整其半径通过设置path.pointRadius，并且通过CSS赋予样式。但是我们还想要label，所以我们需要data join来产生文本元素。通过计算transform属性通过投影地点的坐标，我们可以转换坐标到希望到位置。

svg.selectAll(&quot;.place-label&quot;)
    .data(topojson.feature(uk, uk.objects.places).features)
  .enter().append(&quot;text&quot;)
    .attr(&quot;class&quot;, &quot;place-label&quot;)
    .attr(&quot;transform&quot;, function(d) { return &quot;translate(&quot; + projection(d.geometry.coordinates) + &quot;)&quot;; })
    .attr(&quot;dy&quot;, &quot;.35em&quot;)
    .text(function(d) { return d.properties.name; });

适当的打Label实际上是有难度的，特别是如果你想要将标签自动防止。我们在这个简单的地图上无视了这个问题，因为我们已经通过SCALERANK跳出了我们想要的标签。一个方便的技巧是使用右对齐标签到地图的左边，并且左对齐标签在地图的右边，这时使用了1W作为阈值：

svg.selectAll(&quot;.place-label&quot;)
    .attr(&quot;x&quot;, function(d) { return d.geometry.coordinates[0] &amp;gt; -1 ? 6 : -6; })
    .style(&quot;text-anchor&quot;, function(d) { return d.geometry.coordinates[0] &amp;gt; -1 ? &quot;start&quot; : &quot;end&quot;; });

正如你在下面看到的，看上去已经令人能够理解了，尽管存在一些重叠的标签。如果你想抵制这种情况你可以使用特殊发布可选对齐，或者你可以简单的移除覆盖的标签，你甚至可以使用simulated annealing 或者 forcedirected layout来布局，但是我会在1之后演示automatic label placement.

# 国家标签

我们地图遗失了一个重要的部分：我们还诶有标记国家！我们可以使用Natural Earth的Admin-0 label points，但是我们可以只是简单的使用projected centroid计算出国家标签应该在的位置：

svg.selectAll(&quot;.subunit-label&quot;)
    .data(topojson.feature(uk, uk.objects.subunits).features)
  .enter().append(&quot;text&quot;)
    .attr(&quot;class&quot;, function(d) { return &quot;subunit-label &quot; + d.id; })
    .attr(&quot;transform&quot;, function(d) { return &quot;translate(&quot; + path.centroid(d) + &quot;)&quot;; })
    .attr(&quot;dy&quot;, &quot;.35em&quot;)
    .text(function(d) { return d.properties.name; });

国家标签被样式化成比城市标签显示的更大。通过设置它们的transparent，它们被压在城市标签下面：

.subunit-label {
  fill: #777;
  fill-opacity: .5;
  font-size: 20px;
  font-weight: 300;
  text-anchor: middle;
}


【Topojson是一种使用拓扑编码方式的GeoJSON扩展，而不是代表着离散的几何图形（也可以称为几何体，因为json中的数据实际上是立体的），几何图形在TopoJSON中被一种叫做arcs（弧线，也是TopoJSON中的结构）的共享连接线联系在一起。Arcs实际上是点的集合，把图形用线连接起来可以叫做弧线。每条弧线只会被定义一次（这一点跟GeoJSON不通，GeoJSON的边界可能会被多次重绘），但是可以在不通的形状中多次引用，因此减少了数据冗余而缩小了文件大小。另外，TopoJSON促使应用程序使用拓扑结构，例如使用保留拓扑结构（topology-preserving）的简单形状、自动着色（d3中常用）、统计地图等。TopoJSON说明的引用实现是可行的，作为一个命令行工具将GeoJSON（或者ESRI形状文件）转换成TopoJSON，然后在客户端Javascript库重新将TopoJSON转回GeoJSON。】


&lt;/body&gt;
</description>
        <pubDate>Tue, 16 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2018/01/16/map.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2018/01/16/map.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>Linux、Mac上面ln命令使用说明</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;ln是linux中又一个非常重要命令，它的功能是为某一个文件在另外一个位置建立一个同不的链接，这个命令最常用的参数是 -s，具体用法是：ln –s 源文件 目标文件。&lt;/p&gt;

&lt;p&gt;当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。例如：ln –s /bin/less /usr/local/bin/less 
-s 是代号（symbolic）的意思&lt;/p&gt;

&lt;p&gt;注意：
第一，ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化；
第二，ln的链接又分软链接和硬链接两种，软链接就是ln –s ** **，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接ln ** **，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 　　&lt;/p&gt;

&lt;p&gt;如果你用ls查看一个目录时，发现有的文件后面有一个@的符号，那就是一个用ln命令生成的文件，用ls –l命令去察看，就可以看到显示的link的路径了。&lt;/p&gt;

&lt;p&gt;指令详细说明(可自行man ln查看)
[plain] view plain copy
指令名称 : ln&lt;br /&gt;
使用权限 : 所有使用者&lt;br /&gt;
使用方式 : ln [options] source dist，其中 option 的格式为 :&lt;br /&gt;
[-bdfinsvF] [-S backup-suffix] [-V {numbered,existing,simple}]&lt;br /&gt;
[–help] [–version] [–]&lt;br /&gt;
说明 : Linux/Unix 档案系统中，有所谓的连结(link)，我们可以将其视为档案的别名，而连结又可分为两种 : 硬连结(hard link)与软连结(symbolic link)，硬连结的意思是一个档案可以有多个名称，而软连结的方式则是产生一个特殊的档案，该档案的内容是指向另一个档案的位置。硬连结是存在同一个档 案系统中，而软连结却可以跨越不同的档案系统。&lt;br /&gt;
ln source dist 是产生一个连结(dist)到 source，至于使用硬连结或软链结则由参数决定。&lt;br /&gt;
不论是硬连结或软链结都不会将原本的档案复制一份，只会占用非常少量的磁碟空间。&lt;br /&gt;
-f : 链结时先将与 dist 同档名的档案删除&lt;br /&gt;
-d : 允许系统管理者硬链结自己的目录&lt;br /&gt;
-i : 在删除与 dist 同档名的档案时先进行询问&lt;br /&gt;
-n : 在进行软连结时，将 dist 视为一般的档案&lt;br /&gt;
-s : 进行软链结(symbolic link)&lt;br /&gt;
-v : 在连结之前显示其档名&lt;br /&gt;
-b : 将在链结时会被覆写或删除的档案进行备份&lt;br /&gt;
-S SUFFIX : 将备份的档案都加上 SUFFIX 的字尾&lt;br /&gt;
-V METHOD : 指定备份的方式&lt;br /&gt;
–help : 显示辅助说明&lt;br /&gt;
–version : 显示版本&lt;br /&gt;
范例 :&lt;br /&gt;
将档案 yy 产生一个 symbolic link : zz&lt;br /&gt;
ln -s yy zz&lt;br /&gt;
将档案 yy 产生一个 hard link : zz&lt;br /&gt;
ln yy xx﻿&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2018/01/16/ln.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2018/01/16/ln.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>spark on hive</title>
        <description>&lt;p&gt;$cp hive/hive/conf/hive-site.xml spark/spark/conf/
&lt;!-- more --&gt;
启动hive
启动spark&lt;/p&gt;

&lt;p&gt;import org.apache.spark.sql.SparkSession
val sparkSession = SparkSession.builder.master(“local”).enableHiveSupport().getOrCreate();&lt;/p&gt;

&lt;p&gt;Caused by: ERROR XJ040: Failed to start database ‘metastore_db’ with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@294b045b, see the next exception for details.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)
	… 150 more
Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /Users/didi/metastore_db.&lt;/p&gt;

&lt;p&gt;在使用Hive on Spark模式操作hive里面的数据时，报以上错误，原因是因为HIVE采用了derby这个内嵌数据库作为数据库，它不支持多用户同时访问,解决办法就是把derby数据库换成mysql数据库即可&lt;/p&gt;

&lt;p&gt;解决方式：&lt;/p&gt;

&lt;p&gt;不使用默认的内嵌数据库derby，采用mysql作为统计的存储信息。&lt;/p&gt;

&lt;p&gt;修改相关配置信息（hive-site.xml）：&lt;/p&gt;

&lt;property&gt;

       &lt;name&gt;hive.stats.dbclass&lt;/name&gt;

       &lt;value&gt;jdbc:mysql&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

       &lt;name&gt;hive.stats.jdbcdriver&lt;/name&gt;

       &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

       &lt;name&gt;hive.stats.dbconnectionstring&lt;/name&gt;

       &lt;value&gt;jdbc:mysql://localhost:3306/TempStatsStore&lt;/value&gt;

&lt;/property&gt;

&lt;p&gt;修改完成保存。
另外后面还有一个步骤就是要在mysql里创建TempStatsStore这个数据库（mysql里不会自动创建该库，在derby里会自动创建）&lt;/p&gt;

&lt;p&gt;方式二
mv  metastore_db metastore_db_bak&lt;/p&gt;

&lt;p&gt;scala&amp;gt; val sparkSession = SparkSession.builder.master(“local”).enableHiveSupport().getOrCreate();
18/01/13 15:55:44 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
18/01/13 15:55:45 WARN conf.HiveConf: HiveConf of name hive.conf.hidden.list does not exist
18/01/13 15:55:47 WARN conf.HiveConf: HiveConf of name hive.conf.hidden.list does not exist
18/01/13 15:55:49 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/01/13 15:55:50 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
18/01/13 15:55:51 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/01/13 15:55:51 WARN conf.HiveConf: HiveConf of name hive.conf.hidden.list does not exist
sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@14c16388&lt;/p&gt;

&lt;p&gt;beeline&amp;gt;  !connect jdbc:hive2://localhost:10000&lt;/p&gt;

&lt;p&gt;0: jdbc:hive2://localhost:10000&amp;gt; show tables;
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don’t support retries at the client level.) (state=08S01,code=1)&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“CREATE TABLE IF NOT EXISTS src (key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ “)
18/01/13 16:04:23 WARN metastore.HiveMetaStore: Location: hdfs://localhost:8020/user/hive/warehouse/src specified for non-external table:src
res8: org.apache.spark.sql.DataFrame = []&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“show tables”).collect().foreach(println)
[default,src,false]&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“show databases”).collect().foreach(println)
[default]&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“show create  table src”).collect().foreach(println)
[CREATE TABLE &lt;code&gt;src&lt;/code&gt;(&lt;code&gt;key&lt;/code&gt; int, &lt;code&gt;value&lt;/code&gt; string)
ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe’
WITH SERDEPROPERTIES (
  ‘field.delim’ = ‘	‘,
  ‘serialization.format’ = ‘	‘
)
STORED AS
  INPUTFORMAT ‘org.apache.hadoop.mapred.TextInputFormat’
  OUTPUTFORMAT ‘org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat’
TBLPROPERTIES (
  ‘transient_lastDdlTime’ = ‘1515830663’
)
]&lt;/p&gt;

&lt;p&gt;hive不支持用insert语句一条一条的进行插入操作，也不支持update操作。数据是以load的方式加载到建立好的表中。数据一旦导入就不可以修改。&lt;/p&gt;

&lt;p&gt;DML包括：INSERT插入、UPDATE更新、DELETE删除&lt;/p&gt;

&lt;p&gt;scala&amp;gt; sparkSession.sql(“insert into table src key=1234,value=’abc’”).collect().foreach(println)
org.apache.spark.sql.catalyst.parser.ParseException:
no viable alternative at input ‘key’(line 1, pos 22)&lt;/p&gt;

&lt;p&gt;== SQL ==
insert into table src key=1234,value=’abc’
———————-^^^&lt;/p&gt;

&lt;p&gt;既然Hive没有行级别的数据插入、更新和删除操作，那么往表中装载数据的唯一途径就是使用一种”大量“的数据装载操作。我们以如下格式文件演示五种数据导入Hive方式&lt;/p&gt;

&lt;p&gt;Tom         24    NanJing   Nanjing University&lt;br /&gt;
Jack        29    NanJing   Southeast China University&lt;br /&gt;
Mary Kake   21    SuZhou    Suzhou University&lt;br /&gt;
John Doe    24    YangZhou  YangZhou University&lt;br /&gt;
Bill King   23    XuZhou    Xuzhou Normal University&lt;/p&gt;

&lt;p&gt;数据格式以\t分隔，分别表示：姓名、年龄、地址、学校&lt;/p&gt;

&lt;p&gt;一、从本地文件系统中导入数据
 (1) 创建test1测试表
scala&amp;gt; sparkSession.sql(“CREATE TABLE test1(name STRING,age INT, address STRING,school STRING)   ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’  STORED AS TEXTFILE”).collect().foreach(println)
18/01/13 16:20:50 WARN metastore.HiveMetaStore: Location: hdfs://localhost:8020/user/hive/warehouse/test1 specified for non-external table:test1
(2) 从本地加载数据 
scala&amp;gt; sparkSession.sql(“ LOAD DATA LOCAL INPATH ‘/Users/didi/hive/testHive.txt’ INTO TABLE test1”).collect().foreach(println) 
(3) 查看导入结果
scala&amp;gt; sparkSession.sql(“select * from test1”).collect().foreach(println)
[Tom         24    NanJing   Nanjing University  ,null,null,null]
[Jack        29    NanJing   Southeast China University  ,null,null,null]
[Mary Kake   21    SuZhou    Suzhou University  ,null,null,null]
[John Doe    24    YangZhou  YangZhou University  ,null,null,null]
[Bill King   23    XuZhou    Xuzhou Normal University,null,null,null] 
        注意：此处使用的是LOCAL，表示从本地文件系统中加载数据到Hive中，同时没有OVERWRITE关键字，仅仅会把新增的文件增加到目标文件夹而不会删除之前的数据。如果使用OVERWRITE关键字，那么目标文件夹中之前的数据将会被先删除掉。
二、从HDFS文件系统加载数据到Hive
(1) 清空之前创建的表中数据
insert overwrite table test1  select * from test1 where 1=0;  //清空表，一般不推荐这样操作&lt;br /&gt;
 (2) 从HDFS加载数据
hive&amp;gt; LOAD DATA INPATH “/input/test1.txt”&lt;br /&gt;
    &amp;gt; OVERWRITE INTO TABLE test1;&lt;br /&gt;
Loading data to table hive.test1&lt;br /&gt;
rmr: DEPRECATED: Please use ‘rm -r’ instead.&lt;br /&gt;
Deleted hdfs://secondmgt:8020/hive/warehouse/hive.db/test1&lt;br /&gt;
Table hive.test1 stats: [numFiles=1, numRows=0, totalSize=201, rawDataSize=0]&lt;br /&gt;
OK&lt;br /&gt;
Time taken: 0.355 seconds&lt;br /&gt;
 (3) 查询结果
hive&amp;gt; select * from test1;&lt;br /&gt;
OK&lt;br /&gt;
Tom     24.0    NanJing Nanjing University&lt;br /&gt;
Jack    29.0    NanJing Southeast China University&lt;br /&gt;
Mary Kake       21.0    SuZhou  Suzhou University&lt;br /&gt;
John Doe        24.0    YangZhou        YangZhou University&lt;br /&gt;
Bill King       23.0    XuZhou  Xuzhou Normal University&lt;br /&gt;
Time taken: 0.054 seconds, Fetched: 5 row(s)&lt;br /&gt;
        注意：此处没有LOCAL关键字，表示分布式文件系统中的路径，这就是和第一种方法的主要区别，同时由日志可以发现，因为此处加了OVERWRITE关键字，执行了Deleted操作，即先删除之前存储的数据，然后再执行加载操作。
       同时，INPATH子句中使用的文件路径还有一个限制，那就是这个路径下不可以包含任何文件夹。&lt;/p&gt;

&lt;p&gt;三、通过查询语句向表中插入数据
(1) 创建test4测试表
scala&amp;gt; sparkSession.sql(“ CREATE TABLE test4(name STRING,age FLOAT,address STRING,school STRING)   ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ STORED AS TEXTFILE”).collect().foreach(println)
18/01/13 16:27:53 WARN metastore.HiveMetaStore: Location: hdfs://localhost:8020/user/hive/warehouse/test4 specified for non-external table:test4
 创建表过程基本和前面一样，此处不细讲
(2) 从查询结果中导入数据
scala&amp;gt; sparkSession.sql(“INSERT INTO TABLE test4 SELECT * FROM test1”).collect().foreach(println) 
        注意：新建表的字段数，一定要和后面SELECT中查询的字段数一样，且要注意数据类型。如test4包含四个字段：name、age、address和school，则SELECT查询出的结果也应该对应这四个字段。
(3) 查看导入结果
scala&amp;gt; sparkSession.sql(“select * from test4”).collect().foreach(println)
[Tom         24    NanJing   Nanjing University  ,null,null,null]
[Jack        29    NanJing   Southeast China University  ,null,null,null]
[Mary Kake   21    SuZhou    Suzhou University  ,null,null,null]
[John Doe    24    YangZhou  YangZhou University  ,null,null,null]
[Bill King   23    XuZhou    Xuzhou Normal University,null,null,null]
四、分区插入
        分区插入有两种，一种是静态分区，另一种是动态分区。如果混合使用静态分区和动态分区，则静态分区必须出现在动态分区之前。现分别介绍这两种分区插入
(1) 静态分区插入&lt;/p&gt;

&lt;p&gt;①创建分区表
hive&amp;gt; CREATE TABLE test2(name STRING,address STRING,school STRING)&lt;br /&gt;
    &amp;gt; PARTITIONED BY(age float)&lt;br /&gt;
    &amp;gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’&lt;br /&gt;
    &amp;gt; STORED AS TEXTFILE ;&lt;br /&gt;
OK&lt;br /&gt;
Time taken: 0.144 seconds&lt;br /&gt;
       此处创建了一个test2的分区表，以年龄分区&lt;/p&gt;

&lt;p&gt;②从查询结果中导入数据    &lt;br /&gt;
hive&amp;gt; INSERT INTO  TABLE test2 PARTITION (age=’24’) SELECT * FROM test1;&lt;br /&gt;
FAILED: SemanticException [Error 10044]: Line 1:19 Cannot insert into target table because column number/types are different ‘‘24’’: Table insclause-0 has 3 columns, but query has 4 columns.&lt;br /&gt;
 此处报了一个错误。是因为test2中是以age分区的，有三个字段，SELECT * 语句中包含有四个字段，所以出错。正确如下：
[html] view plain copy
hive&amp;gt; INSERT INTO  TABLE test2 PARTITION (age=’24’) SELECT name,address,school FROM test1;&lt;br /&gt;
 ③ 查看插入结果
hive&amp;gt; select * from test2;&lt;br /&gt;
OK&lt;br /&gt;
Tom     NanJing Nanjing University      24.0&lt;br /&gt;
Jack    NanJing Southeast China University      24.0&lt;br /&gt;
Mary Kake       SuZhou  Suzhou University       24.0&lt;br /&gt;
John Doe        YangZhou        YangZhou University     24.0&lt;br /&gt;
Bill King       XuZhou  Xuzhou Normal University        24.0&lt;br /&gt;
Time taken: 0.079 seconds, Fetched: 5 row(s)&lt;br /&gt;
 由查询结果可知，每条记录的年龄均为24，插入成功。
(2) 动态分区插入
静态分区需要创建非常多的分区，那么用户就需要写非常多的SQL！Hive提供了一个动态分区功能，其可以基于查询参数推断出需要创建的分区名称。&lt;/p&gt;

&lt;p&gt;① 创建分区表，此过程和静态分区创建表一样，此处省略&lt;/p&gt;

&lt;p&gt;② 参数设置
hive&amp;gt; set hive.exec.dynamic.partition=true;&lt;br /&gt;
hive&amp;gt; set hive.exec.dynamic.partition.mode=nonstrict;&lt;br /&gt;
 注意：动态分区默认情况下是没有开启的。开启后，默认是以”严格“模式执行的，在这种模式下要求至少有一列分区字段是静态的。这有助于阻止因设计错误导致查询产生大量的分区。但是此处我们不需要静态分区字段，估将其设为nonstrict。
③ 数据动态插入
hive&amp;gt; insert into table test2 partition (age) select name,address,school,age from test1;&lt;br /&gt;
Total jobs = 1&lt;br /&gt;
Launching Job 1 out of 1&lt;br /&gt;
Number of reduce tasks not specified. Estimated from input data size: 1&lt;br /&gt;
In order to change the average load for a reducer (in bytes):&lt;br /&gt;
  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;  
In order to limit the maximum number of reducers:  
  set hive.exec.reducers.max=&lt;number&gt;  
In order to set a constant number of reducers:  
  set mapreduce.job.reduces=&lt;number&gt;  
Starting Job = job_1419317102229_0029, Tracking URL = http://secondmgt:8088/proxy/application_1419317102229_0029/  
Kill Command = /home/hadoopUser/cloud/hadoop/programs/hadoop-2.2.0/bin/hadoop job  -kill job_1419317102229_0029  
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1  
2014-12-28 20:45:07,996 Stage-1 map = 0%,  reduce = 0%  
2014-12-28 20:45:21,488 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.67 sec  
2014-12-28 20:45:32,926 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.32 sec  
MapReduce Total cumulative CPU time: 7 seconds 320 msec  
Ended Job = job_1419317102229_0029  
Loading data to table hive.test2 partition (age=null)  
        Loading partition {age=29.0}  
        Loading partition {age=23.0}  
        Loading partition {age=21.0}  
        Loading partition {age=24.0}  
Partition hive.test2{age=21.0} stats: [numFiles=1, numRows=1, totalSize=35, rawDataSize=34]  
Partition hive.test2{age=23.0} stats: [numFiles=1, numRows=1, totalSize=42, rawDataSize=41]  
Partition hive.test2{age=24.0} stats: [numFiles=1, numRows=2, totalSize=69, rawDataSize=67]  
Partition hive.test2{age=29.0} stats: [numFiles=1, numRows=1, totalSize=40, rawDataSize=39]  
MapReduce Jobs Launched:  
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 7.32 sec   HDFS Read: 415 HDFS Write: 375 SUCCESS  
Total MapReduce CPU Time Spent: 7 seconds 320 msec  
OK  
Time taken: 41.846 seconds  
 注意：查询语句select查询出来的age字段必须放在最后，和分区字段对应，不然结果会出错
④ 查看插入结果
[html] view plain copy
hive&amp;gt; select * from test2;  
OK  
Mary Kake       SuZhou  Suzhou University       21.0  
Bill King       XuZhou  Xuzhou Normal University        23.0  
John Doe        YangZhou        YangZhou University     24.0  
Tom     NanJing Nanjing University      24.0  
Jack    NanJing Southeast China University      29.0  
五、单个查询语句中创建表并加载数据
         在实际情况中，表的输出结果可能太多，不适于显示在控制台上，这时候，将Hive的查询输出结果直接存在一个新的表中是非常方便的，我们称这种情况为CTAS（create table .. as select）
        (1) 创建表
hive&amp;gt; CREATE TABLE test3  
    &amp;gt; AS  
    &amp;gt; SELECT name,age FROM test1;  
Total jobs = 3  
Launching Job 1 out of 3  
Number of reduce tasks is set to 0 since there's no reduce operator  
Starting Job = job_1419317102229_0030, Tracking URL = http://secondmgt:8088/proxy/application_1419317102229_0030/  
Kill Command = /home/hadoopUser/cloud/hadoop/programs/hadoop-2.2.0/bin/hadoop job  -kill job_1419317102229_0030  
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0  
2014-12-28 20:59:59,375 Stage-1 map = 0%,  reduce = 0%  
2014-12-28 21:00:10,795 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.68 sec  
MapReduce Total cumulative CPU time: 2 seconds 680 msec  
Ended Job = job_1419317102229_0030  
Stage-4 is selected by condition resolver.  
Stage-3 is filtered out by condition resolver.  
Stage-5 is filtered out by condition resolver.  
Moving data to: hdfs://secondmgt:8020/hive/scratchdir/hive_2014-12-28_20-59-45_494_6763514583931347886-1/-ext-10001  
Moving data to: hdfs://secondmgt:8020/hive/warehouse/hive.db/test3  
Table hive.test3 stats: [numFiles=1, numRows=5, totalSize=63, rawDataSize=58]  
MapReduce Jobs Launched:  
Job 0: Map: 1   Cumulative CPU: 2.68 sec   HDFS Read: 415 HDFS Write: 129 SUCCESS  
Total MapReduce CPU Time Spent: 2 seconds 680 msec  
OK  
Time taken: 26.583 seconds  
 (2) 查看插入结果
hive&amp;gt; select * from test3;  
OK  
Tom     24.0  
Jack    29.0  
Mary Kake       21.0  
John Doe        24.0  
Bill King       23.0  
Time taken: 0.045 seconds, Fetched: 5 row(s)&lt;/number&gt;&lt;/number&gt;&lt;/number&gt;&lt;/p&gt;

&lt;p&gt;Exception in thread “main” java.lang.NoClassDefFoundError: scala/Product$class
	at org.apache.spark.internal.config.ConfigBuilder.&lt;init&gt;(ConfigBuilder.scala:176)
	at org.apache.spark.sql.internal.SQLConf$.buildConf(SQLConf.scala:58)
	at org.apache.spark.sql.internal.SQLConf$.&lt;init&gt;(SQLConf.scala:67)
	at org.apache.spark.sql.internal.SQLConf$.&lt;clinit&gt;(SQLConf.scala)
	at org.apache.spark.sql.internal.StaticSQLConf$.&lt;init&gt;(StaticSQLConf.scala:31)
	at org.apache.spark.sql.internal.StaticSQLConf$.&lt;clinit&gt;(StaticSQLConf.scala)
	at org.apache.spark.sql.SparkSession$Builder.enableHiveSupport(SparkSession.scala:843)
	at main.scala.hiveConnection$.main(hiveConnection.scala:6)
	at main.scala.hiveConnection.main(hiveConnection.scala)&lt;/clinit&gt;&lt;/init&gt;&lt;/clinit&gt;&lt;/init&gt;&lt;/init&gt;&lt;/p&gt;

&lt;p&gt;在使用Log4j时若提示如下信息：
log4j:WARN No appenders could be found for logger (org.apache.ibatis.logging.LogFactory).&lt;br /&gt;
log4j:WARN Please initialize the log4j system properly. 
则，解决办法为：在项目的src下面新建file名为log4j.properties文件，内容如下:&lt;/p&gt;

&lt;h1 id=&quot;configure-logging-for-testing-optionally-with-log-file&quot;&gt;Configure logging for testing: optionally with log file&lt;/h1&gt;
&lt;p&gt;#可以设置级别：debug&amp;gt;info&amp;gt;error
#debug:可以显式debug,info,error
#info:可以显式info,error
#error:可以显式error&lt;/p&gt;

&lt;p&gt;log4j.rootLogger=debug,appender1
#log4j.rootLogger=info,appender1
#log4j.rootLogger=error,appender1&lt;/p&gt;

&lt;p&gt;#输出到控制台
log4j.appender.appender1=org.apache.log4j.ConsoleAppender
#样式为TTCCLayout
log4j.appender.appender1.layout=org.apache.log4j.TTCCLayout&lt;/p&gt;

&lt;p&gt;然后，存盘退出。再次运行程序就会显示Log信息了。&lt;/p&gt;

&lt;p&gt;通过配置文件可知，我们需要配置3个方面的内容：
1、根目录（级别和目的地）；
2、目的地（控制台、文件等等）；
3、输出样式。&lt;/p&gt;

&lt;p&gt;或者，使用下面的内容也可以：
 # Configure logging for testing: optionally with log file
log4j.rootLogger=WARN, stdout
 # log4j.rootLogger=WARN, stdout, logfile
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n&lt;/p&gt;

</description>
        <pubDate>Sat, 13 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/13/spark_hive.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/13/spark_hive.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>scala maven 版本冲突问题解决</title>
        <description>&lt;p&gt;scalatest_2.10-1.9.1.jar of core build path is cross-compiled with an incompatible version of Scala (2.10.0)&lt;/p&gt;

&lt;p&gt;Eclipse - Preferences - Scala - Compiler - Build manager
uncheck withVersionClasspathVariable&lt;/p&gt;

&lt;p&gt;More than one scala library found in the build path (/home/hadoop/eclipse/plugins/org.scala-lang.scala-library_2.11.7.v20150622-112736-1fbce4612c.jar, /usr/local/spark/spark-1.5.1-bin-hadoop2.6/lib/spark-assembly-1.5.1-hadoop2.6.0.jar).At least one has an incompatible version. Please update the project build path so it contains only one compatible scala library. hello-test Unknown Scala Classpath Problem&lt;/p&gt;

&lt;p&gt;修改工程中的scala编译版本
右击 –&amp;gt; Scala –&amp;gt; set the Scala Installation&lt;/p&gt;

&lt;p&gt;也可以&lt;/p&gt;

&lt;p&gt;右击工程–&amp;gt; Properties –&amp;gt; Scala Compiler –&amp;gt; Use project Setting 中选择spark对应的scala版本，此处选择Lastest2.10 bundle&lt;/p&gt;

&lt;p&gt;上述方法仍然没有解决
原因maven pom.xml 中的版本与eclipse里面设置的版本冲出
解决办法修改pom.xml&lt;/p&gt;
&lt;properties&gt;
    &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt;
    &lt;scala.version&gt;2.12.3&lt;/scala.version&gt;
    
 问题解决
 
 Unsupported major.minor version 52.0
 You get this error because a Java 7 VM tries to load a class compiled for Java 8

Java 8 has the class file version 52.0 but a Java 7 VM can only load class files up to version 51.0

In your case the Java 7 VM is your gradle build and the class is com.android.build.gradle.AppPlugin
简单来说，就是java的编译环境版本太低，java 8 class file的版本是52，Java 7虚拟机只能支持到51。所以需要升级到java 8 vm才行


mvn -V
Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T15:58:13+08:00)
Maven home: /Users/didi/maven
Java version: 1.8.0_144, vendor: Oracle Corporation

Missing artifact org.scalatest:scalatest_2.12:jar:  2.2.4

http://mvnrepository.com/artifact/org.scalatest/scalatest_2.12/3.0.3


  &lt;dependency&gt;
      &lt;groupId&gt;org.specs2&lt;/groupId&gt;
      &lt;artifactId&gt;specs2-core_${scala.compat.version}&lt;/artifactId&gt;
      &lt;version&gt;${scala.compat.version}&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.scalatest&lt;/groupId&gt;
      &lt;artifactId&gt;scalatest_${scala.compat.version}&lt;/artifactId&gt;
      &lt;version&gt;3.0.3&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    
vi /Users/didi/maven/conf/settings.xml

 在maven的默认配置中，对于jdk的配置是1.4版本，那么创建/导入maven工程过程中，工程中未指定jdk版本。

对工程进行maven的update，就会出现工程依赖的JRE System Library会自动变成JavaSE-1.4。



解决方案1：修改maven的默认jdk配置

           maven的conf\setting.xml文件中找到jdk配置的地方，修改如下：


[html] view plaincopy在CODE上查看代码片派生到我的代码片

&lt;profile&gt;   
    &lt;id&gt;jdk1.6&lt;/id&gt;    
    &lt;activation&gt;   
        &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;    
        &lt;jdk&gt;1.6&lt;/jdk&gt;   
    &lt;/activation&gt;    
    &lt;properties&gt;   
        &lt;maven.compiler.source&gt;1.6&lt;/maven.compiler.source&gt;    
        &lt;maven.compiler.target&gt;1.6&lt;/maven.compiler.target&gt;    
        &lt;maven.compiler.compilerVersion&gt;1.6&lt;/maven.compiler.compilerVersion&gt;   
    &lt;/properties&gt;   
&lt;/profile&gt;  

解决方案2：修改项目中pom.xml文件，这样避免在导入项目时的jdk版本指定

 打开项目中pom.xml文件，修改如下：
&lt;build&gt;  
    &lt;plugins&gt;  
        &lt;plugin&gt;  
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;  
            &lt;configuration&gt;  
                &lt;source /&gt;1.6&amp;lt;/source&amp;gt;  
                &lt;target&gt;1.6&lt;/target&gt;  
            &lt;/configuration&gt;  
        &lt;/plugin&gt;  
    &lt;/plugins&gt;  
&lt;/build&gt;  

右键－》propertity  
   remove jre1.6  
      add jre1.8
      
      
&lt;!-- more --&gt;
运行成功

Could not resolve dependencies for project maven.scala:mavenScala:jar:0.0.1-SNAPSHOT: Failure to find org.specs2:specs2-core_2.12:jar:2.12 in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced

http://maven.outofmemory.cn/org.specs2/specs2-core_2.12.0-M4/3.8.4/


&lt;dependency&gt;
    &lt;groupId&gt;org.specs2&lt;/groupId&gt;
    &lt;artifactId&gt;specs2-core_2.12.0-M4&lt;/artifactId&gt;
    &lt;version&gt;3.8.4&lt;/version&gt;
&lt;/dependency&gt;


删除
&lt;dependency&gt;
    &lt;groupId&gt;org.specs2&lt;/groupId&gt;
    &lt;artifactId&gt;specs2-core_2.12.0-M4&lt;/artifactId&gt;
    &lt;version&gt;3.8.4&lt;/version&gt;
&lt;/dependency&gt;


scalac error: bad option: '-make:transitive'
解决方法：

（1）打开pom.xml，删除

       &lt;parameter value=&quot;-make:transitive&quot; /&gt;
（2）添加dependance

        &lt;dependency&gt;
            &lt;groupId&gt;org.specs2&lt;/groupId&gt;
            &lt;artifactId&gt;specs2_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.6&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;


测试报错  删除

mvn package

[INFO] Building jar: /Users/didi/PhpstormProjects/ProjGit/Spark/ScalaMaven/MavenScala/target/MavenScala-0.0.1-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ----------------


Description Resource  Path  Location  Type
Project configuration is not up-to-date with pom.xml. Select: Maven-&amp;gt;Update Project... from the project context menu or use Quick Fix.  MavenScala    line 1  Maven Configuration Problem

右键  Maven-&amp;gt;Update Project

至此没有错误了




&lt;/properties&gt;
</description>
        <pubDate>Fri, 12 Jan 2018 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2018/01/12/scala_version.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2018/01/12/scala_version.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
  </channel>
</rss>
