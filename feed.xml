<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 13 Oct 2017 13:00:15 +0800</pubDate>
    <lastBuildDate>Fri, 13 Oct 2017 13:00:15 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>spark基本概念</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;我们知道Spark总是以集群的方式运行的，Standalone的部署方式是集群方式中最为精简的一种（另外的是Mesos和Yarn）。Standalone模式中，资源调度是自己实现的，是MS架构的集群模式，故存在单点故障问题。
下面提出几个问题并解决：
1、Standalone部署方式下包含哪些节点？&lt;/p&gt;

&lt;p&gt;由不同级别的三个节点组成，分别是Master主控节点、Worker工作节点、客户端节点；
（1）其中Master主控节点，顾名思义，类似于领导者，在整个集群中，最多只有一个Master处于Active状态。在使用spark-shell等交互式运行或者使用官方提供的run-example实例时，Driver运行在Master节点中；若是使用spark-submit工具进行任务的提交或者IDEA等工具开发运行任务时，Driver是运行在本地客户端的。
Master一方面负责各种信息，比如Driver、Worker、Application的注册；另一方面还负责Executor的启动，Worker心跳等诸多信息的处理。
（2）Woker节点，类似于yarn中的NodeManager，在整个集群中，可以有多个Worker（&amp;gt;0）。负责当前WorkerNode上的资源汇报、监督当前节点运行的Executor。并通过心跳机制来保持和Master的存活性连接。Executor受到Worker掌控，一个Worker启动Executor的个数受限于 机器中CPU核数。每个Worker节点存在一个多个CoarseGrainedExecutorBackend进程，每个进程包含一个Executor对象，该对象持有一个线程池，每个线程执行一个Task。
2、基本的概念？&lt;/p&gt;

&lt;p&gt;（1）Application：指的是用户编写的Spark应用程序，包含了含有一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码。
（2）Driver:运行Application的main函数并创建SparkContext，SparkContext的目的是为了准备Spark应用程序的运行环境。SparkContext负责资源的申请、任务分配和监控等。当Executor运行结束后，Driver负责关闭SparkContext；
（3）Job：一个Application可以产生多个Job，其中Job由Spark Action触发产生。每个Job包含多个Task组成的并行计算。
（4）Stage：每个Job会拆分为多个Task，作为一个TaskSet,称为Stage；Stage的划分和调度是由DAGScheduler负责的。Stage分为Result Stage和Shuffle Map Stage；
（5）Task：Application的运行基本单位，Executor上的工作单元。其调度和 管理又TaskScheduler负责。
（6）RDD：Spark基本计算单元，是Spark最核心的东西。表示已被分区、被序列化、不可变的、有容错机制的、能被并行操作的数据集合。
(7) DAGScheduler:根据Job构建基于Stage的DAG，划分Stage依据是RDD之间的依赖关系。
（8）TaskScheduler：将TaskSet提交给Worker运行，每个Worker运行了什么Task于此处分配。同时还负责监控、汇报任务运行情况等。
3、Standalone启动过程是啥？&lt;/p&gt;

&lt;p&gt;（1）首先，启动master，worker节点。
worker启动后触发Master的RegisterWorker事件，进行注册。主要讲要注册的Worker信息封装成WorkerInfo对象，包括Worker节点的CPU、内存等基本信息。记录Worker的信息（IP、Address）到master缓存中（HashMap），若Worker节点的注册信息有效，持久化已注册的Worker信息。然后给个完成注册的反馈信号。
（2）提交Application
运行spark-shell时，会由Driver端的DAGScheduler向Master发送RegisterApplication请求。根据此请求信息会创建ApplicationInfo对象，将Application加入到Master的缓存apps中，这个结构是HashSet。
如果worker已经注册，发送lanchExecutor指令给相应的Worker。
（3）Worker收到lanchExecutor后，会由ExecutorRunner启动Excutor进程，启动的Executor进程会根据启动时的入参，将自己注册到Drive中的ScheduleBackend。
(4)ScheduleBackend收到Excutor的注册信息后，会将提交到的Spark Job分解为多个具体的Task，然后通过LaunchTask指令将这些Task分散到各个Executor上运行。
4、Standalone部署方式下某一节点出现问题时，系统如何处理？&lt;/p&gt;

&lt;p&gt;出现问题的节点可能发生的情况有三种：
（1）Master崩掉了：这个坏掉了，就真的没法完了。单点故障的问题。
有两种解决办法：第一种基于文件系统的故障恢复，适合Master进程本身挂掉，那直接重启就Ok了。
第二种是基于ZookerKeep的HA方式。此方式被许多的分布式框架使用。
（2）某一Worker崩掉了：
若是所有的Worker挂掉，则整个集群就不可用；
Worker退出之前，会将管控的所有Executor进程kill；由于Worker挂掉，不能向master玩心跳了，根据超时处理会知道Worker挂了，然后Master将相应的情况汇报给Driver。Driver会根据master的信息和没有收到Executor的StatusUpdate确定这个Worker挂了，则Driver会将这个注册的Executor移除。
（3）某Worker的Excutor崩掉了：
Excutor的作为一个独立的进程在运行，由ExcutorRunner线程启动，并收到ExcutorRunner的监控，当Excutor挂了，ExcutorRunner会注意到异常情况，将ExecutorStateChanged汇报给Master，master会再次发送lanchExecutor指令给相应的Worker启动相应的Excutor。&lt;/p&gt;
</description>
        <pubDate>Fri, 13 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/13/spark_concepts.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/13/spark_concepts.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>java_scala</title>
        <description>&lt;!-- more --&gt;

&lt;p&gt;Java似乎可以无缝操纵Scala语言中定义的类，在trait那一节中我们提到，如果trait中全部是抽象成员，则它与java中的interface是等同的，这时候java可以把它当作接口来使用，但如果trait中定义了具体成员，则它有着自己的内部实现，此时在java中使用的时候需要作相应的调整。&lt;/p&gt;

&lt;p&gt;Scala可以直接调用Java实现的任何类，只要符合scala语法就可以，不过某些方法在JAVA类中不存在，此时只要引入scala.collection.JavaConversions._包就可以了，它会我们自动地进行隐式转换，从而可以使用scala中的一些非常方便的高阶函数，如foreach方法,还可以显式地进行转换&lt;/p&gt;

&lt;p&gt;Java中的泛型可以直接转换成Scala中的泛型，在前面的课程中我们已经有所涉及，例如Java中的Comparator&lt;T&gt; 可以直接转换成 Scala中的Comparator[T] 使用方法完全一样，不同的只是语法上的。&lt;/T&gt;&lt;/p&gt;

&lt;p&gt;Scala中的异常处理是通过模式匹配来实现的&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/12/java_scala.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/12/java_scala.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Scala_collection</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Scala 集合分为可变的和不可变的集合。&lt;/p&gt;

&lt;p&gt;1	Scala List(列表)
List的特征是其元素以线性方式存储，集合中可以存放重复对象。
参考 API文档
2	Scala Set(集合)
Set是最简单的一种集合。集合中的对象不按特定的方式排序，并且没有重复对象。
参考 API文档
3	Scala Map(映射)
Map 是一种把键对象和值对象映射的集合，它的每一个元素都包含一对键对象和值对象。
参考 API文档
4	Scala 元组
元组是不同类型的值的集合
5	Scala Option
Option[T] 表示有可能包含值的容器，也可能不包含值。
6	Scala Iterator（迭代器）
迭代器不是一个容器，更确切的说是逐一访问容器内元素的方法。&lt;/p&gt;

&lt;p&gt;　Scala中的三种集合类型包含:Array,List,Tuple．那么究竟这三种有哪些异同呢？说实话，我之前一直没弄明确，所以今天特意花了点时间学习了一下．&lt;/p&gt;

&lt;p&gt;　　　　同样点:
　　　　　1.长度都是固定的，不可变长
　　　　　２.早期的Scala版本号,Array、List都不能混合类型，仅仅有Tuple能够,2.8版本号以后,3者的元素都能够混合不同的类型（转化为Any类型）&lt;/p&gt;

&lt;p&gt;　　　　不同点:
　　　　　1.Array 中的元素值可变，List和Tuple中的元素值不可变
　　　　　２.Array通常是先确定长度，后赋值，而List和Tuple在声明的时候就须要赋值
　　　　　３.Array取单个元素的效率非常高。而List读取单个元素的效率是O(n)
　　　　　4.List和Array的声明不须要newkeyword。而Tuple声明无论有无new 都能够&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      val arrayTest = Array(1,2,3,4)   //正确
      val arrayTest = Array(1,2,3,4)   //错误&amp;lt;span style=&quot;font-family: Arial, Helvetica, sans-serif;&quot;&amp;gt;  &amp;lt;/span&amp;gt;
      val listTest = List(1,2,3,4)         //正确
      val listTest = new List(1,2,3,4)    //错误

      val tupleTest = Tuple(1,2,&quot;aaa&quot;)        //正确
      val tupleTest = new Tuple(1,2,&quot;aaa&quot;)    //正确
      val tupleTest = (1,2,&quot;aaa&quot;)             //正确 　　 　　　　　5.当使用混合类型时，Array和List会将元素类型转化为Any类型,而Tuple则保留每个元素的初始类型

                6.訪问方式不同。Array和List的下标从0開始，且使用小括号,而Tuple的下标从1開始，切使用点加下划线的方式訪问，如：arrayTest(0), listTest(0); Tuple訪问: tupleTest._1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;List 有个叫“ ::: ” 的方法实现叠加功能。你可以这么用：&lt;/p&gt;

&lt;p&gt;val oneTwo = List(1, 2)
val threeFour = List(3, 4)&lt;/p&gt;

&lt;p&gt;val oneTwoThreeFour = oneTwo ::: threeFour&lt;/p&gt;

&lt;p&gt;List 最常用的操作符是发音为“ cons ” 的‘ :: 。 Cons 把一个新元素组合到已有 List 的最前端，然后返回结果 List 。例如，若执行这个脚本：
val twoThree = list(2, 3)
val oneTwoThree = 1 :: twoThree&lt;/p&gt;

&lt;p&gt;表达式“ 1 :: twoThree ” 中， :: 是它右操作数，列表 twoThree ，的方法。你或许会疑惑 :: 方法的关联性上有什么东西搞错了，不过这只是一个简单的需记住的规则：如果一个方法被用作操作符标注，如 a * b ，那么方法被左操作数调用，就像 a.*(b) 除非方法名以冒号结尾。这种情况下，方法被右操作数调用。因此， 1 :: twoThree 里， :: 方法被 twoThree 调用，传入 1 ，像这样： twoThree.::(1) 。&lt;/p&gt;

</description>
        <pubDate>Thu, 12 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/12/Scala_collection.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/12/Scala_collection.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>scala_list</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;一、常用操作符（操作符其实也是函数）&lt;/p&gt;

&lt;p&gt;++ ++&lt;a href=&quot;that: GenTraversableOnce[B]&quot;&gt;B&lt;/a&gt;: List[B] 从列表的尾部添加另外一个列表&lt;/p&gt;

&lt;p&gt;++: ++:&lt;a href=&quot;that: collection.Traversable[B]&quot;&gt;B &amp;gt;: A, That&lt;/a&gt;(implicit bf: CanBuildFrom[List[A], B, That]): That 在列表的头部添加一个列表&lt;/p&gt;

&lt;p&gt;+: +:(elem: A): List[A] 在列表的头部添加一个元素&lt;/p&gt;

&lt;p&gt;:+ :+(elem: A): List[A] 在列表的尾部添加一个元素&lt;/p&gt;

&lt;p&gt;:: ::(x: A): List[A] 在列表的头部添加一个元素&lt;/p&gt;

&lt;p&gt;::: :::(prefix: List[A]): List[A] 在列表的头部添加另外一个列表&lt;/p&gt;

&lt;p&gt;:\ :&lt;a href=&quot;z: B&quot;&gt;B&lt;/a&gt;(op: (A, B) ⇒ B): B 与foldRight等价&lt;/p&gt;

&lt;p&gt;val left = List(1,2,3)
val right = List(4,5,6)&lt;/p&gt;

&lt;p&gt;//以下操作等价
left ++ right   // List(1,2,3,4,5,6)
left ++: right  // List(1,2,3,4,5,6)
right.++:(left)    // List(1,2,3,4,5,6)
right.:::(left)  // List(1,2,3,4,5,6)&lt;/p&gt;

&lt;p&gt;//以下操作等价
0 +: left    //List(0,1,2,3)
left.+:(0)   //List(0,1,2,3)&lt;/p&gt;

&lt;p&gt;//以下操作等价
left :+ 4    //List(1,2,3,4)
left.:+(4)   //List(1,2,3,4)&lt;/p&gt;

&lt;p&gt;//以下操作等价
0 :: left      //List(0,1,2,3)
left.::(0)     //List(0,1,2,3)
看到这里大家应该跟我一样有一点晕吧，怎么这么多奇怪的操作符，这里给大家一个提示，任何以冒号结果的操作符，都是右绑定的，即 0 :: List(1,2,3) = List(1,2,3).::(0) = List(0,1,2,3) 从这里可以看出操作::其实是右边List的操作符，而非左边Int类型的操作符&lt;/p&gt;

&lt;p&gt;二、常用变换操作&lt;/p&gt;

&lt;p&gt;1.map&lt;/p&gt;

&lt;p&gt;map&lt;a href=&quot;f: (A) ⇒ B&quot;&gt;B&lt;/a&gt;: List[B]&lt;/p&gt;

&lt;p&gt;定义一个变换,把该变换应用到列表的每个元素中,原列表不变，返回一个新的列表数据&lt;/p&gt;

&lt;p&gt;Example1 平方变换&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3)
val square = (x: Int) =&amp;gt; x&lt;em&gt;x &lt;br /&gt;
val squareNums1 = nums.map(num =&amp;gt; num&lt;/em&gt;num)    //List(1,4,9)
val squareNums2 = nums.map(math.pow(_,2))    //List(1,4,9)
val squareNums3 = nums.map(square)            //List(1,4,9)&lt;/p&gt;

&lt;p&gt;Example2 保存文本数据中的某几列&lt;/p&gt;

&lt;p&gt;val text = List(“Homeway,25,Male”,”XSDYM,23,Female”)
val usersList = text.map(_.split(“,”)(0))  &lt;br /&gt;
val usersWithAgeList = text.map(line =&amp;gt; {
    val fields = line.split(“,”)
    val user = fields(0)
    val age = fields(1).toInt
    (user,age)
})
2.flatMap, flatten&lt;/p&gt;

&lt;p&gt;flatten: flatten[B]: List[B] 对列表的列表进行平坦化操作 flatMap: flatMap&lt;a href=&quot;f: (A) ⇒ GenTraversableOnce[B]&quot;&gt;B&lt;/a&gt;: List[B] map之后对结果进行flatten&lt;/p&gt;

&lt;p&gt;定义一个变换f, 把f应用列表的每个元素中，每个f返回一个列表，最终把所有列表连结起来。&lt;/p&gt;

&lt;p&gt;val text = List(“A,B,C”,”D,E,F”)
val textMapped = text.map(&lt;em&gt;.split(“,”).toList) // List(List(“A”,”B”,”C”),List(“D”,”E”,”F”))
val textFlattened = textMapped.flatten          // List(“A”,”B”,”C”,”D”,”E”,”F”)
val textFlatMapped = text.flatMap(&lt;/em&gt;.split(“,”).toList) // List(“A”,”B”,”C”,”D”,”E”,”F”)&lt;/p&gt;

&lt;p&gt;3.reduce&lt;/p&gt;

&lt;p&gt;reduce&lt;a href=&quot;op: (A1, A1) ⇒ A1&quot;&gt;A1 &amp;gt;: A&lt;/a&gt;: A1&lt;/p&gt;

&lt;p&gt;定义一个变换f, f把两个列表的元素合成一个，遍历列表，最终把列表合并成单一元素&lt;/p&gt;

&lt;p&gt;Example 列表求和&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3)
val sum1 = nums.reduce((a,b) =&amp;gt; a+b)   //6
val sum2 = nums.reduce(&lt;em&gt;+&lt;/em&gt;)            //6
val sum3 = nums.sum                 //6&lt;/p&gt;

&lt;p&gt;4.reduceLeft,reduceRight&lt;/p&gt;

&lt;p&gt;reduceLeft: reduceLeft&lt;a href=&quot;f: (B, A) ⇒ B&quot;&gt;B &amp;gt;: A&lt;/a&gt;: B&lt;/p&gt;

&lt;p&gt;reduceRight: reduceRight&lt;a href=&quot;op: (A, B) ⇒ B&quot;&gt;B &amp;gt;: A&lt;/a&gt;: B&lt;/p&gt;

&lt;p&gt;reduceLeft从列表的左边往右边应用reduce函数，reduceRight从列表的右边往左边应用reduce函数&lt;/p&gt;

&lt;p&gt;Example&lt;/p&gt;

&lt;p&gt;val nums = List(2.0,2.0,3.0)
val resultLeftReduce = nums.reduceLeft(math.pow)  // = pow( pow(2.0,2.0) , 3.0) = 64.0
val resultRightReduce = nums.reduceRight(math.pow) // = pow(2.0, pow(2.0,3.0)) = 256.0&lt;/p&gt;

&lt;p&gt;5.fold,foldLeft,foldRight&lt;/p&gt;

&lt;p&gt;fold: fold&lt;a href=&quot;z: A1&quot;&gt;A1 &amp;gt;: A&lt;/a&gt;(op: (A1, A1) ⇒ A1): A1 带有初始值的reduce,从一个初始值开始，从左向右将两个元素合并成一个，最终把列表合并成单一元素。&lt;/p&gt;

&lt;p&gt;foldLeft: foldLeft&lt;a href=&quot;z: B&quot;&gt;B&lt;/a&gt;(f: (B, A) ⇒ B): B 带有初始值的reduceLeft&lt;/p&gt;

&lt;p&gt;foldRight: foldRight&lt;a href=&quot;z: B&quot;&gt;B&lt;/a&gt;(op: (A, B) ⇒ B): B 带有初始值的reduceRight&lt;/p&gt;

&lt;p&gt;val nums = List(2,3,4)
val sum = nums.fold(1)(&lt;em&gt;+&lt;/em&gt;)  // = 1+2+3+4 = 9&lt;/p&gt;

&lt;p&gt;val nums = List(2.0,3.0)
val result1 = nums.foldLeft(4.0)(math.pow) // = pow(pow(4.0,2.0),3.0) = 4096
val result2 = nums.foldRight(1.0)(math.pow) // = pow(1.0,pow(2.0,3.0)) = 8.0&lt;/p&gt;

&lt;p&gt;6.sortBy,sortWith,sorted&lt;/p&gt;

&lt;p&gt;sortBy: sortBy&lt;a href=&quot;f: (A) ⇒ B&quot;&gt;B&lt;/a&gt;(implicit ord: math.Ordering[B]): List[A] 按照应用函数f之后产生的元素进行排序&lt;/p&gt;

&lt;p&gt;sorted： sorted&lt;a href=&quot;implicit ord: math.Ordering[B]&quot;&gt;B &amp;gt;: A&lt;/a&gt;: List[A] 按照元素自身进行排序&lt;/p&gt;

&lt;p&gt;sortWith： sortWith(lt: (A, A) ⇒ Boolean): List[A] 使用自定义的比较函数进行排序&lt;/p&gt;

&lt;p&gt;val nums = List(1,3,2,4)
val sorted = nums.sorted  //List(1,2,3,4)&lt;/p&gt;

&lt;p&gt;val users = List((“HomeWay”,25),(“XSDYM”,23))
val sortedByAge = users.sortBy{case(user,age) =&amp;gt; age}  //List((“XSDYM”,23),(“HomeWay”,25))
val sortedWith = users.sortWith{case(user1,user2) =&amp;gt; user1._2 &amp;lt; user2._2} //List((“XSDYM”,23),(“HomeWay”,25))&lt;/p&gt;

&lt;p&gt;7.filter, filterNot&lt;/p&gt;

&lt;p&gt;filter: filter(p: (A) ⇒ Boolean): List[A]&lt;/p&gt;

&lt;p&gt;filterNot: filterNot(p: (A) ⇒ Boolean): List[A]&lt;/p&gt;

&lt;p&gt;filter 保留列表中符合条件p的列表元素 ， filterNot，保留列表中不符合条件p的列表元素&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3,4)
val odd = nums.filter( _ % 2 != 0) // List(1,3)
val even = nums.filterNot( _ % 2 != 0) // List(2,4)&lt;/p&gt;

&lt;p&gt;8.count&lt;/p&gt;

&lt;p&gt;count(p: (A) ⇒ Boolean): Int&lt;/p&gt;

&lt;p&gt;计算列表中所有满足条件p的元素的个数，等价于 filter(p).length&lt;/p&gt;

&lt;p&gt;val nums = List(-1,-2,0,1,2) val plusCnt1 = nums.count( &amp;gt; 0) val plusCnt2 = nums.filter( &amp;gt; 0).length&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;diff, union, intersect&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;diff:diff(that: collection.Seq[A]): List[A] 保存列表中那些不在另外一个列表中的元素，即从集合中减去与另外一个集合的交集&lt;/p&gt;

&lt;p&gt;union : union(that: collection.Seq[A]): List[A] 与另外一个列表进行连结&lt;/p&gt;

&lt;p&gt;intersect: intersect(that: collection.Seq[A]): List[A] 与另外一个集合的交集&lt;/p&gt;

&lt;p&gt;val nums1 = List(1,2,3)
val nums2 = List(2,3,4)
val diff1 = nums1 diff nums2   // List(1)
val diff2 = nums2.diff(num1)   // List(4)
val union1 = nums1 union nums2  // List(1,2,3,2,3,4)
val union2 = nums2 ++ nums1        // List(2,3,4,1,2,3)
val intersection = nums1 intersect nums2  //List(2,3)&lt;/p&gt;

&lt;p&gt;10.distinct&lt;/p&gt;

&lt;p&gt;distinct: List[A] 保留列表中非重复的元素，相同的元素只会被保留一次&lt;/p&gt;

&lt;p&gt;val list = List(“A”,”B”,”C”,”A”,”B”) val distincted = list.distinct // List(“A”,”B”,”C”)
1
11.groupBy, grouped&lt;/p&gt;

&lt;p&gt;groupBy : groupBy&lt;a href=&quot;f: (A) ⇒ K&quot;&gt;K&lt;/a&gt;: Map[K, List[A]] 将列表进行分组，分组的依据是应用f在元素上后产生的新元素 
grouped: grouped(size: Int): Iterator[List[A]] 按列表按照固定的大小进行分组&lt;/p&gt;

&lt;p&gt;val data = List((“HomeWay”,”Male”),(“XSDYM”,”Femail”),(“Mr.Wang”,”Male”))
val group1 = data.groupBy(_._2) // = Map(“Male” -&amp;gt; List((“HomeWay”,”Male”),(“Mr.Wang”,”Male”)),”Female” -&amp;gt; List((“XSDYM”,”Femail”)))
val group2 = data.groupBy{case (name,sex) =&amp;gt; sex} // = Map(“Male” -&amp;gt; List((“HomeWay”,”Male”),(“Mr.Wang”,”Male”)),”Female” -&amp;gt; List((“XSDYM”,”Femail”)))
val fixSizeGroup = data.grouped(2).toList // = Map(“Male” -&amp;gt; List((“HomeWay”,”Male”),(“XSDYM”,”Femail”)),”Female” -&amp;gt; List((“Mr.Wang”,”Male”)))&lt;/p&gt;

&lt;p&gt;12.scan&lt;/p&gt;

&lt;p&gt;scan&lt;a href=&quot;z: B&quot;&gt;B &amp;gt;: A, That&lt;/a&gt;(op: (B, B) ⇒ B)(implicit cbf: CanBuildFrom[List[A], B, That]): That&lt;/p&gt;

&lt;p&gt;由一个初始值开始，从左向右，进行积累的op操作，这个比较难解释，具体的看例子吧。&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3)
val result = nums.scan(10)(&lt;em&gt;+&lt;/em&gt;)   // List(10,10+1,10+1+2,10+1+2+3) = List(10,11,12,13)&lt;/p&gt;

&lt;p&gt;13.scanLeft,scanRight&lt;/p&gt;

&lt;p&gt;scanLeft: scanLeft&lt;a href=&quot;z: B&quot;&gt;B, That&lt;/a&gt;(op: (B, A) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That&lt;/p&gt;

&lt;p&gt;scanRight: scanRight&lt;a href=&quot;z: B&quot;&gt;B, That&lt;/a&gt;(op: (A, B) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That&lt;/p&gt;

&lt;p&gt;scanLeft: 从左向右进行scan函数的操作，scanRight：从右向左进行scan函数的操作&lt;/p&gt;

&lt;p&gt;val nums = List(1.0,2.0,3.0)
val result = nums.scanLeft(2.0)(math.pow)   // List(2.0,pow(2.0,1.0), pow(pow(2.0,1.0),2.0),pow(pow(pow(2.0,1.0),2.0),3.0) = List(2.0,2.0,4.0,64.0)
val result = nums.scanRight(2.0)(math.pow)  // List(2.0,pow(3.0,2.0), pow(2.0,pow(3.0,2.0)), pow(1.0,pow(2.0,pow(3.0,2.0))) = List(1.0,512.0,9.0,2.0)&lt;/p&gt;

&lt;p&gt;14.take,takeRight,takeWhile&lt;/p&gt;

&lt;p&gt;take : takeRight(n: Int): List[A] 提取列表的前n个元素 takeRight: takeRight(n: Int): List[A] 提取列表的最后n个元素 takeWhile: takeWhile(p: (A) ⇒ Boolean): List[A] 从左向右提取列表的元素，直到条件p不成立&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,1,1,4,4,4,4)
val left = nums.take(4)   // List(1,1,1,1)
val right = nums.takeRight(4) // List(4,4,4,4)
val headNums = nums.takeWhile( _ == nums.head)  // List(1,1,1,1)&lt;/p&gt;

&lt;p&gt;15.drop,dropRight,dropWhile&lt;/p&gt;

&lt;p&gt;drop: drop(n: Int): List[A] 丢弃前n个元素，返回剩下的元素 dropRight: dropRight(n: Int): List[A] 丢弃最后n个元素，返回剩下的元素 dropWhile: dropWhile(p: (A) ⇒ Boolean): List[A] 从左向右丢弃元素，直到条件p不成立&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,1,1,4,4,4,4)
val left = nums.drop(4)   // List(4,4,4,4)
val right = nums.dropRight(4) // List(1,1,1,1)
val tailNums = nums.dropWhile( _ == nums.head)  // List(4,4,4,4)&lt;/p&gt;

&lt;p&gt;16.span, splitAt, partition&lt;/p&gt;

&lt;p&gt;span : span(p: (A) ⇒ Boolean): (List[A], List[A]) 从左向右应用条件p进行判断，直到条件p不成立，此时将列表分为两个列表&lt;/p&gt;

&lt;p&gt;splitAt: splitAt(n: Int): (List[A], List[A]) 将列表分为前n个，与，剩下的部分&lt;/p&gt;

&lt;p&gt;partition: partition(p: (A) ⇒ Boolean): (List[A], List[A]) 将列表分为两部分，第一部分为满足条件p的元素，第二部分为不满足条件p的元素&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,1,2,3,2,1)
val (prefix,suffix) = nums.span( _ == 1) // prefix = List(1,1,1), suffix = List(2,3,2,1)
val (prefix,suffix) = nums.splitAt(3)  // prefix = List(1,1,1), suffix = List(2,3,2,1)
val (prefix,suffix) = nums.partition( _ == 1) // prefix = List(1,1,1,1), suffix = List(2,3,2)&lt;/p&gt;

&lt;p&gt;17.padTo&lt;/p&gt;

&lt;p&gt;padTo(len: Int, elem: A): List[A]&lt;/p&gt;

&lt;p&gt;将列表扩展到指定长度，长度不够的时候，使用elem进行填充，否则不做任何操作。&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,1)
 val padded = nums.padTo(6,2)   // List(1,1,1,2,2,2)&lt;/p&gt;

&lt;p&gt;18.combinations,permutations&lt;/p&gt;

&lt;p&gt;combinations: combinations(n: Int): Iterator[List[A]] 取列表中的n个元素进行组合，返回不重复的组合列表，结果一个迭代器&lt;/p&gt;

&lt;p&gt;permutations: permutations: Iterator[List[A]] 对列表中的元素进行排列，返回不重得的排列列表，结果是一个迭代器&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,3)
val combinations = nums.combinations(2).toList //List(List(1,1),List(1,3))
val permutations = nums.permutations.toList        // List(List(1,1,3),List(1,3,1),List(3,1,1))&lt;/p&gt;

&lt;p&gt;19.zip, zipAll, zipWithIndex, unzip,unzip3&lt;/p&gt;

&lt;p&gt;zip: zip&lt;a href=&quot;that: GenIterable[B]&quot;&gt;B&lt;/a&gt;: List[(A, B)] 与另外一个列表进行拉链操作，将对应位置的元素组成一个pair，返回的列表长度为两个列表中短的那个&lt;/p&gt;

&lt;p&gt;zipAll: zipAll&lt;a href=&quot;that: collection.Iterable[B], thisElem: A, thatElem: B&quot;&gt;B&lt;/a&gt;: List[(A, B)] 与另外一个列表进行拉链操作，将对应位置的元素组成一个pair，若列表长度不一致，自身列表比较短的话使用thisElem进行填充，对方列表较短的话使用thatElem进行填充&lt;/p&gt;

&lt;p&gt;zipWithIndex：zipWithIndex: List[(A, Int)] 将列表元素与其索引进行拉链操作，组成一个pair&lt;/p&gt;

&lt;p&gt;unzip: unzip&lt;a href=&quot;implicit asPair: (A) ⇒ (A1, A2)&quot;&gt;A1, A2&lt;/a&gt;: (List[A1], List[A2]) 解开拉链操作&lt;/p&gt;

&lt;p&gt;unzip3: unzip3&lt;a href=&quot;implicit asTriple: (A) ⇒ (A1, A2, A3)&quot;&gt;A1, A2, A3&lt;/a&gt;: (List[A1], List[A2], List[A3]) 3个元素的解拉链操作&lt;/p&gt;

&lt;p&gt;val alphabet = List(“A”,B”,”C”)
val nums = List(1,2)
val zipped = alphabet zip nums   // List((“A”,1),(“B”,2))
val zippedAll = alphabet.zipAll(nums,”*”,-1)   // List((“A”,1),(“B”,2),(“C”,-1))
val zippedIndex = alphabet.zipWithIndex  // List((“A”,0),(“B”,1),(“C”,3))
val (list1,list2) = zipped.unzip        // list1 = List(“A”,”B”), list2 = List(1,2)
val (l1,l2,l3) = List((1, “one”, ‘1’),(2, “two”, ‘2’),(3, “three”, ‘3’)).unzip3   // l1=List(1,2,3),l2=List(“one”,”two”,”three”),l3=List(‘1’,’2’,’3’)&lt;/p&gt;

&lt;p&gt;20.slice&lt;/p&gt;

&lt;p&gt;slice(from: Int, until: Int): List[A] 提取列表中从位置from到位置until(不含该位置)的元素列表&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3,4,5)
val sliced = nums.slice(2,4)  //List(3,4)&lt;/p&gt;

&lt;p&gt;21.sliding&lt;/p&gt;

&lt;p&gt;sliding(size: Int, step: Int): Iterator[List[A]] 将列表按照固定大小size进行分组，步进为step，step默认为1,返回结果为迭代器&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,2,2,3,3,4,4)
val groupStep2 = nums.sliding(2,2).toList  //List(List(1,1),List(2,2),List(3,3),List(4,4))
val groupStep1 = nums.sliding(2).toList //List(List(1,1),List(1,2),List(2,2),List(2,3),List(3,3),List(3,4),List(4,4))&lt;/p&gt;

&lt;p&gt;22.updated&lt;/p&gt;

&lt;p&gt;updated(index: Int, elem: A): List[A] 对列表中的某个元素进行更新操作&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3,3)
val fixed = nums.updated(3,4)  // List(1,2,3,4)&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/11/scala_list.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/11/scala_list.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark-session-context</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;初始化Spark
一个Spark程序首先必须要做的是创建一个SparkContext对象，这个对象告诉Spark如何访问一个集群。为了创建一个SparkContext，你首先需要构建一个包含了关于你的应用的信息的SparkConf对象。&lt;/p&gt;

&lt;p&gt;只有一个SparkContext可以激活一个JVM。你必须在创建一个新的SparkContext之前stop()掉这个激活的SparkContext。&lt;/p&gt;

&lt;p&gt;val conf = new SparkConf().setAppName(appName).setMaster(master)
new SparkContext(conf)
参数appName是在集群UI上展示的你的应用的名字。master是一个Spark，Mesos或者YARN集群的URL，或者是表示以本地模式运行的一个特定的字符串”local”。在实践中，当运行在集群上时，你是不会想在程序中硬编码master的，而是通过spark-submit启动应用并指定master的。然而，对于本地测试和单元测试，你可以通过传递”local”来在进程内运行Spark。&lt;/p&gt;

&lt;p&gt;使用shell
在Spark shell中，一个特殊的SparkContext已经为你创建好了，变量名是sc。如果再创建你自己的SparkContext就不起作用了。你可以使用参数–master来设置context连接到哪个master，还可以通过给参数–jars传递逗号分隔的列表来给classpath增加JARs。你还可以通过给参数–packages提供逗号分隔的Maven坐标来给你的shell会话增加依赖（例如Spark包）。任何可能存在依赖的附加库（例如Sonatype）都可以被传递给参数–repositories&lt;/p&gt;

&lt;p&gt;　在Spark的早期版本，sparkContext是进入Spark的切入点。我们都知道RDD是Spark中重要的API，然而它的创建和操作得使用sparkContext提供的API；对于RDD之外的其他东西，我们需要使用其他的Context。比如对于流处理来说，我们得使用StreamingContext；对于SQL得使用sqlContext；而对于hive得使用HiveContext。然而DataSet和Dataframe提供的API逐渐称为新的标准API，我们需要一个切入点来构建它们，所以在 Spark 2.0中我们引入了一个新的切入点(entry point)：SparkSession&lt;/p&gt;

&lt;p&gt;　　SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。&lt;/p&gt;

&lt;p&gt;创建SparkSession&lt;/p&gt;

&lt;p&gt;　　SparkSession的设计遵循了工厂设计模式（factory design pattern），下面代码片段介绍如何创建SparkSession
[python] view plain copy
val sparkSession = SparkSession.builder.&lt;br /&gt;
      master(“local”)&lt;br /&gt;
      .appName(“spark session example”)&lt;br /&gt;
      .getOrCreate()&lt;br /&gt;
上面代码类似于创建一个SparkContext，master设置为local，然后创建了一个SQLContext封装它。如果你想创建hiveContext，可以使用下面的方法来创建SparkSession，以使得它支持Hive：
[python] view plain copy
val sparkSession = SparkSession.builder.&lt;br /&gt;
      master(“local”)&lt;br /&gt;
      .appName(“spark session example”)&lt;br /&gt;
      .enableHiveSupport()&lt;br /&gt;
      .getOrCreate()&lt;br /&gt;
enableHiveSupport 函数的调用使得SparkSession支持hive，类似于HiveContext。&lt;/p&gt;

&lt;p&gt;spark2.0 主要变化
1 更容易的SQL和Streamlined APIs&lt;/p&gt;

&lt;p&gt;Spark 2.0主要聚焦于两个方面：（1）、对标准的SQL支持（2）、统一DataFrame和Dataset API。&lt;/p&gt;

&lt;p&gt;　　在SQL方面，Spark 2.0已经显著地扩大了它的SQL功能，比如引进了一个新的ANSI SQL解析器和对子查询的支持。现在Spark 2.0已经可以运行TPC-DS所有的99个查询，这99个查询需要SQL 2003的许多特性。因为SQL是Spark应用程序的主要接口之一，Spark 2.0 SQL的扩展大幅减少了应用程序往Spark迁移的代价。&lt;/p&gt;

&lt;p&gt;　　在编程API方面，我们对API进行了精简。&lt;/p&gt;

&lt;p&gt;　　1、统一Scala和Java中DataFrames和Datasets的API：从Spark 2.0开始，DataFrame仅仅是Dataset的一个别名。有类型的方法(typed methods)（比如：map, filter, groupByKey）和无类型的方法(untyped methods)(比如：select, groupBy)目前在Dataset类上可用。同样，新的Dataset接口也在Structured Streaming中使用。因为编译时类型安全(compile-time type-safety)在Python和R中并不是语言特性，所以Dataset的概念并不在这些语言中提供相应的API。而DataFrame仍然作为这些语言的主要编程抽象。&lt;/p&gt;

&lt;p&gt;　　2、SparkSession：一个新的切入点，用于替代旧的SQLContext和HiveContext。对于那些使用DataFrame API的用户，一个常见的困惑就是我们正在使用哪个context？现在我们可以使用SparkSession了，其涵括了SQLContext和HiveContext，仅仅提供一个切入点。需要注意的是为了向后兼容，旧的SQLContext和HiveContext目前仍然可以使用。&lt;/p&gt;

&lt;p&gt;　　3、简单以及性能更好的Accumulator API：Spark 2.0中设计出一种新的Accumulator API，它拥有更加简洁的类型层次，而且支持基本类型。为了向后兼容，旧的Accumulator API仍然可以使用。&lt;/p&gt;

&lt;p&gt;　　4、基于DataFrame的Machine Learning API可以作为主要的ML API了：在Spark 2.0中， spark.ml包以其pipeline API将会作为主要的机器学习API了，而之前的spark.mllib仍然会保存，将来的开发会聚集在基于DataFrame的API上。&lt;/p&gt;

&lt;p&gt;　　5、Machine learning pipeline持久化：现在用户可以保存和加载Spark支持所有语言的Machine learning pipeline和models。&lt;/p&gt;

&lt;p&gt;　　6、R的分布式算法：在R语言中添加支持了Generalized Linear Models (GLM), Naive Bayes, Survival Regression, and K-Means。
　　
2 更快：Spark作为编译器&lt;/p&gt;

&lt;p&gt;Spark 2.0中附带了第二代Tungsten engine，这一代引擎是建立在现代编译器和MPP数据库的想法上，并且把它们应用于数据的处理过程中。主要想法是通过在运行期间优化那些拖慢整个查询的代码到一个单独的函数中，消除虚拟函数的调用以及利用CPU寄存器来存放那些中间数据。我们把这些技术称为”整段代码生成”(whole-stage code generation)。&lt;/p&gt;

&lt;p&gt;3 更加智能：Structured Streaming&lt;/p&gt;

</description>
        <pubDate>Fri, 06 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/06/spark-session-context.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/06/spark-session-context.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>scala_main_class</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;解决
错误: 找不到或无法加载主类 SparkTest.SparkSessionTest&lt;/p&gt;

&lt;p&gt;project-&amp;gt;buildin path-&amp;gt; configure build in path 
-&amp;gt;scala compiler -&amp;gt;use project settings -&amp;gt;scala installation
-&amp;gt; dixed scala installation 2.11.8 built in&lt;/p&gt;
</description>
        <pubDate>Fri, 29 Sep 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/09/29/scala_main_class.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/09/29/scala_main_class.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>alibaba_fast_json</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Fastjson是一个Java语言编写的高性能功能完善的JSON库。将解析json的性能提升到极致，是目前Java语言中最快的JSON库。Fastjson接口简单易用，已经被广泛使用在缓存序列化、协议交互、Web输出、Android客户端等多种应用场景。&lt;/p&gt;

&lt;p&gt;GitHub下载地址: 
https://github.com/alibaba/fastjson&lt;/p&gt;

&lt;p&gt;最新发布版本jar包 1.2.23 下载地址: https://search.maven.org/remote_content?g=com.alibaba&amp;amp;a=fastjson&amp;amp;v=LATEST&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;com.alibaba.fastjson.JSON&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FastJsonExp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]){&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{\&amp;quot;name\&amp;quot;:\&amp;quot;chenggang\&amp;quot;,\&amp;quot;age\&amp;quot;:24}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5 &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//反序列化&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6 &lt;/span&gt; &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;userInfo&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parseObject&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;7 &lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;name:&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;, age:&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;8 &lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;9 &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
</description>
        <pubDate>Fri, 29 Sep 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/09/29/alibaba_fast_json.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/09/29/alibaba_fast_json.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>gorpc</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;gRPC初体验
96  作者 CZ_Golang 关注
2016.03.11 16:38 字数 1629 阅读 16506评论 2喜欢 29
gRPC是由Google主导开发的RPC框架，使用HTTP/2协议并用ProtoBuf作为序列化工具。其客户端提供Objective-C、Java接口，服务器侧则有Java、Golang、C++等接口，从而为移动端（iOS/Androi）到服务器端通讯提供了一种解决方案。 当然在当下的环境下，这种解决方案更热门的方式是RESTFull API接口。该方式需要自己去选择编码方式、服务器架构、自己搭建框架（JSON-RPC）。gRPC官方对REST的声音是：&lt;/p&gt;

&lt;p&gt;和REST一样遵循HTTP协议(明确的说是HTTP/2)，但是gRPC提供了全双工流
和传统的REST不同的是gRPC使用了静态路径，从而提高性能
用一些格式化的错误码代替了HTTP的状态码更好的标示错误
至于是否要选择用gRPC。对于已经有一套方案的团队，可以参考下。如果是从头来做，可以考虑下gRPC提供的从客户端到服务器的整套解决方案，这样不用客户端去实现http的请求会话，JSON等的解析，服务器端也有现成的框架用。从15年3月到现在gRPC也发展了一年了，慢慢趋于成熟。下面我们就以gRPC的Golang版本看下其在golang上面的表现。至于服务端的RPC，感觉golang标准库的RPC框架基本够用了,没必要再去用另一套方案。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;安装protobuf&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;虽然gRPC也支持protobuf2.x，但是建议还是使用protobuf3.x，尽管还没有正式版本，不过golang版本基本没有什么问题，另外3.x官方支持了Objective-C，这也是我们使用gRPC的初衷：提供一个移动端到服务器的解决方案。去到Protocol Buffers下载最新版本（Version3.0.0 beta2），然后解压到本地。本地需要已经安装好autoconf automake libtool.rpm系列（fedora/centos/redheat）可以用yum安装。Mac上可以用brew进行安装&lt;/p&gt;

&lt;p&gt;brew install autoconf automake libtool
然后执行&lt;/p&gt;

&lt;p&gt;./configure –prefix=your_pb_install_path
接着&lt;/p&gt;

&lt;p&gt;make 
make install
set your_pb_install_path to your $PATH
检查是否安装完成&lt;/p&gt;

&lt;p&gt;protoc –version
libprotoc 3.0.0
然后安装golang protobuf直接使用golang的get即可&lt;/p&gt;

&lt;p&gt;go get -u github.com/golang/protobuf/proto // golang protobuf 库
go get -u github.com/golang/protobuf/protoc-gen-go //protoc –go_out 工具&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;安装gRPC-go&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;gRPC-go可以通过golang 的get命令直接安装，非常方便。&lt;/p&gt;

&lt;p&gt;go get google.golang.org/grpc
这里大家可能比较奇怪，为什么gRPC-go在github的地址是”https://github.com/grpc/grpc-go”,但是为什么要用“google.golang.org/grpc”进行安装呢？应该grpc原本是google内部的项目，归属golang，就放在了google.golang.org下面了，后来对外开放，又将其迁移到github上面了，又因为golang比较坑爹的import路径规则，所以就都没有改路径名了。&lt;/p&gt;

&lt;p&gt;但是这样就有个问题了。要如何去管理版本呢？这个目前我还没有什么比较好的方法，希望知道的朋友一起分享下。目前想到一个方法是手动下载某个版本，然后写个脚本统一修改代码中的import里面的路径.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;示例程序&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3.1 protobuf&lt;/p&gt;

&lt;p&gt;该示例源自gRPC-go的examples的helloworld。先看PB的描述：&lt;/p&gt;

&lt;p&gt;syntax = “proto3”;&lt;/p&gt;

&lt;p&gt;option objc_class_prefix = “HLW”;&lt;/p&gt;

&lt;p&gt;package helloworld;&lt;/p&gt;

&lt;p&gt;// The greeting service definition.
service Greeter {
  // Sends a greeting
  rpc SayHello (HelloRequest) returns (HelloReply) {}
}&lt;/p&gt;

&lt;p&gt;// The request message containing the user’s name.
message HelloRequest {
  string name = 1;
}&lt;/p&gt;

&lt;p&gt;// The response message containing the greetings
message HelloReply {
  string message = 1;
}
这里定义了一个服务Greeter，其中有个API SayHello。其接受参数为HelloRequest类型，返回HelloReply类型。这里HelloRequest和HelloReply就是普通的PB定义&lt;/p&gt;

&lt;p&gt;服务定义为：&lt;/p&gt;

&lt;p&gt;// The greeting service definition.
service Greeter {
  // Sends a greeting
  rpc SayHello (HelloRequest) returns (HelloReply) {}
}
service定义了一个server。其中的接口可以是四种类型&lt;/p&gt;

&lt;p&gt;rpc GetFeature(Point) returns (Feature) {}
类似普通的函数调用，客户端发送请求Point到服务器，服务器返回相应Feature.
rpc ListFeatures(Rectangle) returns (stream Feature) {}
客户端发起一次请求，服务器端返回一个流式数据，比如一个数组中的逐个元素
rpc RecordRoute(stream Point) returns (RouteSummary) {}
客户端发起的请求是一个流式的数据，比如数组中的逐个元素，服务器返回一个相应
rpc RouteChat(stream RouteNote) returns (stream RouteNote) {}
客户端发起的请求是一个流式数据，比如数组中的逐个元素，二服务器返回的也是一个类似的数据结构
后面三种可以参考官方的route_guide示例。&lt;/p&gt;

&lt;p&gt;使用protoc命令生成相关文件：&lt;/p&gt;

&lt;p&gt;protoc –go_out=plugins=grpc:. helloworld.proto
ls
helloworld.pb.go    helloworld.proto
生成对应的pb.go文件。这里用了plugins选项，提供对grpc的支持，否则不会生成Service的接口。&lt;/p&gt;

&lt;p&gt;3.2 服务器端程序&lt;/p&gt;

&lt;p&gt;然后编辑服务器端程序：&lt;/p&gt;

&lt;p&gt;package main&lt;/p&gt;

&lt;p&gt;import (
    “log”
    “net”&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pb &quot;your_path_to_gen_pb_dir/helloworld&quot;
&quot;golang.org/x/net/context&quot;
&quot;google.golang.org/grpc&quot; )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;const (
    port = “:50051”
)&lt;/p&gt;

&lt;p&gt;// server is used to implement helloworld.GreeterServer.
type server struct{}&lt;/p&gt;

&lt;p&gt;// SayHello implements helloworld.GreeterServer
func (s &lt;em&gt;server) SayHello(ctx context.Context, in *pb.HelloRequest) (&lt;/em&gt;pb.HelloReply, error) {
    return &amp;amp;pb.HelloReply{Message: “Hello “ + in.Name}, nil
}&lt;/p&gt;

&lt;p&gt;func main() {
    lis, err := net.Listen(“tcp”, port)
    if err != nil {
        log.Fatalf(“failed to listen: %v”, err)
    }
    s := grpc.NewServer()
    pb.RegisterGreeterServer(s, &amp;amp;server{})
    s.Serve(lis)
}
这里首先定义一个server结构，然后实现SayHello的接口，其定义在“your_path_to_gen_pb_dir/helloworld”&lt;/p&gt;

&lt;p&gt;SayHello(context.Context, &lt;em&gt;HelloRequest) (&lt;/em&gt;HelloReply, error)
然后调用grpc.NewServer() 创建一个server s。接着注册这个server s到结构server上面 pb.RegisterGreeterServer(s, &amp;amp;server{}) 最后将创建的net.Listener传给s.Serve()。就可以开始监听并服务了，类似HTTP的ListenAndServe。&lt;/p&gt;

&lt;p&gt;3.3 客户端程序&lt;/p&gt;

&lt;p&gt;客户端程序：&lt;/p&gt;

&lt;p&gt;package main&lt;/p&gt;

&lt;p&gt;import (
    “log”
    “os”&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pb &quot;your_path_to_gen_pb_dir/helloworld&quot;
&quot;golang.org/x/net/context&quot;
&quot;google.golang.org/grpc&quot; )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;const (
    address     = “localhost:50051”
    defaultName = “world”
)&lt;/p&gt;

&lt;p&gt;func main() {
    // Set up a connection to the server.
    conn, err := grpc.Dial(address, grpc.WithInsecure())
    if err != nil {
        log.Fatalf(“did not connect: %v”, err)
    }
    defer conn.Close()
    c := pb.NewGreeterClient(conn)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Contact the server and print out its response.
name := defaultName
if len(os.Args) &amp;gt; 1 {
    name = os.Args[1]
}
r, err := c.SayHello(context.Background(), &amp;amp;pb.HelloRequest{Name: name})
if err != nil {
    log.Fatalf(&quot;could not greet: %v&quot;, err)
}
log.Printf(&quot;Greeting: %s&quot;, r.Message) } 这里通过pb.NewGreeterClient()传入一个conn创建一个client，然后直接调用client上面对应的服务器的接口
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SayHello(context.Context, &lt;em&gt;HelloRequest) (&lt;/em&gt;HelloReply, error)
接口，返回*HelloReply 对象。&lt;/p&gt;

&lt;p&gt;先运行服务器，在运行客户端，可以看到。&lt;/p&gt;

&lt;p&gt;./greeter_server &amp;amp;&lt;/p&gt;

&lt;p&gt;./greeter_client
2016/03/10 21:42:19 Greeting: Hello world&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Sep 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/jekyll/2017/09/25/gorpc.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/jekyll/2017/09/25/gorpc.html</guid>
        
        
        <category>jekyll</category>
        
      </item>
    
      <item>
        <title>spark-kafka</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;参考文档：
https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html&lt;/p&gt;

&lt;p&gt;Reading Data from Kafka
Creating a Kafka Source for Streaming Queries
Scala
Java
Python
// Subscribe to 1 topic
val df = spark
  .readStream
  .format(“kafka”)
  .option(“kafka.bootstrap.servers”, “host1:port1,host2:port2”)
  .option(“subscribe”, “topic1”)
  .load()
df.selectExpr(“CAST(key AS STRING)”, “CAST(value AS STRING)”)
  .as[(String, String)]&lt;/p&gt;

&lt;p&gt;// Subscribe to multiple topics
val df = spark
  .readStream
  .format(“kafka”)
  .option(“kafka.bootstrap.servers”, “host1:port1,host2:port2”)
  .option(“subscribe”, “topic1,topic2”)
  .load()
df.selectExpr(“CAST(key AS STRING)”, “CAST(value AS STRING)”)
  .as[(String, String)]&lt;/p&gt;

&lt;p&gt;// Subscribe to a pattern
val df = spark
  .readStream
  .format(“kafka”)
  .option(“kafka.bootstrap.servers”, “host1:port1,host2:port2”)
  .option(“subscribePattern”, “topic.*”)
  .load()
df.selectExpr(“CAST(key AS STRING)”, “CAST(value AS STRING)”)
  .as[(String, String)]&lt;/p&gt;

&lt;p&gt;Writing Data to Kafka&lt;/p&gt;

&lt;p&gt;Creating a Kafka Sink for Streaming Queries
Scala
Java
Python
// Write key-value data from a DataFrame to a specific Kafka topic specified in an option
val ds = df
  .selectExpr(“CAST(key AS STRING)”, “CAST(value AS STRING)”)
  .writeStream
  .format(“kafka”)
  .option(“kafka.bootstrap.servers”, “host1:port1,host2:port2”)
  .option(“topic”, “topic1”)
  .start()&lt;/p&gt;

&lt;p&gt;// Write key-value data from a DataFrame to Kafka using a topic specified in the data
val ds = df
  .selectExpr(“topic”, “CAST(key AS STRING)”, “CAST(value AS STRING)”)
  .writeStream
  .format(“kafka”)
  .option(“kafka.bootstrap.servers”, “host1:port1,host2:port2”)
  .start()
Writing the output of Batch Queries to Kafka
Scala
Java
Python
// Write key-value data from a DataFrame to a specific Kafka topic specified in an option
df.selectExpr(“CAST(key AS STRING)”, “CAST(value AS STRING)”)
  .write
  .format(“kafka”)
  .option(“kafka.bootstrap.servers”, “host1:port1,host2:port2”)
  .option(“topic”, “topic1”)
  .save()&lt;/p&gt;

&lt;p&gt;// Write key-value data from a DataFrame to Kafka using a topic specified in the data
df.selectExpr(“topic”, “CAST(key AS STRING)”, “CAST(value AS STRING)”)
  .write
  .format(“kafka”)
  .option(“kafka.bootstrap.servers”, “host1:port1,host2:port2”)
  .save()&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Sep 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/09/22/spark-kafka.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/09/22/spark-kafka.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>mysqldump</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;mysqldump  -P端口  -hIP -u用户名 -p密码 表名 库名 &amp;gt; 目标文件.sql&lt;/p&gt;

&lt;p&gt;mysqldump: [Warning] Using a password on the command line interface can be insecure.&lt;/p&gt;

&lt;p&gt;mysqldump  -P端口  -hIP -u用户名 -p 表名 库名 &amp;gt; 目标文件.sql&lt;/p&gt;

&lt;p&gt;然后输入密码&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Sep 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/09/20/mysqldump.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/09/20/mysqldump.html</guid>
        
        
        <category>web</category>
        
      </item>
    
  </channel>
</rss>
