<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 09 Dec 2017 16:10:00 +0800</pubDate>
    <lastBuildDate>Sat, 09 Dec 2017 16:10:00 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>linux sysfs</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;在调试驱动，或驱动涉及一些参数的输入输出时，难免需要对驱动里的某些变量或内核参数进行读写，或函数调用。此时sysfs接口就很有用了，它可以使得可以在用户空间直接对驱动的这些变量读写或调用驱动的某些函数。sysfs接口与proc文件系统很相似，有人将proc文件系统形容为Windows XP，而将sysfs接口形容为Windows 7。
而在Android系统中，振动器、背光、电源系统等往往使用sysfs接口作为内核空间和用户空间的接口，驱动程序需要提供这些接口内容。&lt;/p&gt;

&lt;p&gt;Sysfs文件系统是一个类 似于proc文件系统的特殊文件系统，用于将系统中的设备组织成层次结构，并向用户模式程序提供详细的内核数据结构信息。&lt;/p&gt;

&lt;p&gt;去/sys看一看，
localhost:/sys#ls /sys/
block/ bus/ class/ devices/ firmware/ kernel/ module/ power/
Block目录：包含所有的块设备
Devices目录：包含系统所有的设备，并根据设备挂接的总线类型组织成层次结构
Bus目录：包含系统中所有的总线类型
Drivers目录：包括内核中所有已注册的设备驱动程序
Class目录：系统中的设备类型（如网卡设备，声卡设备等） 
sys下面的目录和文件反映了整台机器的系统状况。比如bus，
localhost:/sys/bus#ls
i2c/ ide/ pci/ pci express/ platform/ pnp/ scsi/ serio/ usb/
里面就包含了系统用到的一系列总线，比如pci, ide, scsi, usb等等。比如你可以在usb文件夹中发现你使用的U盘，USB鼠标的信息。
我们要讨论一个文件系统，首先要知道这个文件系统的信息来源在哪里。所谓信息来源是指文件组织存放的地点。比如，我们挂载一个分区，
mount -t vfat /dev/hda2 /mnt/C
我们就知道挂载在/mnt/C下的是一个vfat类型的文件系统，它的信息来源是在第一块硬盘的第2个分区。
但是，你可能根本没有去关心过sysfs的挂载过程，她是这样被挂载的。
mount -t sysfs sysfs /sys
ms看不出她的信息来源在哪。sysfs是一个特殊文件系统，并没有一个实际存放文件的介质。断电后就玩完了。简而言之，sysfs的信息来源是kobject层次结构，读一个sysfs文件，就是动态的从kobject结构提取信息，生成文件。&lt;/p&gt;

&lt;p&gt;Kobject 
Kobject 是Linux 2.6引入的新的设备管理机制，在内核中由struct kobject表示。通过这个数据结构使所有设备在底层都具有统一的接口，kobject提供基本的对象管理，是构成Linux2.6设备模型的核心结 构，它与sysfs文件系统紧密关联，每个在内核中注册的kobject对象都对应于sysfs文件系统中的一个目录。Kobject是组成设备模型的基 本结构。类似于C++中的基类，它嵌入于更大的对象的对象中–所谓的容器–用来描述设备模型的组件。如bus,devices, drivers 都是典型的容器。这些容器就是通过kobject连接起来了，形成了一个树状结构。这个树状结构就与/sys向对应。
kobject 结构为一些大的数据结构和子系统提供了基本的对象管理，避免了类似机能的重复实现。这些机能包括&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;对象引用计数.&lt;/li&gt;
  &lt;li&gt;维护对象链表(集合).&lt;/li&gt;
  &lt;li&gt;对象上锁.&lt;/li&gt;
  &lt;li&gt;在用户空间的表示.
Kobject结构定义为：
struct kobject {
char * k name; 指向设备名称的指针
char name[KOBJ NAME LEN]; 设备名称
struct kref kref; 对象引用计数
struct list head entry; 挂接到所在kset中去的单元
struct kobject * parent; 指向父对象的指针
struct kset * kset; 所属kset的指针
struct kobj type * ktype; 指向其对象类型描述符的指针
struct dentry * dentry; sysfs文件系统中与该对象对应的文件节点路径指针
};
其 中的kref域表示该对象引用的计数，内核通过kref实现对象引用计数管理，内核提供两个函数kobject_get()、kobject_put() 分别用于增加和减少引用计数，当引用计数为0时，所有该对象使用的资源释放。Ktype 域是一个指向kobj type结构的指针，表示该对象的类型。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kobj type 
struct kobj_type {
void (*release)(struct kobject *);
struct sysfs_ops * sysfs_ops;
struct attribute ** default_attrs;
};
Kobj type数据结构包含三个域：一个release方法用于释放kobject占用的资源；一个sysfs ops指针指向sysfs操作表和一个sysfs文件系统缺省属性列表。Sysfs操作表包括两个函数store()和show()。当用户态读取属性 时，show()函数被调用，该函数编码指定属性值存入buffer中返回给用户态；而store()函数用于存储用户态传入的属性值。&lt;/p&gt;

&lt;p&gt;kset 
kset最重要的是建立上层(sub-system)和下层的 (kobject)的关联性。kobject 也会利用它了分辨自已是属于那一個类型，然後在/sys 下建立正确的目录位置。而kset 的优先权比较高，kobject会利用自已的&lt;em&gt;kset 找到自已所属的kset，並把&lt;/em&gt;ktype 指定成該kset下的ktype，除非沒有定义kset，才会用ktype來建立关系。Kobject通过kset组织成层次化的结构，kset是具有相 同类型的kobject的集合，在内核中用kset数据结构表示，定义为：
struct kset {
struct subsystem * subsys; 所在的subsystem的指针
struct kobj type * ktype; 指向该kset对象类型描述符的指针
struct list head list; 用于连接该kset中所有kobject的链表头
struct kobject kobj; 嵌入的kobject
struct kset hotplug ops * hotplug ops; 指向热插拔操作表的指针
};&lt;/p&gt;

&lt;p&gt;包 含在kset中的所有kobject被组织成一个双向循环链表，list域正是该链表的头。Ktype域指向一个kobj type结构，被该kset中的所有kobject共享，表示这些对象的类型。Kset数据结构还内嵌了一个kobject对象（由kobj域表示），所 有属于这个kset 的kobject对象的parent域均指向这个内嵌的对象。此外，kset还依赖于kobj维护引用计数：kset的引用计数实际上就是内嵌的 kobject对象的引用计数。&lt;/p&gt;

&lt;p&gt;subsystem 
如果說kset 是管理kobject 的集合，同理，subsystem 就是管理kset 的集合。它描述系统中某一类设备子系统，如block subsys表示所有的块设备，对应于sysfs文件系统中的block目录。类似的，devices subsys对应于sysfs中的devices目录，描述系统中所有的设备。Subsystem由struct subsystem数据结构描述，定义为：
struct subsystem {
struct kset kset; 内嵌的kset对象
struct rw semaphore rwsem; 互斥访问信号量
};
可以看出，subsystem与kset的区别就是多了一个信号量，所以在后来的代码中，subsystem已经完全被kset取缔了。
每个kset属于某个subsystem，通过设置kset结构中的subsys域指向指定的subsystem可以将一个kset加入到该subsystem。所有挂接到同一subsystem的kset共享同一个rwsem信号量，用于同步访问kset中的链表。&lt;/p&gt;

&lt;p&gt;sysfs是用于表现设备驱动模型的文件系统，它基于ramfs。要学习linux的设备驱动模型，就要先做好底层工作，总结sysfs提供给外界的API就是其中之一。sysfs文件系统中提供了四类文件的创建与管理，分别是目录、普通文件、软链接文件、二进制文件。目录层次往往代表着设备驱动模型的结构，软链接文件则代表着不同部分间的关系。比如某个设备的目录只出现在/sys/devices下，其它地方涉及到它时只好用软链接文件链接过去，保持了设备唯一的实例。而普通文件和二进制文件往往代表了设备的属性，读写这些文件需要调用相应的属性读写。
    sysfs是表现设备驱动模型的文件系统，它的目录层次实际反映的是对象的层次。为了配合这种目录，linux专门提供了两个结构作为sysfs的骨架，它们就是struct kobject和struct kset。我们知道，sysfs是完全虚拟的，它的每个目录其实都对应着一个kobject，要想知道这个目录下有哪些子目录，就要用到kset。从面向对象的角度来讲，kset继承了kobject的功能，既可以表示sysfs中的一个目录，还可以包含下层目录。对于kobject和kset，会在其它文章中专门分析到，这里简单描述只是为了更好地介绍sysfs提供的API。
sysfs 与 proc 相比有很多优点，最重要的莫过于设计上的清晰。一个 proc 虚拟文件可能有内部格式，如 /proc/scsi/scsi
，它是可读可写的，(其文件权限被错误地标记为了 0444
！，这是内核的一个BUG)，并且读写格式不一样，代表不同的操作，应用程序中读到了这个文件的内容一般还需要进行字符串解析，而在写入时需要先用字符串
格式化按指定的格式写入字符串进行操作；相比而言， sysfs 的设计原则是一个属性文件只做一件事情， sysfs
属性文件一般只有一个值，直接读取或写入。整个 /proc/scsi 目录在2.6内核中已被标记为过时(LEGACY)，它的功能已经被相应的 /sys 属性文件所完全取代。新设计的内核机制应该尽量使用 sysfs 机制，而将 proc 保留给纯净的“进程文件系统”。&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/09/sysfs.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/09/sysfs.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>proc文件系统</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;proc文件系统是一种无存储的文件系统，当读其中的文件时，其内容动态生成，当写文件时，文件所关联的写函数被调用。每个proc文件都关联的字节特定的读写函数，因而它提供了另外的一种和内核通信的机制：内核部件可以通过该文件系统向用户空间提供接口来提供查询信息、修改软件行为，因而它是一种比较重要的特殊文件系统。
由于proc文件系统以文件的形式向用户空间提供了访问接口，这些接口可以用于在运行时获取相关部件的信息或者修改部件的行为，因而它是非常方便的一个接口。内核中大量使用了该文件系统。proc文件系统就是一个文件系统，它可以挂载在目录树的任意位置，不过通常挂载在/proc下，它大致包含了如下信息：
内存管理
每个进程的相关信息
文件系统
设备驱动程序
系统总线
电源管理
终端
系统控制参数
网络
使用proc文件系统之前必须将其初始化并且挂载到系统中。proc文件系统的的初始化主要完成：
调用proc_init_inodecache创建proc文件系统所使用的专用缓冲区
调用register_filesystem注册proc文件系统，这里会提供proc文件系统自己的file_system_type，其中包括了用于mount的函数指针。在执行mount的时候会用到这些信息，并最终找到mount函数进行挂载操作
调用proc_mkdir创建一些proc文件目录
在sys文件系统下注册proc文件系统的相关信息
在proc的mount函数中会调用proc_fill_super，它会给出proc文件系统超级块所需要的信息（比如文件系统的超级块操作函数指针，超级块大小等），并且会创建proc文件系统的根目录，在创建根目录时也会指定与之对应的inode_operations和file_operations，有了这些信息后，VFS就可以在该文件系统上进行各种操作了（创建、删除、查找文件）。&lt;/p&gt;

&lt;p&gt;文件操作的过程基本都是类似的，首先VFS会找到文件对应的inode，然后调用inode上的相关函数完成对应的操作，比如当读写一个文件时，要执行的步骤：
首先要打开文件，对于proc文件系统来说，它要做的是：找到文件的，在找到文件后文件数据结构的的file_operations会被初始化为文件的inode中的file_operations（参考路径do_filp_open-&amp;gt;path_openat-&amp;gt;do_last-&amp;gt;nameidata_to_filp-&amp;gt;__dentry_open-&amp;gt;fops_get）。之后就可以调用文件的打开操作了。
写文件，这一步也很简单， 文件数据结构中的写操作即可。
这个过程也很好理解， 因为对于任何文件系统，它们都存在inode，inode被用来表示一个文件，它包含了很多有用的信息，它用于维护文件自身而不涉及文件的内容。对于存储在存储器上的文件系统，其inode的有些信息是保存在存储器上的，但是仍有一部分是动态生成的；对于无存储器的文件系统，比如proc文件系统，其inode都是动态生成的。再使用一个文件时，其inode就会被加载到内存中以供使用，如果还不存在相关的inode，则就会创建一个新的。每个文件系统的超级块的super_operations包含了为本文件系统创建和删除inode节点的函数指针，在inode的操作函数集中包含了操作inode节点的函数指针，其中包括了查找inode节点的函数。打开一个文件时，如果inode缓冲中还没有该文件的inode，则经VFS处理后最终会调用lookup_real，它会调用inode操作函数集中的lookup函数用于查找所要打开的文件的inode；如果该文件的inode已经存在则会从缓存中得到该文件对应的inode，这部分工作由do_lookup完成。
简单的说，文件操作第一步时找到inode信息（如果没有就创建并初始化），然后用inode信息初始化文件的file结构。再用file结构对文件进行操作。
加载 proc 文件系统&lt;/p&gt;

&lt;p&gt;如果系统中还没有加载 proc 文件系统，可以通过如下命令加载 proc 文件系统： 
mount -t proc proc /proc&lt;/p&gt;

&lt;p&gt;上述命令将成功加载你的 proc 文件系统。更多细节请阅读 mount 命令的 man page。&lt;/p&gt;

&lt;p&gt;察看 /proc 的文件&lt;/p&gt;

&lt;p&gt;/proc 的文件可以用于访问有关内核的状态、计算机的属性、正在运行的进程的 状态等信息。大部分 /proc 中的文件和目录提供系统物理环境最新的信息。尽管 /proc 中的文件是虚拟的，但它们仍可以使用任何文件编辑器或像’more’, ‘less’或 ‘cat’这样的程序来查看。当编辑程序试图打开一个虚拟文件时，这个文件就通过内核 中的信息被凭空地 (on the fly) 创建了。这是一些我从我的系统中得到的一些有趣 结果：&lt;/p&gt;

&lt;p&gt;$ ls -l /proc/cpuinfo
-r–r–r– 1 root root 0 Dec 25 11:01 /proc/cpuinfo&lt;/p&gt;

&lt;p&gt;得到有用的系统/内核信息&lt;/p&gt;

&lt;p&gt;proc 文件系统可以被用于收集有用的关于系统和运行中的内核的信息。下面是一些重要 的文件：&lt;/p&gt;

&lt;p&gt;/proc/cpuinfo - CPU 的信息 (型号, 家族, 缓存大小等)
/proc/meminfo - 物理内存、交换空间等的信息
/proc/mounts - 已加载的文件系统的列表
/proc/devices - 可用设备的列表
/proc/filesystems - 被支持的文件系统
/proc/modules - 已加载的模块
/proc/version - 内核版本
/proc/cmdline - 系统启动时输入的内核命令行参数
proc 中的文件远不止上面列出的这么多。想要进一步了解的读者可以对 /proc 的每一个 文件都’more’一下或读参考文献[1]获取更多的有关 /proc 目录中的文件的信息。我建议 使用’more’而不是’cat’，除非你知道这个文件很小，因为有些文件 (比如 kcore) 可能 会非常长。&lt;/p&gt;

&lt;p&gt;有关运行中的进程的信息&lt;/p&gt;

&lt;p&gt;/proc 文件系统可以用于获取运行中的进程的信息。在 /proc 中有一些编号的子目录。每个编号的目录对应一个进程 id (PID)。这样，每一个运行中的进程 /proc 中都有一个用它的 PID 命名的目录。这些子目录中包含可以提供有关进程的状态和环境的重要细节信息的文件。让我们试着查找一个运行中的进程。&lt;/p&gt;

&lt;p&gt;$ ps -aef | grep mozilla
root 32558 32425 8  22:53 pts/1  00:01:23  /usr/bin/mozilla
上述命令显示有一个正在运行的 mozilla 进程的 PID 是 32558。相对应的，/proc 中应该有一个名叫 32558 的目录&lt;/p&gt;

&lt;p&gt;通过 /proc 与内核交互&lt;/p&gt;

&lt;p&gt;上面讨论的大部分 /proc 的文件是只读的。而实际上 /proc 文件系统通过 /proc 中可读写的文件提供了对内核的交互机制。写这些文件可以改变内核 的状态，因而要慎重改动这些文件。/proc/sys 目录存放所有可读写的文件 的目录，可以被用于改变内核行为。&lt;/p&gt;

&lt;p&gt;/proc/sys/kernel - 这个目录包含反通用内核行为的信息。 /proc/sys/kernel/{domainname, hostname} 存放着机器/网络的域名和主机名。 这些文件可以用于修改这些名字。&lt;/p&gt;

&lt;p&gt;$ hostname
machinename.domainname.com&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/09/proc.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/09/proc.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>netfliter</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Netfilter是Linux 2.4.x引入的一个子系统，它作为一个通用的、抽象的框架，提供一整套的hook函数的管理机制，使得诸如数据包过滤、网络地址转换(NAT)和基于协议类型的连接跟踪成为了可能。
netfilter的架构就是在整个网络流程的若干位置放置了一些检测点HOOK），而在每个检测点上登记了一些处理函数进行处理。&lt;/p&gt;

&lt;p&gt;netfilter是由Rusty Russell提出的Linux 2.4内核防火墙框架，该框架既简洁又灵活，可实现安全策略应用中的许多功能，如数据包过滤、数据包处理、地址伪装、透明代理、动态网络地址转换（Network Address Translation，NAT），以及基于用户及媒体访问控制（Media Access Control，MAC）地址的过滤和基于状态的过滤、包速率限制等。
框架
netfilter提供了一个抽象、通用化的框架[1]，作为中间件，为每种网络协议（IPv4、IPv6等）定义一套钩子函数。Ipv4定义了5个钩子函数，这些钩子函数在数据报流过协议栈的5个关键点被调用，也就是说，IPv4协议栈上定义了5个“允许垂钓点”。在每一个“垂钓点”，都可以让netfilter放置一个“鱼钩”，把经过的网络包（Packet）钓上来，与相应的规则链进行比较，并根据审查的结果，决定包的下一步命运，即是被原封不动地放回IPv4协议栈，继续向上层递交；还是经过一些修改，再放回网络；或者干脆丢弃掉。
Ipv4中的一个数据包通过netfilter系统的过程如图1所示。
图1 Netfilter的功能框架
关键技术
netfilter主要采用连线跟踪（Connection Tracking）、包过滤（Packet Filtering）、地址转换、包处理（Packet Mangling)4种关键技术。
⒈2.1 连线跟踪
连线跟踪是包过滤、地址转换的基础，它作为一个独立的模块运行。采用连线跟踪技术在协议栈低层截取数据包，将当前数据包及其状态信息与历史数据包及其状态信息进行比较，从而得到当前数据包的控制信息，根据这些信息决定对网络数据包的操作，达到保护网络的目的。
当下层网络接收到初始化连接同步（Synchronize，SYN）包，将被netfilter规则库检查。该数据包将在规则链中依次序进行比较。如果该包应被丢弃，发送一个复位（Reset，RST）包到远端主机，否则连接接收。这次连接的信息将被保存在连线跟踪信息表中，并表明该数据包所应有的状态。这个连线跟踪信息表位于内核模式下，其后的网络包就将与此连线跟踪信息表中的内容进行比较，根据信息表中的信息来决定该数据包的操作。因为数据包首先是与连线跟踪信息表进行比较，只有SYN包才与规则库进行比较，数据包与连线跟踪信息表的比较都是在内核模式下进行的，所以速度很快。
⒈2.2 包过滤
包过滤检查通过的每个数据包的头部，然后决定如何处置它们，可以选择丢弃，让包通过，或者更复杂的操作。
⒈2.3 地址转换
网络地址转换 分为源NAT（Source NAT，SNAT）和目的NAT(Destination NAT,DNAT)2种不同的类型。SNAT是指修改数据包的源地址（改变连接的源IP）。SNAT会在数据包送出之前的最后一刻做好转换工作。地址伪装（Masquerading）是SNAT的一种特殊形式。DNAT 是指修改数据包的目标地址（改变连接的目的IP）。DNAT 总是在数据包进入以后立即完成转换。端口转发、负载均衡和透明代理都属于DNAT。
⒈2.4 包处理
利用包处理可以设置或改变数据包的服务类型（Type of Service,TOS）字段；改变包的生存期（Time to Live,TTL）字段；在包中设置标志值，利用该标志值可以进行带宽限制和分类查询。&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/netfliter.png&quot; /&gt;
	&lt;/div&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Netfilter 是Linux内核中进行数据包过滤、连接跟踪、地址转换等的主要实现框架。当我们希望过滤特定的数据包或者需要修改数据包的内容再发送出去，这些动作主要都在netfilter中完成。
iptables工具就是用户空间和内核的Netfilter模块通信的手段，iptables命令提供很多选项来实现过滤数据包的各种操作，所以，我们在定义数据包过滤规则时，并不需要去直接修改内核中的netfilter模块，后面会讲到iptables命令如何作用于内核中的netfilter。
Netfilter的实质就是定义一系列的hook点（挂钩），每个hook点上可以挂载多个hook函数，hook函数中就实现了我们要对数据包的内容做怎样的修改、以及要将数据包放行还是过滤掉。数据包进入netfilter框架后，实际上就是依次经过所有hook函数的处理，数据包的命运就掌握在这些hook函数的手里。
本文基于内核版本2.6.31。
所有的hook点都放在一个全局的二维数组，每个hook点上的hook函数按照优先级顺序注册到一个链表中，注册的接口为nf_register_hook()。这个二维数组的定义如下：
struct list_head nf_hooks[NFPROTO_NUMPROTO][NF_MAX_HOOKS]__read_mostly;
其中NFPROTO_NUMPROTO 为netfilter支持的协议类型：
enum {
   NFPROTO_UNSPEC=  0,
   NFPROTO_IPV4   =  2,
   NFPROTO_ARP    =  3,
   NFPROTO_BRIDGE=  7,
   NFPROTO_IPV6   = 10,
   NFPROTO_DECNET= 12,
   NFPROTO_NUMPROTO,
};&lt;/p&gt;

</description>
        <pubDate>Sat, 09 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/jekyll/2017/12/09/netfliter.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/jekyll/2017/12/09/netfliter.html</guid>
        
        
        <category>jekyll</category>
        
      </item>
    
      <item>
        <title>namespace</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Linux Namespace&lt;/p&gt;

&lt;p&gt;Linux Namespace是Linux提供的一种OS-level virtualization的方法。目前在Linux系统上实现OS-level virtualization的系统有Linux VServer、OpenVZ、LXC Linux Container、Virtuozzo等，其中Virtuozzo是OpenVZ的商业版本。以上种种本质来说都是使用了Linux Namespace来进行隔离。&lt;/p&gt;

&lt;p&gt;那么究竟什么是Linux Namespace？Linux很早就实现了一个系统调用chroot，该系统调用能够为进程提供一个限制的文件系统。虽然文件系统的隔离要比单纯的chroot复杂的多，但是至少chroot提供了一种简单的隔离模式：chroot内部的文件系统无法访问外部的内容。Linux Namespace在此基础上，提供了对UTS、IPC、mount、PID、network的隔离机制，例如对不同的PID namespace中的进程无法看到彼此，而且每个PID namespace中的进程PID都是单独制定的。这一点对OS-level Virtualization非常有用，这是因为：对于不同的Linux运行环境中，都有一个init进程，其PID=0，由于不同的PID namespace中都可以指定自己的0号进程，所以可以通过该技术来进行PID环境的隔离。&lt;/p&gt;

&lt;p&gt;OS-level Virtualization相比其他的虚拟化技术更加轻量级。&lt;/p&gt;

&lt;p&gt;Linux在使用Namespace的时候，需要显式的在配置中指定将那些Namespace的支持编译到内核中。&lt;/p&gt;

&lt;p&gt;进程的若干个ID的意义&lt;/p&gt;

&lt;p&gt;PID：Process ID，进程ID，即进程的唯一标识
TGID：处于某个线程组中的所有进程都有统一的线程组ID（Thread Group IP，TGID）。线程可以用clone加CLONE_THREAD来创建。线程组中的主进程成为group leader，可以通过线程组中任何线程的的task_struct-&amp;gt;group_leader成员获得。
独立进程可以合并成进程组（使用setpgrp系统调用）。进程组成员的task_struct-&amp;gt;pgrp属性值都是相同的（PGID），即进程组组长的PID。用管道连接的进程在一个进程组中。
几个进程组可以合并成一个会话。会话中所有进程都有同样的SID（Session ID，会话ID），保存在task_struct-&amp;gt;session中。SID可以通过setsid系统调用设置。&lt;/p&gt;

&lt;p&gt;引入进程PID命名空间后的PID框架
随着内核不断的添加新的内核特性,尤其是PID Namespace机制的引入,这导致PID存在命名空间的概念,并且命名空间还有层级的概念存在,高级别的可以被低级别的看到,这就导致高级别的进程有多个PID,比如说在默认命名空间下,创建了一个新的命名空间,占且叫做level1,默认命名空间这里称之为level0,在level1中运行了一个进程在level1中这个进程的pid为1,因为高级别的pid namespace需要被低级别的pid namespace所看见,所以这个进程在level0中会有另外一个pid,为xxx.套用上面说到的pid位图的概念,可想而知,对于每一个pid namespace来说都应该有一个pidmap,上文中提到的level1进程有两个pid一个是1,另一个是xxx,其中pid为1是在level1中的pidmap进行分配的,pid为xxx则是在level0的pidmap中分配的. 下面这幅图是整个pidnamespace的一个框架&lt;/p&gt;

&lt;p&gt;Linux Namespaces机制提供一种资源隔离方案。PID,IPC,Network等系统资源不再是全局性的，而是属于特定的Namespace。每个Namespace里面的资源对其他Namespace都是透明的。要创建新的Namespace，只需要在调用clone时指定相应的flag。Linux Namespaces机制为实现基于容器的虚拟化技术提供了很好的基础，LXC（Linux containers）就是利用这一特性实现了资源的隔离。不同container内的进程属于不同的Namespace，彼此透明，互不干扰。下面我们就从clone系统调用的flag出发，来介绍各个Namespace。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWPID，就会创建一个新的PID Namespace，clone出来的新进程将成为Namespace里的第一个进程。一个PID Namespace为进程提供了一个独立的PID环境，PID Namespace内的PID将从1开始，在Namespace内调用fork，vfork或clone都将产生一个在该Namespace内独立的PID。新创建的Namespace里的第一个进程在该Namespace内的PID将为1，就像一个独立的系统里的init进程一样。该Namespace内的孤儿进程都将以该进程为父进程，当该进程被结束时，该Namespace内所有的进程都会被结束。PID Namespace是层次性，新创建的Namespace将会是创建该Namespace的进程属于的Namespace的子Namespace。子Namespace中的进程对于父Namespace是可见的，一个进程将拥有不止一个PID，而是在所在的Namespace以及所有直系祖先Namespace中都将有一个PID。系统启动时，内核将创建一个默认的PID Namespace，该Namespace是所有以后创建的Namespace的祖先，因此系统所有的进程在该Namespace都是可见的。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWIPC，就会创建一个新的IPC Namespace，clone出来的进程将成为Namespace里的第一个进程。一个IPC Namespace有一组System V IPC objects 标识符构成，这标识符有IPC相关的系统调用创建。在一个IPC Namespace里面创建的IPC object对该Namespace内的所有进程可见，但是对其他Namespace不可见，这样就使得不同Namespace之间的进程不能直接通信，就像是在不同的系统里一样。当一个IPC Namespace被销毁，该Namespace内的所有IPC object会被内核自动销毁。&lt;/p&gt;

&lt;p&gt;PID Namespace和IPC Namespace可以组合起来一起使用，只需在调用clone时，同时指定CLONE_NEWPID和CLONE_NEWIPC，这样新创建的Namespace既是一个独立的PID空间又是一个独立的IPC空间。不同Namespace的进程彼此不可见，也不能互相通信，这样就实现了进程间的隔离。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWNS，就会创建一个新的mount Namespace。每个进程都存在于一个mount Namespace里面，mount Namespace为进程提供了一个文件层次视图。如果不设定这个flag，子进程和父进程将共享一个mount Namespace，其后子进程调用mount或umount将会影响到所有该Namespace内的进程。如果子进程在一个独立的mount Namespace里面，就可以调用mount或umount建立一份新的文件层次视图。该flag配合pivot_root系统调用，可以为进程创建一个独立的目录空间。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWNET，就会创建一个新的Network Namespace。一个Network Namespace为进程提供了一个完全独立的网络协议栈的视图。包括网络设备接口，IPv4和IPv6协议栈，IP路由表，防火墙规则，sockets等等。一个Network Namespace提供了一份独立的网络环境，就跟一个独立的系统一样。一个物理设备只能存在于一个Network Namespace中，可以从一个Namespace移动另一个Namespace中。虚拟网络设备(virtual network device)提供了一种类似管道的抽象，可以在不同的Namespace之间建立隧道。利用虚拟化网络设备，可以建立到其他Namespace中的物理设备的桥接。当一个Network Namespace被销毁时，物理设备会被自动移回init Network Namespace，即系统最开始的Namespace。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWUTS，就会创建一个新的UTS Namespace。一个UTS Namespace就是一组被uname返回的标识符。新的UTS Namespace中的标识符通过复制调用进程所属的Namespace的标识符来初始化。Clone出来的进程可以通过相关系统调用改变这些标识符，比如调用sethostname来改变该Namespace的hostname。这一改变对该Namespace内的所有进程可见。CLONE_NEWUTS和CLONE_NEWNET一起使用，可以虚拟出一个有独立主机名和网络空间的环境，就跟网络上一台独立的主机一样。&lt;/p&gt;

&lt;p&gt;以上所有clone flag都可以一起使用，为进程提供了一个独立的运行环境。LXC正是通过在clone时设定这些flag，为进程创建一个有独立PID，IPC，FS，Network，UTS空间的container。一个container就是一个虚拟的运行环境，对container里的进程是透明的，它会以为自己是直接在一个系统上运行的。&lt;/p&gt;

&lt;p&gt;一个container就像传统虚拟化技术里面的一台安装了OS的虚拟机，但是开销更小，部署更为便捷。&lt;/p&gt;
&lt;div class=&quot;container&quot;&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterSlider.png&quot; /&gt;
	&lt;/div&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;/div&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 08 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/08/namespace.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/08/namespace.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>vfs</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;在Linux中，文件系统主要分为下面3种：
（1）基于磁盘的文件系统（Disk-based Filesystem) 是在非易失介质上存储文件的经典方法，用以在多次会话之间保持文件的内容。如Ext2/3/4， Reiserfs, FAT等。
（2）虚拟文件系统（Virtual Filesystem） 在内核中生成，是一种用户应用程序与内核通信的方法。如proc，它不许要在任何类的硬件设备上分配存储空间，所有的信息都是动态在内存中开辟和存储。
（3）网络文件系统（Network Filesystem） 是基于磁盘的文件系统和虚拟文件系统之间的折中。这种文件系统允许访问另一台计算机上的数据，该计算机通过网络连接到本地计算机。在这种情况下，数据实际上存储在一个不同系统的硬件设备上。
由于VFS抽象层的存在，用户空间进程不会看到本地文件系统与网络文件系统之间的区别。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;VFS的模型与结构
VFS不仅为文件系统提供了方法和抽象，还支持文件系统中对象（或文件）的统一视图。并非每一种文件系统都支持VFS中的所有抽象，如FAT，因为其设计没有考虑到此类对象。定义一个最小的通用模型，来支持内核中所有文件系统都实现的那些功能，这是不实际的。因为这样会损失许多本质性的功能特性，或者导致这些特性只能通过特定文件系统的路径访问。
VFS的方案完全相反：提供一种结构模型，包含了一个强大文件系统所具备的所有组件。但该模型只存在于虚拟中，必须使用各种对象和函数指针与每种文件系统适配。所有文件系统的实现都必须提供与VFS定义的结构配合的例程，以弥合两种视图之间的差异。
VFS是由基于经典文件系统的结构衍化而来，所以VFS与Ext类文件系统类似，从而在处理Ext类文件系统的时候，Ext和VFS之间的转换，几乎不会损失时间。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一些基本概念:
文件 一组在逻辑上具有完整意义的信息项的系列。在Linux中，除了普通文件，其他诸如目录、设备、套接字等 也以文件被对待。总之，“一切皆文件”。
目录 目录好比一个文件夹，用来容纳相关文件。因为目录可以包含子目录，所以目录是可以层层嵌套，形成 文件路径。在Linux中，目录也是以一种特殊文件被对待的，所以用于文件的操作同样也可以用在目录上。
目录项 在一个文件路径中，路径中的每一部分都被称为目录项；如路径/home/source/helloworld.c中，目录 /, home, source和文件 helloworld.c都是一个目录项。
索引节点 用于存储文件的元数据的一个数据结构。文件的元数据，也就是文件的相关信息，和文件本身是两个不同 的概念。它包含的是诸如文件的大小、拥有者、创建时间、磁盘位置等和文件相关的信息。
超级块 用于存储文件系统的控制信息的数据结构。描述文件系统的状态、文件系统类型、大小、区块数、索引节 点数等，存放于磁盘的特定扇区中&lt;/p&gt;

</description>
        <pubDate>Thu, 07 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/07/vfs.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/07/vfs.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>mysql_index</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;如果想知道MySQL数据库中每个表占用的空间、表记录的行数的话，可以打开MySQL的 information_schema 数据库。在该库中有一个 TABLES 表，这个表主要字段分别是：&lt;/p&gt;

&lt;p&gt;TABLE_SCHEMA : 数据库名
TABLE_NAME：表名
ENGINE：所使用的存储引擎
TABLES_ROWS：记录数
DATA_LENGTH：数据大小
INDEX_LENGTH：索引大小&lt;/p&gt;

&lt;p&gt;其他字段请参考MySQL的手册，我们只需要了解这几个就足够了。&lt;/p&gt;

&lt;p&gt;1  首先查看某一实例下的所有占用磁盘空间（表数据+索引数据，得到的结果为B，这里做了数据处理转成M）：&lt;/p&gt;

&lt;p&gt;select concat(round((sum(DATA_LENGTH)+sum(INDEX_LENGTH))/1024/1024,2),’M’) from information_schema.tables where table_schema=’实例名称’;
 上面是查询所有的表计的累计量，下面是是查询单个表计的的SQL(按照实例名查询)：&lt;/p&gt;

&lt;p&gt;select table_name,
DATA_LENGTH/1024/1024 as tablesData,
INDEX_LENGTH/1024/1024 as indexData 
from information_schema.tables
where table_schema=’dsm’
ORDER BY  tablesData desc;&lt;/p&gt;

</description>
        <pubDate>Thu, 07 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/12/07/mysql_index.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/12/07/mysql_index.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>linux_rcu</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;1:RCU使用在读者多而写者少的情况.RCU和读写锁相似.但RCU的读者占锁没有任何的系统开销.写者与写写者之间必须要保持同步,且写者必须要等它之前的读者全部都退出之后才能释放之前的资源.
2:RCU保护的是指针.这一点尤其重要.因为指针赋值是一条单指令.也就是说是一个原子操作.因它更改指针指向没必要考虑它的同步.只需要考虑cache的影响. 
3:读者是可以嵌套的.也就是说rcu_read_lock()可以嵌套调用. 
4:读者在持有rcu_read_lock()的时候,不能发生进程上下文切换.否则,因为写者需要要等待读者完成,写者进程也会一直被阻塞.&lt;/p&gt;

&lt;p&gt;RCU（Read-Copy Update）是数据同步的一种方式，在当前的Linux内核中发挥着重要的作用。RCU主要针对的数据对象是链表，目的是提高遍历读取数据的效率，为了达到目的使用RCU机制读取数据的时候不对链表进行耗时的加锁操作。这样在同一时间可以有多个线程同时读取该链表，并且允许一个线程对链表进行修改（修改的时候，需要加锁）。RCU适用于需要频繁的读取数据，而相应修改数据并不多的情景，例如在文件系统中，经常需要查找定位目录，而对目录的修改相对来说并不多，这就是RCU发挥作用的最佳场景。&lt;/p&gt;

&lt;p&gt;在RCU的实现过程中，我们主要解决以下问题：
1，在读取过程中，另外一个线程删除了一个节点。删除线程可以把这个节点从链表中移除，但它不能直接销毁这个节点，必须等到所有的读取线程读取完成以后，才进行销毁操作。RCU中把这个过程称为宽限期（Grace period）。
2，在读取过程中，另外一个线程插入了一个新节点，而读线程读到了这个节点，那么需要保证读到的这个节点是完整的。这里涉及到了发布-订阅机制（Publish-Subscribe Mechanism）。
3， 保证读取链表的完整性。新增或者删除一个节点，不至于导致遍历一个链表从中间断开。但是RCU并不保证一定能读到新增的节点或者不读到要被删除的节点。&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_rcu.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_rcu.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Linux的mmap内存映射机制</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;一个进程应该包括一个mm_struct(memory manage struct), 该结构是进程虚拟地址空间的抽象描述,里面包括了进程虚拟空间的一些管理信息: start_code, end_code, start_data, end_data, start_brk, end_brk等等信息.另外,也有一个指向进程虚存区表(vm_area_struct: virtual memory area)的指针,该链是按照虚拟地址的增长顺序排列的.在Linux进程的地址空间被分作许多区(vma),每个区(vma)都对应虚拟地址空间上一段连续的区域, vma是可以被共享和保护的独立实体,这里的vma就是前面提到的内存对象.&lt;/p&gt;

&lt;p&gt;设备驱动的mmap实现主要是将一个物理设备的可操作区域（设备空间）映射到一个进程的虚拟地址空间。这样就可以直接采用指针的方式像访问内存的方式访问设备。在驱动中的mmap实现主要是完成一件事，就是实际物理设备的操作区域到进程虚拟空间地址的映射过程。同时也需要保证这段映射的虚拟存储器区域不会被进程当做一般的空间使用，因此需要添加一系列的保护方式。&lt;/p&gt;

&lt;p&gt;Linux提供了内存映射函数mmap,它把文件内容映射到一段内存上(准确说是虚拟内存上),通过对这段内存的读取和修改,实现对文件的读取和修改 。普通文件被映射到进程地址空间后，进程可以向访问普通内存一样对文件进行访问，不必再调用read()，write（）等操作。
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/linuxMMap.jpeg&quot; /&gt;
mmap系统调用的实现过程是
1.先通过文件系统定位要映射的文件；
2.权限检查,映射的权限不会超过文件打开的方式,也就是说如果文件是以只读方式打开,那么则不允许建立一个可写映射； 
3.创建一个vma对象,并对之进行初始化； 
4.调用映射文件的mmap函数,其主要工作是给vm_ops向量表赋值；
5.把该vma链入该进程的vma链表中,如果可以和前后的vma合并则合并；
6.如果是要求VM_LOCKED(映射区不被换出)方式映射,则发出缺页请求,把映射页面读入内存中.&lt;/p&gt;

&lt;p&gt;2、munmap函数
munmap(void * start, size_t length):
该调用可以看作是mmap的一个逆过程.它将进程中从start开始length长度的一段区域的映射关闭,如果该区域不是恰好对应一个vma,则有可能会分割几个或几个vma.
 msync(void * start, size_t length, int flags):
把映射区域的修改回写到后备存储中.因为munmap时并不保证页面回写,如果不调用msync,那么有可能在munmap后丢失对映射区的修改.其中flags可以是MS_SYNC, MS_ASYNC, MS_INVALIDATE, MS_SYNC要求回写完成后才返回, MS_ASYNC发出回写请求后立即返回, MS_INVALIDATE使用回写的内容更新该文件的其它映射.该系统调用是通过调用映射文件的sync函数来完成工作的.
brk(void * end_data_segement):
将进程的数据段扩展到end_data_segement指定的地址,该系统调用和mmap的实现方式十分相似,同样是产生一个vma,然后指定其属性.不过在此之前需要做一些合法性检查,比如该地址是否大于mm-&amp;gt;end_code, end_data_segement和mm-&amp;gt;brk之间是否还存在其它vma等等.通过brk产生的vma映射的文件为空,这和匿名映射产生的vma相似,关于匿名映射不做进一步介绍.库函数malloc就是通过brk实现的.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_mmap.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_mmap.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>linux_lock</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;在现代操作系统里，同一时间可能有多个内核执行流在执行，因此内核其实象多进程多线程编程一样也需要一些同步机制来同步各执行单元对共享数据的访问。尤其是在多处理器系统上，更需要一些同步机制来同步不同处理器上的执行单元对共享的数据的访问。在主流的Linux内核中包含了几乎所有现代的操作系统具有的同步机制，这些同步机制包括：原子操作、信号量（semaphore）、读写信号量（rw_semaphore）、spinlock、 BKL(Big Kernel Lock)、rwlock、brlock（只包含在2.4内核中）、RCU（只包含在2.6内核中）和seqlock（只包含在2.6内核中）。&lt;/p&gt;

&lt;p&gt;原子操作通常用于实现资源的引用计数，在TCP/IP协议栈的IP碎片处理中，就使用了引用计数，碎片队列结构struct ipq描述了一个IP碎片，字段refcnt就是引用计数器，它的类型为atomic_t，当创建IP碎片时（在函数ip_frag_create中），使用atomic_set函数把它设置为1，当引用该IP碎片时，就使用函数atomic_inc把引用计数加1，当不需要引用该IP碎片时，就使用函数 ipq_put来释放该IP碎片，ipq_put使用函数atomic_dec_and_test把引用计数减1并判断引用计数是否为0，如果是就释放 IP碎片。函数ipq_kill把IP碎片从ipq队列中删除，并把该删除的IP碎片的引用计数减1（通过使用函数atomic_dec实现）。&lt;/p&gt;

&lt;p&gt;Linux内核的信号量在概念和原理上与用户态的System V的IPC机制信号量是一样的，但是它绝不可能在内核之外使用，因此它与System V的IPC机制信号量毫不相干。&lt;/p&gt;

&lt;p&gt;信号量在创建时需要设置一个初始值，表示同时可以有几个任务可以访问该信号量保护的共享资源，初始值为1就变成互斥锁（Mutex），即同时只能有一个任务可以访问信号量保护的共享资源。一个任务要想访问共享资源，首先必须得到信号量，获取信号量的操作将把信号量的值减1，若当前信号量的值为负数，表明无法获得信号量，该任务必须挂起在该信号量的等待队列等待该信号量可用；若当前信号量的值为非负数，表示可以获得信号量，因而可以立刻访问被该信号量保护的共享资源。当任务访问完被信号量保护的共享资源后，必须释放信号量，释放信号量通过把信号量的值加1实现，如果信号量的值为非正数，表明有任务等待当前信号量，因此它也唤醒所有等待该信号量的任务。&lt;/p&gt;

&lt;p&gt;读写信号量对访问者进行了细分，或者为读者，或者为写者，读者在保持读写信号量期间只能对该读写信号量保护的共享资源进行读访问，如果一个任务除了需要读，可能还需要写，那么它必须被归类为写者，它在对共享资源访问之前必须先获得写者身份，写者在发现自己不需要写访问的情况下可以降级为读者。读写信号量同时拥有的读者数不受限制，也就说可以有任意多个读者同时拥有一个读写信号量。如果一个读写信号量当前没有被写者拥有并且也没有写者等待读者释放信号量，那么任何读者都可以成功获得该读写信号量；否则，读者必须被挂起直到写者释放该信号量。如果一个读写信号量当前没有被读者或写者拥有并且也没有写者等待该信号量，那么一个写者可以成功获得该读写信号量，否则写者将被挂起，直到没有任何访问者。因此，写者是排他性的，独占性的。&lt;/p&gt;

&lt;p&gt;读写信号量有两种实现，一种是通用的，不依赖于硬件架构，因此，增加新的架构不需要重新实现它，但缺点是性能低，获得和释放读写信号量的开销大；另一种是架构相关的，因此性能高，获取和释放读写信号量的开销小，但增加新的架构需要重新实现。在内核配置时，可以通过选项去控制使用哪一种实现。&lt;/p&gt;

&lt;p&gt;自旋锁与互斥锁有点类似，只是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。由于自旋锁使用者一般保持锁时间非常短，因此选择自旋而不是睡眠是非常必要的，自旋锁的效率远高于互斥锁。&lt;/p&gt;

&lt;p&gt;信号量和读写信号量适合于保持时间较长的情况，它们会导致调用者睡眠，因此只能在进程上下文使用（_trylock的变种能够在中断上下文使用），而自旋锁适合于保持时间非常短的情况，它可以在任何上下文使用。如果被保护的共享资源只在进程上下文访问，使用信号量保护该共享资源非常合适，如果对共巷资源的访问时间非常短，自旋锁也可以。但是如果被保护的共享资源需要在中断上下文访问（包括底半部即中断处理句柄和顶半部即软中断），就必须使用自旋锁。&lt;/p&gt;

&lt;p&gt;自旋锁保持期间是抢占失效的，而信号量和读写信号量保持期间是可以被抢占的。自旋锁只有在内核可抢占或SMP的情况下才真正需要，在单CPU且不可抢占的内核下，自旋锁的所有操作都是空操作。&lt;/p&gt;

&lt;p&gt;跟互斥锁一样，一个执行单元要想访问被自旋锁保护的共享资源，必须先得到锁，在访问完共享资源后，必须释放锁。如果在获取自旋锁时，没有任何执行单元保持该锁，那么将立即得到锁；如果在获取自旋锁时锁已经有保持者，那么获取锁操作将自旋在那里，直到该自旋锁的保持者释放了锁。&lt;/p&gt;

&lt;p&gt;无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_lock.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_lock.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>linux_elf</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;可执行连接格式是UNIX系统实验室(USL)作为应用程序二进制接口
(Application Binary Interface(ABI)而开发和发布的。工具接口标准委
员会(TIS)选择了正在发展中的ELF标准作为工作在32位INTEL体系上不同操
作系统之间可移植的二进制文件格式。
假定开发者定义了一个二进制接口集合，ELF标准用它来支持流线型的软件
发展。 应该减少不同执行接口的数量。因此可以减少重新编程重新编译的
代码。&lt;/p&gt;

&lt;p&gt;在object文件中有三种主要的类型。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;一个可重定位(relocatable)文件保存着代码和适当的数据，用来和其他的
object文件一起来创建一个可执行文件或者是一个共享文件。&lt;/li&gt;
  &lt;li&gt;一个可执行(executable)文件保存着一个用来执行的程序；该文件指出了
exec(BA_OS)如何来创建程序进程映象。&lt;/li&gt;
  &lt;li&gt;一个共享object文件保存着代码和合适的数据，用来被下面的两个链接器
链接。第一个是连接编辑器[请参看ld(SD_CMD)]，可以和其他的可重定位和
共享object文件来创建其他的object。第二个是动态链接器，联合一个
可执行文件和其他的共享object文件来创建一个进程映象。
一个object文件被汇编器和联接器创建, 想要在处理机上直接运行的object
文件都是以二进制来存放的。那些需要抽象机制的程序，比如象shell脚本，
是不被接受的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个ELF头在文件的开始，保存了路线图(road map)，描述了该文件的组织情况。
sections保存着object 文件的信息，从连接角度看：包括指令，数据，
符号表，重定位信息等等。&lt;/p&gt;

&lt;p&gt;假如一个程序头表（program header table）存在，那么它告诉系统如何来创建一
个进程的内存映象。被用来建立进程映象(执行一个程序)的文件必须要有一个程
序头表（program header table）；可重定位文件不需要这个头表。一个
section头表（section header table）包含了描述文件sections的信息。每个
section在这个表中有一个入口；每个入口给出了该section的名字，大小，
等等信息。在联接过程中的文件必须有一个section头表；其他object文件可要
可不要这个section头表。&lt;/p&gt;

&lt;p&gt;对象文件(Object files)有三个种类：&lt;/p&gt;

&lt;p&gt;1) 可重定位的对象文件(Relocatable file)&lt;/p&gt;

&lt;p&gt;这是由汇编器汇编生成的 .o 文件。后面的链接器(link editor)拿一个或一些 Relocatable object files 作为输入，经链接处理后，生成一个可执行的对象文件 (Executable file) 或者一个可被共享的对象文件(Shared object file)。我们可以使用 ar 工具将众多的 .o Relocatable object files 归档(archive)成 .a 静态库文件。如何产生 Relocatable file，你应该很熟悉了，请参见我们相关的基本概念文章和JulWiki。另外，可以预先告诉大家的是我们的内核可加载模块 .ko 文件也是 Relocatable object file。&lt;/p&gt;

&lt;p&gt;2) 可执行的对象文件(Executable file)&lt;/p&gt;

&lt;p&gt;这我们见的多了。文本编辑器vi、调式用的工具gdb、播放mp3歌曲的软件mplayer等等都是Executable object file。你应该已经知道，在我们的 Linux 系统里面，存在两种可执行的东西。除了这里说的 Executable object file，另外一种就是可执行的脚本(如shell脚本)。注意这些脚本不是 Executable object file，它们只是文本文件，但是执行这些脚本所用的解释器就是 Executable object file，比如 bash shell 程序。&lt;/p&gt;

&lt;p&gt;3) 可被共享的对象文件(Shared object file)&lt;/p&gt;

&lt;p&gt;这些就是所谓的动态库文件，也即 .so 文件。如果拿前面的静态库来生成可执行程序，那每个生成的可执行程序中都会有一份库代码的拷贝。如果在磁盘中存储这些可执行程序，那就会占用额外的磁盘空间；另外如果拿它们放到Linux系统上一起运行，也会浪费掉宝贵的物理内存。如果将静态库换成动态库，那么这些问题都不会出现。动态库在发挥作用的过程中，必须经过两个步骤：&lt;/p&gt;

&lt;p&gt;a) 链接编辑器(link editor)拿它和其他Relocatable object file以及其他shared object file作为输入，经链接处理后，生存另外的 shared object file 或者 executable file。&lt;/p&gt;

&lt;p&gt;b) 在运行时，动态链接器(dynamic linker)拿它和一个Executable file以及另外一些 Shared object file 来一起处理，在Linux系统里面创建一个进程映像。&lt;/p&gt;

&lt;p&gt;以上所提到的 link editor 以及 dynamic linker 是什么东西，你可以参考我们基本概念中的相关文章。对于什么是编译器，汇编器等你应该也已经知道，在这里只是使用他们而不再对他们进行详细介绍。为了下面的叙述方便，你可以下载test.tar.gz包，解压缩后使用”make”进行编译。编译完成后，会在目录中生成一系列的ELF对象文件，更多描述见里面的 README 文件。我们下面的论述都基于这些产生的对象文件。&lt;/p&gt;

&lt;p&gt;ELF格式需要使用在两种场合：&lt;/p&gt;

&lt;p&gt;a) 组成不同的可重定位文件，以参与可执行文件或者可被共享的对象文件的链接构建；&lt;/p&gt;

&lt;p&gt;b) 组成可执行文件或者可被共享的对象文件，以在运行时内存中进程映像的构建。&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_elf.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_elf.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
  </channel>
</rss>
