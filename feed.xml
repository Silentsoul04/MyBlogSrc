<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 08 Dec 2017 00:14:20 +0800</pubDate>
    <lastBuildDate>Fri, 08 Dec 2017 00:14:20 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>namespace</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Linux Namespace&lt;/p&gt;

&lt;p&gt;Linux Namespace是Linux提供的一种OS-level virtualization的方法。目前在Linux系统上实现OS-level virtualization的系统有Linux VServer、OpenVZ、LXC Linux Container、Virtuozzo等，其中Virtuozzo是OpenVZ的商业版本。以上种种本质来说都是使用了Linux Namespace来进行隔离。&lt;/p&gt;

&lt;p&gt;那么究竟什么是Linux Namespace？Linux很早就实现了一个系统调用chroot，该系统调用能够为进程提供一个限制的文件系统。虽然文件系统的隔离要比单纯的chroot复杂的多，但是至少chroot提供了一种简单的隔离模式：chroot内部的文件系统无法访问外部的内容。Linux Namespace在此基础上，提供了对UTS、IPC、mount、PID、network的隔离机制，例如对不同的PID namespace中的进程无法看到彼此，而且每个PID namespace中的进程PID都是单独制定的。这一点对OS-level Virtualization非常有用，这是因为：对于不同的Linux运行环境中，都有一个init进程，其PID=0，由于不同的PID namespace中都可以指定自己的0号进程，所以可以通过该技术来进行PID环境的隔离。&lt;/p&gt;

&lt;p&gt;OS-level Virtualization相比其他的虚拟化技术更加轻量级。&lt;/p&gt;

&lt;p&gt;Linux在使用Namespace的时候，需要显式的在配置中指定将那些Namespace的支持编译到内核中。&lt;/p&gt;

&lt;p&gt;进程的若干个ID的意义&lt;/p&gt;

&lt;p&gt;PID：Process ID，进程ID，即进程的唯一标识
TGID：处于某个线程组中的所有进程都有统一的线程组ID（Thread Group IP，TGID）。线程可以用clone加CLONE_THREAD来创建。线程组中的主进程成为group leader，可以通过线程组中任何线程的的task_struct-&amp;gt;group_leader成员获得。
独立进程可以合并成进程组（使用setpgrp系统调用）。进程组成员的task_struct-&amp;gt;pgrp属性值都是相同的（PGID），即进程组组长的PID。用管道连接的进程在一个进程组中。
几个进程组可以合并成一个会话。会话中所有进程都有同样的SID（Session ID，会话ID），保存在task_struct-&amp;gt;session中。SID可以通过setsid系统调用设置。&lt;/p&gt;

&lt;p&gt;引入进程PID命名空间后的PID框架
随着内核不断的添加新的内核特性,尤其是PID Namespace机制的引入,这导致PID存在命名空间的概念,并且命名空间还有层级的概念存在,高级别的可以被低级别的看到,这就导致高级别的进程有多个PID,比如说在默认命名空间下,创建了一个新的命名空间,占且叫做level1,默认命名空间这里称之为level0,在level1中运行了一个进程在level1中这个进程的pid为1,因为高级别的pid namespace需要被低级别的pid namespace所看见,所以这个进程在level0中会有另外一个pid,为xxx.套用上面说到的pid位图的概念,可想而知,对于每一个pid namespace来说都应该有一个pidmap,上文中提到的level1进程有两个pid一个是1,另一个是xxx,其中pid为1是在level1中的pidmap进行分配的,pid为xxx则是在level0的pidmap中分配的. 下面这幅图是整个pidnamespace的一个框架&lt;/p&gt;

&lt;p&gt;Linux Namespaces机制提供一种资源隔离方案。PID,IPC,Network等系统资源不再是全局性的，而是属于特定的Namespace。每个Namespace里面的资源对其他Namespace都是透明的。要创建新的Namespace，只需要在调用clone时指定相应的flag。Linux Namespaces机制为实现基于容器的虚拟化技术提供了很好的基础，LXC（Linux containers）就是利用这一特性实现了资源的隔离。不同container内的进程属于不同的Namespace，彼此透明，互不干扰。下面我们就从clone系统调用的flag出发，来介绍各个Namespace。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWPID，就会创建一个新的PID Namespace，clone出来的新进程将成为Namespace里的第一个进程。一个PID Namespace为进程提供了一个独立的PID环境，PID Namespace内的PID将从1开始，在Namespace内调用fork，vfork或clone都将产生一个在该Namespace内独立的PID。新创建的Namespace里的第一个进程在该Namespace内的PID将为1，就像一个独立的系统里的init进程一样。该Namespace内的孤儿进程都将以该进程为父进程，当该进程被结束时，该Namespace内所有的进程都会被结束。PID Namespace是层次性，新创建的Namespace将会是创建该Namespace的进程属于的Namespace的子Namespace。子Namespace中的进程对于父Namespace是可见的，一个进程将拥有不止一个PID，而是在所在的Namespace以及所有直系祖先Namespace中都将有一个PID。系统启动时，内核将创建一个默认的PID Namespace，该Namespace是所有以后创建的Namespace的祖先，因此系统所有的进程在该Namespace都是可见的。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWIPC，就会创建一个新的IPC Namespace，clone出来的进程将成为Namespace里的第一个进程。一个IPC Namespace有一组System V IPC objects 标识符构成，这标识符有IPC相关的系统调用创建。在一个IPC Namespace里面创建的IPC object对该Namespace内的所有进程可见，但是对其他Namespace不可见，这样就使得不同Namespace之间的进程不能直接通信，就像是在不同的系统里一样。当一个IPC Namespace被销毁，该Namespace内的所有IPC object会被内核自动销毁。&lt;/p&gt;

&lt;p&gt;PID Namespace和IPC Namespace可以组合起来一起使用，只需在调用clone时，同时指定CLONE_NEWPID和CLONE_NEWIPC，这样新创建的Namespace既是一个独立的PID空间又是一个独立的IPC空间。不同Namespace的进程彼此不可见，也不能互相通信，这样就实现了进程间的隔离。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWNS，就会创建一个新的mount Namespace。每个进程都存在于一个mount Namespace里面，mount Namespace为进程提供了一个文件层次视图。如果不设定这个flag，子进程和父进程将共享一个mount Namespace，其后子进程调用mount或umount将会影响到所有该Namespace内的进程。如果子进程在一个独立的mount Namespace里面，就可以调用mount或umount建立一份新的文件层次视图。该flag配合pivot_root系统调用，可以为进程创建一个独立的目录空间。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWNET，就会创建一个新的Network Namespace。一个Network Namespace为进程提供了一个完全独立的网络协议栈的视图。包括网络设备接口，IPv4和IPv6协议栈，IP路由表，防火墙规则，sockets等等。一个Network Namespace提供了一份独立的网络环境，就跟一个独立的系统一样。一个物理设备只能存在于一个Network Namespace中，可以从一个Namespace移动另一个Namespace中。虚拟网络设备(virtual network device)提供了一种类似管道的抽象，可以在不同的Namespace之间建立隧道。利用虚拟化网络设备，可以建立到其他Namespace中的物理设备的桥接。当一个Network Namespace被销毁时，物理设备会被自动移回init Network Namespace，即系统最开始的Namespace。&lt;/p&gt;

&lt;p&gt;当调用clone时，设定了CLONE_NEWUTS，就会创建一个新的UTS Namespace。一个UTS Namespace就是一组被uname返回的标识符。新的UTS Namespace中的标识符通过复制调用进程所属的Namespace的标识符来初始化。Clone出来的进程可以通过相关系统调用改变这些标识符，比如调用sethostname来改变该Namespace的hostname。这一改变对该Namespace内的所有进程可见。CLONE_NEWUTS和CLONE_NEWNET一起使用，可以虚拟出一个有独立主机名和网络空间的环境，就跟网络上一台独立的主机一样。&lt;/p&gt;

&lt;p&gt;以上所有clone flag都可以一起使用，为进程提供了一个独立的运行环境。LXC正是通过在clone时设定这些flag，为进程创建一个有独立PID，IPC，FS，Network，UTS空间的container。一个container就是一个虚拟的运行环境，对container里的进程是透明的，它会以为自己是直接在一个系统上运行的。&lt;/p&gt;

&lt;p&gt;一个container就像传统虚拟化技术里面的一台安装了OS的虚拟机，但是开销更小，部署更为便捷。&lt;/p&gt;
&lt;div class=&quot;container&quot;&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/jupyterSlider.png&quot; /&gt;
	&lt;/div&gt;
	&lt;div class=&quot;row&quot;&gt;
	&lt;/div&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 08 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/08/namespace.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/08/namespace.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>vfs</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;在Linux中，文件系统主要分为下面3种：
（1）基于磁盘的文件系统（Disk-based Filesystem) 是在非易失介质上存储文件的经典方法，用以在多次会话之间保持文件的内容。如Ext2/3/4， Reiserfs, FAT等。
（2）虚拟文件系统（Virtual Filesystem） 在内核中生成，是一种用户应用程序与内核通信的方法。如proc，它不许要在任何类的硬件设备上分配存储空间，所有的信息都是动态在内存中开辟和存储。
（3）网络文件系统（Network Filesystem） 是基于磁盘的文件系统和虚拟文件系统之间的折中。这种文件系统允许访问另一台计算机上的数据，该计算机通过网络连接到本地计算机。在这种情况下，数据实际上存储在一个不同系统的硬件设备上。
由于VFS抽象层的存在，用户空间进程不会看到本地文件系统与网络文件系统之间的区别。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;VFS的模型与结构
VFS不仅为文件系统提供了方法和抽象，还支持文件系统中对象（或文件）的统一视图。并非每一种文件系统都支持VFS中的所有抽象，如FAT，因为其设计没有考虑到此类对象。定义一个最小的通用模型，来支持内核中所有文件系统都实现的那些功能，这是不实际的。因为这样会损失许多本质性的功能特性，或者导致这些特性只能通过特定文件系统的路径访问。
VFS的方案完全相反：提供一种结构模型，包含了一个强大文件系统所具备的所有组件。但该模型只存在于虚拟中，必须使用各种对象和函数指针与每种文件系统适配。所有文件系统的实现都必须提供与VFS定义的结构配合的例程，以弥合两种视图之间的差异。
VFS是由基于经典文件系统的结构衍化而来，所以VFS与Ext类文件系统类似，从而在处理Ext类文件系统的时候，Ext和VFS之间的转换，几乎不会损失时间。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一些基本概念:
文件 一组在逻辑上具有完整意义的信息项的系列。在Linux中，除了普通文件，其他诸如目录、设备、套接字等 也以文件被对待。总之，“一切皆文件”。
目录 目录好比一个文件夹，用来容纳相关文件。因为目录可以包含子目录，所以目录是可以层层嵌套，形成 文件路径。在Linux中，目录也是以一种特殊文件被对待的，所以用于文件的操作同样也可以用在目录上。
目录项 在一个文件路径中，路径中的每一部分都被称为目录项；如路径/home/source/helloworld.c中，目录 /, home, source和文件 helloworld.c都是一个目录项。
索引节点 用于存储文件的元数据的一个数据结构。文件的元数据，也就是文件的相关信息，和文件本身是两个不同 的概念。它包含的是诸如文件的大小、拥有者、创建时间、磁盘位置等和文件相关的信息。
超级块 用于存储文件系统的控制信息的数据结构。描述文件系统的状态、文件系统类型、大小、区块数、索引节 点数等，存放于磁盘的特定扇区中&lt;/p&gt;

</description>
        <pubDate>Thu, 07 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/07/vfs.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/07/vfs.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>mysql_index</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;如果想知道MySQL数据库中每个表占用的空间、表记录的行数的话，可以打开MySQL的 information_schema 数据库。在该库中有一个 TABLES 表，这个表主要字段分别是：&lt;/p&gt;

&lt;p&gt;TABLE_SCHEMA : 数据库名
TABLE_NAME：表名
ENGINE：所使用的存储引擎
TABLES_ROWS：记录数
DATA_LENGTH：数据大小
INDEX_LENGTH：索引大小&lt;/p&gt;

&lt;p&gt;其他字段请参考MySQL的手册，我们只需要了解这几个就足够了。&lt;/p&gt;

&lt;p&gt;1  首先查看某一实例下的所有占用磁盘空间（表数据+索引数据，得到的结果为B，这里做了数据处理转成M）：&lt;/p&gt;

&lt;p&gt;select concat(round((sum(DATA_LENGTH)+sum(INDEX_LENGTH))/1024/1024,2),’M’) from information_schema.tables where table_schema=’实例名称’;
 上面是查询所有的表计的累计量，下面是是查询单个表计的的SQL(按照实例名查询)：&lt;/p&gt;

&lt;p&gt;select table_name,
DATA_LENGTH/1024/1024 as tablesData,
INDEX_LENGTH/1024/1024 as indexData 
from information_schema.tables
where table_schema=’dsm’
ORDER BY  tablesData desc;&lt;/p&gt;

</description>
        <pubDate>Thu, 07 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/web/2017/12/07/mysql_index.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/web/2017/12/07/mysql_index.html</guid>
        
        
        <category>web</category>
        
      </item>
    
      <item>
        <title>linux_rcu</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;1:RCU使用在读者多而写者少的情况.RCU和读写锁相似.但RCU的读者占锁没有任何的系统开销.写者与写写者之间必须要保持同步,且写者必须要等它之前的读者全部都退出之后才能释放之前的资源.
2:RCU保护的是指针.这一点尤其重要.因为指针赋值是一条单指令.也就是说是一个原子操作.因它更改指针指向没必要考虑它的同步.只需要考虑cache的影响. 
3:读者是可以嵌套的.也就是说rcu_read_lock()可以嵌套调用. 
4:读者在持有rcu_read_lock()的时候,不能发生进程上下文切换.否则,因为写者需要要等待读者完成,写者进程也会一直被阻塞.&lt;/p&gt;

&lt;p&gt;RCU（Read-Copy Update）是数据同步的一种方式，在当前的Linux内核中发挥着重要的作用。RCU主要针对的数据对象是链表，目的是提高遍历读取数据的效率，为了达到目的使用RCU机制读取数据的时候不对链表进行耗时的加锁操作。这样在同一时间可以有多个线程同时读取该链表，并且允许一个线程对链表进行修改（修改的时候，需要加锁）。RCU适用于需要频繁的读取数据，而相应修改数据并不多的情景，例如在文件系统中，经常需要查找定位目录，而对目录的修改相对来说并不多，这就是RCU发挥作用的最佳场景。&lt;/p&gt;

&lt;p&gt;在RCU的实现过程中，我们主要解决以下问题：
1，在读取过程中，另外一个线程删除了一个节点。删除线程可以把这个节点从链表中移除，但它不能直接销毁这个节点，必须等到所有的读取线程读取完成以后，才进行销毁操作。RCU中把这个过程称为宽限期（Grace period）。
2，在读取过程中，另外一个线程插入了一个新节点，而读线程读到了这个节点，那么需要保证读到的这个节点是完整的。这里涉及到了发布-订阅机制（Publish-Subscribe Mechanism）。
3， 保证读取链表的完整性。新增或者删除一个节点，不至于导致遍历一个链表从中间断开。但是RCU并不保证一定能读到新增的节点或者不读到要被删除的节点。&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_rcu.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_rcu.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Linux的mmap内存映射机制</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;一个进程应该包括一个mm_struct(memory manage struct), 该结构是进程虚拟地址空间的抽象描述,里面包括了进程虚拟空间的一些管理信息: start_code, end_code, start_data, end_data, start_brk, end_brk等等信息.另外,也有一个指向进程虚存区表(vm_area_struct: virtual memory area)的指针,该链是按照虚拟地址的增长顺序排列的.在Linux进程的地址空间被分作许多区(vma),每个区(vma)都对应虚拟地址空间上一段连续的区域, vma是可以被共享和保护的独立实体,这里的vma就是前面提到的内存对象.&lt;/p&gt;

&lt;p&gt;设备驱动的mmap实现主要是将一个物理设备的可操作区域（设备空间）映射到一个进程的虚拟地址空间。这样就可以直接采用指针的方式像访问内存的方式访问设备。在驱动中的mmap实现主要是完成一件事，就是实际物理设备的操作区域到进程虚拟空间地址的映射过程。同时也需要保证这段映射的虚拟存储器区域不会被进程当做一般的空间使用，因此需要添加一系列的保护方式。&lt;/p&gt;

&lt;p&gt;Linux提供了内存映射函数mmap,它把文件内容映射到一段内存上(准确说是虚拟内存上),通过对这段内存的读取和修改,实现对文件的读取和修改 。普通文件被映射到进程地址空间后，进程可以向访问普通内存一样对文件进行访问，不必再调用read()，write（）等操作。
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/linuxMMap.jpeg&quot; /&gt;
mmap系统调用的实现过程是
1.先通过文件系统定位要映射的文件；
2.权限检查,映射的权限不会超过文件打开的方式,也就是说如果文件是以只读方式打开,那么则不允许建立一个可写映射； 
3.创建一个vma对象,并对之进行初始化； 
4.调用映射文件的mmap函数,其主要工作是给vm_ops向量表赋值；
5.把该vma链入该进程的vma链表中,如果可以和前后的vma合并则合并；
6.如果是要求VM_LOCKED(映射区不被换出)方式映射,则发出缺页请求,把映射页面读入内存中.&lt;/p&gt;

&lt;p&gt;2、munmap函数
munmap(void * start, size_t length):
该调用可以看作是mmap的一个逆过程.它将进程中从start开始length长度的一段区域的映射关闭,如果该区域不是恰好对应一个vma,则有可能会分割几个或几个vma.
 msync(void * start, size_t length, int flags):
把映射区域的修改回写到后备存储中.因为munmap时并不保证页面回写,如果不调用msync,那么有可能在munmap后丢失对映射区的修改.其中flags可以是MS_SYNC, MS_ASYNC, MS_INVALIDATE, MS_SYNC要求回写完成后才返回, MS_ASYNC发出回写请求后立即返回, MS_INVALIDATE使用回写的内容更新该文件的其它映射.该系统调用是通过调用映射文件的sync函数来完成工作的.
brk(void * end_data_segement):
将进程的数据段扩展到end_data_segement指定的地址,该系统调用和mmap的实现方式十分相似,同样是产生一个vma,然后指定其属性.不过在此之前需要做一些合法性检查,比如该地址是否大于mm-&amp;gt;end_code, end_data_segement和mm-&amp;gt;brk之间是否还存在其它vma等等.通过brk产生的vma映射的文件为空,这和匿名映射产生的vma相似,关于匿名映射不做进一步介绍.库函数malloc就是通过brk实现的.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_mmap.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_mmap.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>linux_lock</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;在现代操作系统里，同一时间可能有多个内核执行流在执行，因此内核其实象多进程多线程编程一样也需要一些同步机制来同步各执行单元对共享数据的访问。尤其是在多处理器系统上，更需要一些同步机制来同步不同处理器上的执行单元对共享的数据的访问。在主流的Linux内核中包含了几乎所有现代的操作系统具有的同步机制，这些同步机制包括：原子操作、信号量（semaphore）、读写信号量（rw_semaphore）、spinlock、 BKL(Big Kernel Lock)、rwlock、brlock（只包含在2.4内核中）、RCU（只包含在2.6内核中）和seqlock（只包含在2.6内核中）。&lt;/p&gt;

&lt;p&gt;原子操作通常用于实现资源的引用计数，在TCP/IP协议栈的IP碎片处理中，就使用了引用计数，碎片队列结构struct ipq描述了一个IP碎片，字段refcnt就是引用计数器，它的类型为atomic_t，当创建IP碎片时（在函数ip_frag_create中），使用atomic_set函数把它设置为1，当引用该IP碎片时，就使用函数atomic_inc把引用计数加1，当不需要引用该IP碎片时，就使用函数 ipq_put来释放该IP碎片，ipq_put使用函数atomic_dec_and_test把引用计数减1并判断引用计数是否为0，如果是就释放 IP碎片。函数ipq_kill把IP碎片从ipq队列中删除，并把该删除的IP碎片的引用计数减1（通过使用函数atomic_dec实现）。&lt;/p&gt;

&lt;p&gt;Linux内核的信号量在概念和原理上与用户态的System V的IPC机制信号量是一样的，但是它绝不可能在内核之外使用，因此它与System V的IPC机制信号量毫不相干。&lt;/p&gt;

&lt;p&gt;信号量在创建时需要设置一个初始值，表示同时可以有几个任务可以访问该信号量保护的共享资源，初始值为1就变成互斥锁（Mutex），即同时只能有一个任务可以访问信号量保护的共享资源。一个任务要想访问共享资源，首先必须得到信号量，获取信号量的操作将把信号量的值减1，若当前信号量的值为负数，表明无法获得信号量，该任务必须挂起在该信号量的等待队列等待该信号量可用；若当前信号量的值为非负数，表示可以获得信号量，因而可以立刻访问被该信号量保护的共享资源。当任务访问完被信号量保护的共享资源后，必须释放信号量，释放信号量通过把信号量的值加1实现，如果信号量的值为非正数，表明有任务等待当前信号量，因此它也唤醒所有等待该信号量的任务。&lt;/p&gt;

&lt;p&gt;读写信号量对访问者进行了细分，或者为读者，或者为写者，读者在保持读写信号量期间只能对该读写信号量保护的共享资源进行读访问，如果一个任务除了需要读，可能还需要写，那么它必须被归类为写者，它在对共享资源访问之前必须先获得写者身份，写者在发现自己不需要写访问的情况下可以降级为读者。读写信号量同时拥有的读者数不受限制，也就说可以有任意多个读者同时拥有一个读写信号量。如果一个读写信号量当前没有被写者拥有并且也没有写者等待读者释放信号量，那么任何读者都可以成功获得该读写信号量；否则，读者必须被挂起直到写者释放该信号量。如果一个读写信号量当前没有被读者或写者拥有并且也没有写者等待该信号量，那么一个写者可以成功获得该读写信号量，否则写者将被挂起，直到没有任何访问者。因此，写者是排他性的，独占性的。&lt;/p&gt;

&lt;p&gt;读写信号量有两种实现，一种是通用的，不依赖于硬件架构，因此，增加新的架构不需要重新实现它，但缺点是性能低，获得和释放读写信号量的开销大；另一种是架构相关的，因此性能高，获取和释放读写信号量的开销小，但增加新的架构需要重新实现。在内核配置时，可以通过选项去控制使用哪一种实现。&lt;/p&gt;

&lt;p&gt;自旋锁与互斥锁有点类似，只是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。由于自旋锁使用者一般保持锁时间非常短，因此选择自旋而不是睡眠是非常必要的，自旋锁的效率远高于互斥锁。&lt;/p&gt;

&lt;p&gt;信号量和读写信号量适合于保持时间较长的情况，它们会导致调用者睡眠，因此只能在进程上下文使用（_trylock的变种能够在中断上下文使用），而自旋锁适合于保持时间非常短的情况，它可以在任何上下文使用。如果被保护的共享资源只在进程上下文访问，使用信号量保护该共享资源非常合适，如果对共巷资源的访问时间非常短，自旋锁也可以。但是如果被保护的共享资源需要在中断上下文访问（包括底半部即中断处理句柄和顶半部即软中断），就必须使用自旋锁。&lt;/p&gt;

&lt;p&gt;自旋锁保持期间是抢占失效的，而信号量和读写信号量保持期间是可以被抢占的。自旋锁只有在内核可抢占或SMP的情况下才真正需要，在单CPU且不可抢占的内核下，自旋锁的所有操作都是空操作。&lt;/p&gt;

&lt;p&gt;跟互斥锁一样，一个执行单元要想访问被自旋锁保护的共享资源，必须先得到锁，在访问完共享资源后，必须释放锁。如果在获取自旋锁时，没有任何执行单元保持该锁，那么将立即得到锁；如果在获取自旋锁时锁已经有保持者，那么获取锁操作将自旋在那里，直到该自旋锁的保持者释放了锁。&lt;/p&gt;

&lt;p&gt;无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_lock.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_lock.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>linux_elf</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;可执行连接格式是UNIX系统实验室(USL)作为应用程序二进制接口
(Application Binary Interface(ABI)而开发和发布的。工具接口标准委
员会(TIS)选择了正在发展中的ELF标准作为工作在32位INTEL体系上不同操
作系统之间可移植的二进制文件格式。
假定开发者定义了一个二进制接口集合，ELF标准用它来支持流线型的软件
发展。 应该减少不同执行接口的数量。因此可以减少重新编程重新编译的
代码。&lt;/p&gt;

&lt;p&gt;在object文件中有三种主要的类型。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;一个可重定位(relocatable)文件保存着代码和适当的数据，用来和其他的
object文件一起来创建一个可执行文件或者是一个共享文件。&lt;/li&gt;
  &lt;li&gt;一个可执行(executable)文件保存着一个用来执行的程序；该文件指出了
exec(BA_OS)如何来创建程序进程映象。&lt;/li&gt;
  &lt;li&gt;一个共享object文件保存着代码和合适的数据，用来被下面的两个链接器
链接。第一个是连接编辑器[请参看ld(SD_CMD)]，可以和其他的可重定位和
共享object文件来创建其他的object。第二个是动态链接器，联合一个
可执行文件和其他的共享object文件来创建一个进程映象。
一个object文件被汇编器和联接器创建, 想要在处理机上直接运行的object
文件都是以二进制来存放的。那些需要抽象机制的程序，比如象shell脚本，
是不被接受的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个ELF头在文件的开始，保存了路线图(road map)，描述了该文件的组织情况。
sections保存着object 文件的信息，从连接角度看：包括指令，数据，
符号表，重定位信息等等。&lt;/p&gt;

&lt;p&gt;假如一个程序头表（program header table）存在，那么它告诉系统如何来创建一
个进程的内存映象。被用来建立进程映象(执行一个程序)的文件必须要有一个程
序头表（program header table）；可重定位文件不需要这个头表。一个
section头表（section header table）包含了描述文件sections的信息。每个
section在这个表中有一个入口；每个入口给出了该section的名字，大小，
等等信息。在联接过程中的文件必须有一个section头表；其他object文件可要
可不要这个section头表。&lt;/p&gt;

&lt;p&gt;对象文件(Object files)有三个种类：&lt;/p&gt;

&lt;p&gt;1) 可重定位的对象文件(Relocatable file)&lt;/p&gt;

&lt;p&gt;这是由汇编器汇编生成的 .o 文件。后面的链接器(link editor)拿一个或一些 Relocatable object files 作为输入，经链接处理后，生成一个可执行的对象文件 (Executable file) 或者一个可被共享的对象文件(Shared object file)。我们可以使用 ar 工具将众多的 .o Relocatable object files 归档(archive)成 .a 静态库文件。如何产生 Relocatable file，你应该很熟悉了，请参见我们相关的基本概念文章和JulWiki。另外，可以预先告诉大家的是我们的内核可加载模块 .ko 文件也是 Relocatable object file。&lt;/p&gt;

&lt;p&gt;2) 可执行的对象文件(Executable file)&lt;/p&gt;

&lt;p&gt;这我们见的多了。文本编辑器vi、调式用的工具gdb、播放mp3歌曲的软件mplayer等等都是Executable object file。你应该已经知道，在我们的 Linux 系统里面，存在两种可执行的东西。除了这里说的 Executable object file，另外一种就是可执行的脚本(如shell脚本)。注意这些脚本不是 Executable object file，它们只是文本文件，但是执行这些脚本所用的解释器就是 Executable object file，比如 bash shell 程序。&lt;/p&gt;

&lt;p&gt;3) 可被共享的对象文件(Shared object file)&lt;/p&gt;

&lt;p&gt;这些就是所谓的动态库文件，也即 .so 文件。如果拿前面的静态库来生成可执行程序，那每个生成的可执行程序中都会有一份库代码的拷贝。如果在磁盘中存储这些可执行程序，那就会占用额外的磁盘空间；另外如果拿它们放到Linux系统上一起运行，也会浪费掉宝贵的物理内存。如果将静态库换成动态库，那么这些问题都不会出现。动态库在发挥作用的过程中，必须经过两个步骤：&lt;/p&gt;

&lt;p&gt;a) 链接编辑器(link editor)拿它和其他Relocatable object file以及其他shared object file作为输入，经链接处理后，生存另外的 shared object file 或者 executable file。&lt;/p&gt;

&lt;p&gt;b) 在运行时，动态链接器(dynamic linker)拿它和一个Executable file以及另外一些 Shared object file 来一起处理，在Linux系统里面创建一个进程映像。&lt;/p&gt;

&lt;p&gt;以上所提到的 link editor 以及 dynamic linker 是什么东西，你可以参考我们基本概念中的相关文章。对于什么是编译器，汇编器等你应该也已经知道，在这里只是使用他们而不再对他们进行详细介绍。为了下面的叙述方便，你可以下载test.tar.gz包，解压缩后使用”make”进行编译。编译完成后，会在目录中生成一系列的ELF对象文件，更多描述见里面的 README 文件。我们下面的论述都基于这些产生的对象文件。&lt;/p&gt;

&lt;p&gt;ELF格式需要使用在两种场合：&lt;/p&gt;

&lt;p&gt;a) 组成不同的可重定位文件，以参与可执行文件或者可被共享的对象文件的链接构建；&lt;/p&gt;

&lt;p&gt;b) 组成可执行文件或者可被共享的对象文件，以在运行时内存中进程映像的构建。&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_elf.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_elf.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>linux_cow</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;在Linux程序中，fork（）会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调用，出于效率考虑，linux中引入了“写时复制“技术，也就是只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  那么子进程的物理空间没有代码，怎么去取指令执行exec系统调用呢？

  在fork之后exec之前两个进程用的是相同的物理空间（内存区），子进程的代码段、数据段、堆栈都是指向父进程的物理空间，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间，如果不是因为exec，内核会给子进程的数据段、堆栈段分配相应的物理空间（至此两者有各自的进程空间，互不影响），而代码段继续共享父进程的物理空间（两者的代码完全相同）。而如果是因为exec，由于两者执行的代码不同，子进程的代码段也会分配单独的物理空间。      

  在网上看到还有个细节问题就是，fork之后内核会通过将子进程放在队列的前面，以让子进程先执行，以免父进程执行导致写时复制，而后子进程执行exec系统调用，因无意义的复制而造成效率的下降。
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_cow.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/06/linux_cow.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Kibana</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;ibana是一个针对Elasticsearch的开源分析及可视化平台，用来搜索、查看交互存储在Elasticsearch索引中的数据。使用Kibana，可以通过各种图表进行高级数据分析及展示。&lt;/p&gt;

&lt;p&gt;Kibana让海量数据更容易理解。它操作简单，基于浏览器的用户界面可以快速创建仪表板（dashboard）实时显示Elasticsearch查询动态。&lt;/p&gt;

&lt;p&gt;设置Kibana非常简单。无需编码或者额外的基础架构，几分钟内就可以完成Kibana安装并启动Elasticsearch索引监测。&lt;/p&gt;

&lt;p&gt;主要功能&lt;/p&gt;

&lt;p&gt;Elasticsearch无缝之集成
Kibana架构为Elasticsearch定制，可以将任何结构化和非结构化数据加入Elasticsearch索引。Kibana还充分利用了Elasticsearch强大的搜索和分析功能&lt;/p&gt;

&lt;p&gt;Kibana可以非常方便地把来自Logstash、ES-Hadoop、Beats或第三方技术的数据整合到Elasticsearch，支持的第三方技术包括Apache Flume、Fluentd等。&lt;/p&gt;

&lt;p&gt;Sense是一个可视化终端，通过Kibana插件支持自动补全、自动缩进和语法检查功能。提升了与Elasticsearch API交互的体验&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/12/06/Kibana.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/12/06/Kibana.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>linux_memory</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;TLB(Translation Lookaside Buffer)转换检测缓冲区是一个内存管理单元,用于改进虚拟地址到物理地址转换速度的缓存。
TLB是一个小的，虚拟寻址的缓存，其中每一行都保存着一个由单个PTE(Page Table Entry,页表项)组成的块。如果没有TLB，则每次取数据都需要两次访问内存，即查页表获得物理地址和取数据。
当cpu要访问一个虚拟地址/线性地址时，CPU会首先根据虚拟地址的高20位（20是x86特定的，不同架构有不同的值）在TLB中查找。如果是表中没有相应的表项，称为TLB miss，需要通过访问慢速RAM中的页表计算出相应的物理地址。同时，物理地址被存放在一个TLB表项中，以后对同一线性地址的访问，直接从TLB表项中获取物理地址即可，称为TLB hit。
Linux把物理内存划分为三个层次来管理
存储节点(Node)	CPU被划分为多个节点(node), 内存则被分簇, 每个CPU对应一个本地物理内存, 即一个CPU-node对应一个内存簇bank，即每个内存簇被认为是一个节点
管理区(Zone)	每个物理内存节点node被划分为多个内存管理区域, 用于表示不同范围的内存, 内核可以使用不同的映射方式映射物理内存
页面(Page)	内存被细分为多个页面帧, 页面是最基本的页面分配的单位　
为了支持NUMA模型，也即CPU对不同内存单元的访问时间可能不同，此时系统的物理内存被划分为几个节点(node), 一个node对应一个内存簇bank，即每个内存簇被认为是一个节点&lt;/p&gt;

&lt;p&gt;首先, 内存被划分为结点. 每个节点关联到系统中的一个处理器, 内核中表示为pg_data_t的实例. 系统中每个节点被链接到一个以NULL结尾的pgdat_list链表中&amp;lt;而其中的每个节点利用pg_data_tnode_next字段链接到下一节．而对于PC这种UMA结构的机器来说, 只使用了一个成为contig_page_data的静态pg_data_t结构.&lt;/p&gt;

&lt;p&gt;接着各个节点又被划分为内存管理区域, 一个管理区域通过struct zone_struct描述, 其被定义为zone_t, 用以表示内存的某个范围, 低端范围的16MB被描述为ZONE_DMA, 某些工业标准体系结构中的(ISA)设备需要用到它, 然后是可直接映射到内核的普通内存域ZONE_NORMAL,最后是超出了内核段的物理地址域ZONE_HIGHMEM, 被称为高端内存.　是系统中预留的可用内存空间, 不能被内核直接映射.&lt;/p&gt;

&lt;p&gt;最后页帧(page frame)代表了系统内存的最小单位, 堆内存中的每个页都会创建一个struct page的一个实例. 传统上，把内存视为连续的字节，即内存为字节数组，内存单元的编号(地址)可作为字节数组的索引. 分页管理时，将若干字节视为一页，比如4K byte. 此时，内存变成了连续的页，即内存为页数组，每一页物理内存叫页帧，以页为单位对内存进行编号，该编号可作为页数组的索引，又称为页帧号.&lt;/p&gt;

&lt;p&gt;传统伙伴系统算法&lt;/p&gt;

&lt;p&gt;在内核分配内存时, 必须记录页帧的已分配或空闲状态, 以免两个进程使用同样的内存区域. 由于内存分配和释放非常频繁, 内核还必须保证相关操作尽快完成. 内核可以只分配完整的页帧. 将内存划分为更小的部分的工作, 则委托给用户空间中的标准库. 标准库将来源于内核的页帧拆分为小的区域, 并为进程分配内存.&lt;/p&gt;

&lt;p&gt;内核中很多时候要求分配连续页. 为快速检测内存中的连续区域, 内核采用了一种古老而历经检验的技术: 伙伴系统&lt;/p&gt;

&lt;p&gt;系统中的空闲内存块总是两两分组, 每组中的两个内存块称作伙伴. 伙伴的分配可以是彼此独立的. 但如果两个伙伴都是空闲的, 内核会将其合并为一个更大的内存块, 作为下一层次上某个内存块的伙伴.
Linux内核中引入了伙伴系统算法(buddy system)。把所有的空闲页框分组为11个块链表，每个块链表分别包含大小为1，2，4，8，16，32，64，128，256，512和1024个连续页框的页框块。最大可以申请1024个连续页框，对应4MB大小的连续内存。每个页框块的第一个页框的物理地址是该块大小的整数倍。
 假设要申请一个256个页框的块，先从256个页框的链表中查找空闲块，如果没有，就去512个页框的链表中找，找到了则将页框块分为2个256个页框的块，一个分配给应用，另外一个移到256个页框的链表中。如果512个页框的链表中仍没有空闲块，继续向1024个页框的链表查找，如果仍然没有，则返回错误&lt;/p&gt;

&lt;p&gt;Buddy算法的优缺点：
1）尽管伙伴内存算法在内存碎片问题上已经做的相当出色，但是该算法中，一个很小的块往往会阻碍一个大块的合并，一个系统中，对内存块的分配，大小是随机的，一片内存中仅一个小的内存块没有释放，旁边两个大的就不能合并。&lt;/p&gt;

&lt;p&gt;2）算法中有一定的浪费现象，伙伴算法是按2的幂次方大小进行分配内存块，当然这样做是有原因的，即为了避免把大的内存块拆的太碎，更重要的是使分配和释放过程迅速。但是他也带来了不利的一面，如果所需内存大小不是2的幂次方，就会有部分页面浪费。有时还很严重。比如原来是1024个块，申请了16个块，再申请600个块就申请不到了，因为已经被分割了。
3）另外拆分和合并涉及到 较多的链表和位图操作，开销还是比较大的。
Buddy（伙伴的定义）：
这里给出伙伴的概念，满足以下三个条件的称为伙伴：
1）两个块大小相同；
2）两个块地址连续；
3）两个块必须是同一个大块中分离出来的；
Buddy算法的分配原理：
假如系统需要4(2&lt;em&gt;2)个页面大小的内存块，该算法就到free_area[2]中查找，如果链表中有空闲块，就直接从中摘下并分配出去。如果没有，算法将顺着数组向上查找free_area[3],如果free_area[3]中有空闲块，则将其从链表中摘下，分成等大小的两部分，前四个页面作为一个块插入free_area[2]，后4个页面分配出去，free_area[3]中也没有，就再向上查找，如果free_area[4]中有，就将这16(2&lt;/em&gt;2&lt;em&gt;2&lt;/em&gt;2)个页面等分成两份，前一半挂如free_area[3]的链表头部，后一半的8个页等分成两等分，前一半挂free_area[2]
的链表中，后一半分配出去。假如free_area[4]也没有，则重复上面的过程，知道到达free_area数组的最后，如果还没有则放弃分配。&lt;/p&gt;

&lt;p&gt;Buddy算法的释放原理：
内存的释放是分配的逆过程，也可以看作是伙伴的合并过程。当释放一个块时，先在其对应的链表中考查是否有伙伴存在，如果没有伙伴块，就直接把要释放的块挂入链表头；如果有，则从链表中摘下伙伴，合并成一个大块，然后继续考察合并后的块在更大一级链表中是否有伙伴存在，直到不能合并或者已经合并到了最大的块(2&lt;em&gt;2&lt;/em&gt;2&lt;em&gt;2&lt;/em&gt;2&lt;em&gt;2&lt;/em&gt;2&lt;em&gt;2&lt;/em&gt;2个页面)。
slab机制
slab是Linux操作系统的一种内存分配机制。其工作是针对一些经常分配并释放的对象，如进程描述符等，这些对象的大小一般比较小，如果直接采用伙伴系统来进行分配和释放，不仅会造成大量的内碎片，而且处理速度也太慢。而slab分配器是基于对象进行管理的，相同类型的对象归为一类(如进程描述符就是一类)，每当要申请这样一个对象，slab分配器就从一个slab列表中分配一个这样大小的单元出去，而当要释放时，将其重新保存在该列表中，而不是直接返回给伙伴系统，从而避免这些内碎片。slab分配器并不丢弃已分配的对象，而是释放并把它们保存在内存中。当以后又要请求新的对象时，就可以从内存直接获取而不用重复初始化。&lt;/p&gt;

&lt;p&gt;Linux 的slab 可有三种状态：
 满的：slab 中的所有对象被标记为使用。
 空的：slab 中的所有对象被标记为空闲。
 部分：slab 中的对象有的被标记为使用，有的被标记为空闲。
slab 分配器首先从部分空闲的slab 进行分配。如没有，则从空的slab 进行分配。如没有，则从物理连续页上分配新的slab，并把它赋给一个cache ，然后再从新slab 分配空间。&lt;/p&gt;

&lt;p&gt;与传统的内存管理模式相比， slab 缓存分配器提供了很多优点。
1、内核通常依赖于对小对象的分配，它们会在系统生命周期内进行无数次分配。
2、slab 缓存分配器通过对类似大小的对象进行缓存而提供这种功能，从而避免了常见的碎片问题。
3、slab 分配器还支持通用对象的初始化，从而避免了为同一目的而对一个对象重复进行初始化。
4、slab 分配器还可以支持硬件缓存对齐和着色，这允许不同缓存中的对象占用相同的缓存行，从而提高缓存的利用率并获得更好的性能&lt;/p&gt;

&lt;p&gt;Slab是基础，是最早从Sun OS那引进的；Slub是在Slab上进行的改进，在大型机上表现出色（不知道在普通PC上如何），据说还被IA-64作为默认；而Slob是针对小型系统设计的，当然了，主要是嵌入式。&lt;/p&gt;

</description>
        <pubDate>Tue, 05 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/linux/2017/12/05/linux_memory.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/linux/2017/12/05/linux_memory.html</guid>
        
        
        <category>linux</category>
        
      </item>
    
  </channel>
</rss>
