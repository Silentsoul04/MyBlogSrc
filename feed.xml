<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>泽民博客</title>
    <description>夏泽民的个人主页，学习笔记。</description>
    <link>https://xiazemin.github.io/MyBlog/</link>
    <atom:link href="https://xiazemin.github.io/MyBlog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 13 Oct 2017 13:36:57 +0800</pubDate>
    <lastBuildDate>Fri, 13 Oct 2017 13:36:57 +0800</lastBuildDate>
    <generator>Jekyll v3.6.0.pre.beta1</generator>
    
      <item>
        <title>spark基本概念</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;我们知道Spark总是以集群的方式运行的，Standalone的部署方式是集群方式中最为精简的一种（另外的是Mesos和Yarn）。Standalone模式中，资源调度是自己实现的，是MS架构的集群模式，故存在单点故障问题。
下面提出几个问题并解决：
1、Standalone部署方式下包含哪些节点？&lt;/p&gt;

&lt;p&gt;由不同级别的三个节点组成，分别是Master主控节点、Worker工作节点、客户端节点；
（1）其中Master主控节点，顾名思义，类似于领导者，在整个集群中，最多只有一个Master处于Active状态。在使用spark-shell等交互式运行或者使用官方提供的run-example实例时，Driver运行在Master节点中；若是使用spark-submit工具进行任务的提交或者IDEA等工具开发运行任务时，Driver是运行在本地客户端的。
Master一方面负责各种信息，比如Driver、Worker、Application的注册；另一方面还负责Executor的启动，Worker心跳等诸多信息的处理。
（2）Woker节点，类似于yarn中的NodeManager，在整个集群中，可以有多个Worker（&amp;gt;0）。负责当前WorkerNode上的资源汇报、监督当前节点运行的Executor。并通过心跳机制来保持和Master的存活性连接。Executor受到Worker掌控，一个Worker启动Executor的个数受限于 机器中CPU核数。每个Worker节点存在一个多个CoarseGrainedExecutorBackend进程，每个进程包含一个Executor对象，该对象持有一个线程池，每个线程执行一个Task。
2、基本的概念？&lt;/p&gt;

&lt;p&gt;（1）Application：指的是用户编写的Spark应用程序，包含了含有一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码。
（2）Driver:运行Application的main函数并创建SparkContext，SparkContext的目的是为了准备Spark应用程序的运行环境。SparkContext负责资源的申请、任务分配和监控等。当Executor运行结束后，Driver负责关闭SparkContext；
（3）Job：一个Application可以产生多个Job，其中Job由Spark Action触发产生。每个Job包含多个Task组成的并行计算。
（4）Stage：每个Job会拆分为多个Task，作为一个TaskSet,称为Stage；Stage的划分和调度是由DAGScheduler负责的。Stage分为Result Stage和Shuffle Map Stage；
（5）Task：Application的运行基本单位，Executor上的工作单元。其调度和 管理又TaskScheduler负责。
（6）RDD：Spark基本计算单元，是Spark最核心的东西。表示已被分区、被序列化、不可变的、有容错机制的、能被并行操作的数据集合。
(7) DAGScheduler:根据Job构建基于Stage的DAG，划分Stage依据是RDD之间的依赖关系。
（8）TaskScheduler：将TaskSet提交给Worker运行，每个Worker运行了什么Task于此处分配。同时还负责监控、汇报任务运行情况等。
3、Standalone启动过程是啥？&lt;/p&gt;

&lt;p&gt;（1）首先，启动master，worker节点。
worker启动后触发Master的RegisterWorker事件，进行注册。主要讲要注册的Worker信息封装成WorkerInfo对象，包括Worker节点的CPU、内存等基本信息。记录Worker的信息（IP、Address）到master缓存中（HashMap），若Worker节点的注册信息有效，持久化已注册的Worker信息。然后给个完成注册的反馈信号。
（2）提交Application
运行spark-shell时，会由Driver端的DAGScheduler向Master发送RegisterApplication请求。根据此请求信息会创建ApplicationInfo对象，将Application加入到Master的缓存apps中，这个结构是HashSet。
如果worker已经注册，发送lanchExecutor指令给相应的Worker。
（3）Worker收到lanchExecutor后，会由ExecutorRunner启动Excutor进程，启动的Executor进程会根据启动时的入参，将自己注册到Drive中的ScheduleBackend。
(4)ScheduleBackend收到Excutor的注册信息后，会将提交到的Spark Job分解为多个具体的Task，然后通过LaunchTask指令将这些Task分散到各个Executor上运行。
4、Standalone部署方式下某一节点出现问题时，系统如何处理？&lt;/p&gt;

&lt;p&gt;出现问题的节点可能发生的情况有三种：
（1）Master崩掉了：这个坏掉了，就真的没法完了。单点故障的问题。
有两种解决办法：第一种基于文件系统的故障恢复，适合Master进程本身挂掉，那直接重启就Ok了。
第二种是基于ZookerKeep的HA方式。此方式被许多的分布式框架使用。
（2）某一Worker崩掉了：
若是所有的Worker挂掉，则整个集群就不可用；
Worker退出之前，会将管控的所有Executor进程kill；由于Worker挂掉，不能向master玩心跳了，根据超时处理会知道Worker挂了，然后Master将相应的情况汇报给Driver。Driver会根据master的信息和没有收到Executor的StatusUpdate确定这个Worker挂了，则Driver会将这个注册的Executor移除。
（3）某Worker的Excutor崩掉了：
Excutor的作为一个独立的进程在运行，由ExcutorRunner线程启动，并收到ExcutorRunner的监控，当Excutor挂了，ExcutorRunner会注意到异常情况，将ExecutorStateChanged汇报给Master，master会再次发送lanchExecutor指令给相应的Worker启动相应的Excutor。&lt;/p&gt;
</description>
        <pubDate>Fri, 13 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/13/spark_concepts.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/13/spark_concepts.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Actor模型原理</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;1.Actor模型
在使用Java进行并发编程时需要特别的关注锁和内存原子性等一系列线程问题，而Actor模型内部的状态由它自己维护即它内部数据只能由它自己修改(通过消息传递来进行状态修改)，所以使用Actors模型进行并发编程可以很好地避免这些问题，Actor由状态(state)、行为(Behavior)和邮箱(mailBox)三部分组成&lt;/p&gt;

&lt;p&gt;状态(state)：Actor中的状态指的是Actor对象的变量信息，状态由Actor自己管理，避免了并发环境下的锁和内存原子性等问题
行为(Behavior)：行为指定的是Actor中计算逻辑，通过Actor接收到消息来改变Actor的状态
邮箱(mailBox)：邮箱是Actor和Actor之间的通信桥梁，邮箱内部通过FIFO消息队列来存储发送方Actor消息，接受方Actor从邮箱队列中获取消息
Actor的基础就是消息传递&lt;/p&gt;

&lt;p&gt;2.使用Actor模型的好处：
事件模型驱动–Actor之间的通信是异步的，即使Actor在发送消息后也无需阻塞或者等待就能够处理其他事情
强隔离性–Actor中的方法不能由外部直接调用，所有的一切都通过消息传递进行的，从而避免了Actor之间的数据共享，想要
观察到另一个Actor的状态变化只能通过消息传递进行询问
位置透明–无论Actor地址是在本地还是在远程机上对于代码来说都是一样的
轻量性–Actor是非常轻量的计算单机，单个Actor仅占400多字节，只需少量内存就能达到高并发
3.Actor模型原理&lt;/p&gt;

&lt;p&gt;创建ActorSystem
ActorSystem作为顶级Actor，可以创建和停止Actors,甚至可关闭整个Actor环境，
此外Actors是按层次划分的，ActorSystem就好比Java中的Object对象，Scala中的Any，
是所有Actors的根，当你通过ActorSystem的actof方法创建Actor时，实际就是在ActorSystem
下创建了一个子Actor。
可通过以下代码来初始化ActorSystem&lt;/p&gt;

&lt;p&gt;val system = ActorSystem(“UniversityMessageSystem”)&lt;/p&gt;

&lt;p&gt;通过ActorSystem创建TeacherActor的代理(ActorRef)
看看TeacherActor的代理的创建代码&lt;/p&gt;

&lt;p&gt;val teacherActorRef:ActorRef = system.actorOf(Props[TeacherActor])&lt;/p&gt;

&lt;p&gt;ActorSystem通过actorOf创建Actor，但其并不返回TeacherActor而是返
回一个类型为ActorRef的东西。
ActorRef作为Actor的代理，使得客户端并不直接与Actor对话，这种Actor
模型也是为了避免TeacherActor的自定义/私有方法或变量被直接访问，所
以你最好将消息发送给ActorRef，由它去传递给目标Actor
发送QuoteRequest消息到代理中
你只需通过!方法将QuoteReques消息发送给ActorRef(注意：ActorRef也有个tell方法,其作用就委托回调给!)&lt;/p&gt;

&lt;p&gt;techerActorRef!QuoteRequest
等价于teacherActorRef.tell(QuoteRequest, teacherActorRef)&lt;/p&gt;

&lt;p&gt;MailBox&lt;/p&gt;

&lt;p&gt;每个Actor都有一个MailBox,同样，Teacher也有个MailBox，其会检查MailBox并处理消息。
MailBox内部采用的是FIFO队列来存储消息，有一点不同的是，现实中我们的最新邮件
会在邮箱的最前面。
Dispatcher&lt;/p&gt;

&lt;p&gt;Dispatcher从ActorRef中获取消息并传递给MailBox,Dispatcher封装了一个线程池，之后在
线程池中执行MailBox。&lt;/p&gt;

&lt;p&gt;protected[akka] override def registerForExecution(mbox: Mailbox, …): Boolean = {
  …
 try {
 executorService execute mbox
 …
}&lt;/p&gt;

&lt;p&gt;为什么能执行MailBox?
看看MailBox的实现,没错，其实现了Runnable接口&lt;/p&gt;

&lt;p&gt;private[akka] abstract class Mailbox(val messageQueue: MessageQueue) extends SystemMessageQueue with Runnable&lt;/p&gt;

</description>
        <pubDate>Fri, 13 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/13/actor_intro.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/13/actor_intro.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Actor系统的实体</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;在Actor系统中，actor之间具有树形的监管结构，并且actor可以跨多个网络节点进行透明通信。 
对于一个Actor而言，其源码中存在Actor，ActorContext，ActorRef等多个概念，它们都是为了描述Actor对象而进行的不同层面的抽象。 
我们先给出一个官方的示例图，再对各个概念进行解释。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/ActorPath.png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;上图很清晰的展示了一个actor在源码层面的不同抽象，和不同actor之间的父子关系：  Actor类的一个成员context是ActorContext类型，ActorContext存储了Actor类的上下文，包括self、sender。  ActorContext还混入了ActorRefFactory特质，其中实现了actorOf方法用来创建子actor。  这是Actor中context的源码：
&lt;/code&gt;&lt;/pre&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt; 1 &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;trait&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Actor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 2 &lt;/span&gt;  &lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 3 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   * Stores the context for this actor, including self, and sender.&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 4 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   * It is implicit to support operations such as `forward`.&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 5 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   *&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 6 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   * WARNING: Only valid within the Actor itself, so do not close over it and&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 7 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   * publish it to other threads!&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 8 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   *&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 9 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   * [[akka.actor.ActorContext]] is the Scala API. `getContext` returns a&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;10 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   * [[akka.actor.UntypedActorContext]], which is the Java API of the actor&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;11 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   * context.&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;12 &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;   */&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;13 &lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;implicit&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ActorContext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;14 &lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contextStack&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ActorCell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contextStack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;15 &lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contextStack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isEmpty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contextStack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;16 &lt;/span&gt;      &lt;span class=&quot;k&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ActorInitializationException&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;17 &lt;/span&gt;        &lt;span class=&quot;s&quot;&gt;s&amp;quot;You cannot create an instance of [&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getClass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getName&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;] explicitly using the constructor (new). &amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;18 &lt;/span&gt;          &lt;span class=&quot;s&quot;&gt;&amp;quot;You have to use one of the &amp;#39;actorOf&amp;#39; factory methods to create a new actor. See the documentation.&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;19 &lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contextStack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;20 &lt;/span&gt;    &lt;span class=&quot;nc&quot;&gt;ActorCell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contextStack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;::&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contextStack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;21 &lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;22 &lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;23 &lt;/span&gt;  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;ActorCell的self成员是ActorRef类型，ActorRef是一个actor的不可变，可序列化的句柄（handle），它可能不在本地或同一个ActorSystem中，它是实现网络空间位置透明性的关键设计。 
这是ActorContext中self的源码：&lt;/p&gt;

&lt;p&gt;trait ActorContext extends ActorRefFactory {&lt;/p&gt;

&lt;p&gt;def self: ActorRef
ActorRef的path成员是ActorPath类型，ActorPath是actor树结构中唯一的地址，它定义了根actor到子actor的顺序。 
这是ActorRef中path的源码：&lt;/p&gt;

&lt;p&gt;abstract class ActorRef extends java.lang.Comparable[ActorRef] with Serializable {
  /**&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Returns the path for this actor (from this actor up to the root actor).
   */
  def path: ActorPath
Actor引用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Actor引用是ActorRef的子类，它的最重要功能是支持向它所代表的actor发送消息。每个actor通过self来访问它的标准（本地）引用，在发送给其它actor的消息中也缺省包含这个引用。反过来，在消息处理过程中，actor可以通过sender来访问到当前消息的发送者的引用。&lt;/p&gt;

&lt;p&gt;不同类型的Actor引用&lt;/p&gt;

&lt;p&gt;根据actor系统的配置，支持几种不同的actor引用：&lt;/p&gt;

&lt;p&gt;纯本地引用被配置成不支持网络功能的，这些actor引用发送的消息不能通过一个网络发送到另一个远程的JVM。
支持远程调用的本地引用使用在支持同一个jvm中actor引用之间的网络功能的actor系统中。为了在发送到其它网络节点后被识别，这些引用包含了协议和远程地址信息。
本地actor引用有一个子类是用在路由（比如，混入了Router trait的actor）。它的逻辑结构与之前的本地引用是一样的，但是向它们发送的消息会被直接重定向到它的子actor。
远程actor引用代表可以通过远程通讯访问的actor，i.e. 从别的jvm向他们发送消息时，Akka会透明地对消息进行序列化。
有几种特殊的actor引用类型，在实际用途中比较类似本地actor引用： 
PromiseActorRef表示一个Promise，作用是从一个actor返回的响应来完成，它是由akka.pattern.ask调用来创建的
DeadLetterActorRef是死信服务的缺省实现，所有接收方被关闭或不存在的消息都在此被重新路由。
EmptyLocalActorRef是查找一个不存在的本地actor路径时返回的：它相当于DeadLetterActorRef，但是它保有其路径因此可以在网络上发送，以及与其它相同路径的存活的actor引用进行比较，其中一些存活的actor引用可能在该actor消失之前得到了。
然后有一些内部实现，你可能永远不会用上： 
有一个actor引用并不表示任何actor，只是作为根actor的伪监管者存在，我们称它为“时空气泡穿梭者”。
在actor创建设施启动之前运行的第一个日志服务是一个伪actor引用，它接收日志事件并直接显示到标准输出上；它就是Logging.StandardOutLogger。
获得Actor引用&lt;/p&gt;

&lt;p&gt;创建Actor&lt;/p&gt;

&lt;p&gt;一个actor系统通常是在根actor上使用ActorSystem.actorOf创建actor，然后使用ActorContext.actorOf从创建出的actor中生出actor树来启动的。这些方法返回指向新创建的actor的引用。每个actor都拥有到它的父亲，它自己和它的子actor的引用。这些引用可以与消息一直发送给别的actor，以便接收方直接回复。&lt;/p&gt;

&lt;p&gt;具体路径查找&lt;/p&gt;

&lt;p&gt;另一种查找actor引用的途径是使用ActorSystem.actorSelection方法，也可以使用ActorContext.actorSelection来在actor之中查询。它会返回一个（未验证的）本地、远程或集群actor引用。向这个引用发送消息或试图观察它的存活状态会在actor系统树中从根开始一层一层从父向子actor发送消息，直到消息到达目标或是出现某种失败，i.e.路径中的某一个actor名字不存在（在实际中这个过程会使用缓存来优化，但相较使用物理actor路径来说仍然增加了开销，因为物理路径能够从actor的响应消息中的发送方引用中获得），这个消息传递过程由Akka自动完成的，对客户端代码不可见。 
使用相对路径向兄弟actor发送消息：&lt;/p&gt;

&lt;p&gt;context.actorSelection(“../brother”) ! msg
1
也可以用绝对路径：&lt;/p&gt;

&lt;p&gt;context.actorSelection(“/user/serviceA”) ! msg
1
查询逻辑Actor层次结构&lt;/p&gt;

&lt;p&gt;由于actor系统是一个类似文件系统的树形结构，对actor的匹配与unix shell中支持的一样：你可以将路径（中的一部分）用通配符(«*» 和«?»)替换来组成对0个或多个实际actor的匹配。由于匹配的结果不是一个单一的actor引用，它拥有一个不同的类型ActorSelection，这个类型不完全支持ActorRef的所有操作。同样，路径选择也可以用ActorSystem.actorSelection或ActorContext.actorSelection两种方式来获得，并且支持发送消息。 
下面是将msg发送给包括当前actor在内的所有兄弟actor：&lt;/p&gt;

&lt;p&gt;context.actorSelection(“../*”) ! msg
1
与远程部署之间的互操作&lt;/p&gt;

&lt;p&gt;当一个actor创建一个子actor，actor系统的部署者会决定新的actor是在同一个jvm中或是在其它的节点上。如果是在其他节点创建actor，actor的创建会通过网络连接来到另一个jvm中进行，结果是新的actor会进入另一个actor系统。 远程系统会将新的actor放在一个专为这种场景所保留的特殊路径下。新的actor的监管者会是一个远程actor引用（代表会触发创建动作的actor）。这时，context.parent（监管者引用）和context.path.parent（actor路径上的父actor）表示的actor是不同的。但是在其监管者中查找这个actor的名称能够在远程节点上找到它，保持其逻辑结构，e.g.当向另外一个未确定(unresolved)的actor引用发送消息时。&lt;/p&gt;

&lt;p&gt;因为设计分布式执行会带来一些限制，最明显的一点就是所有通过电缆发送的消息都必须可序列化。虽然有一点不太明显的就是包括闭包在内的远程角色工厂，用来在远程节点创建角色（即Props内部）。 
另一个结论是，要意识到所有交互都是完全异步的，它意味着在一个计算机网络中一条消息需要几分钟才能到达接收者那里（基于配置），而且可能比在单JVM中有更高丢失率，后者丢失率接近于0（还没有确凿的证据）。&lt;/p&gt;

&lt;p&gt;Akka使用的特殊路径&lt;/p&gt;

&lt;p&gt;在路径树的根上是根监管者，所有的的actor都可以从通过它找到。在第二个层次上是以下这些：&lt;/p&gt;

&lt;p&gt;“/user”是所有由用户创建的顶级actor的监管者，用ActorSystem.actorOf创建的actor在其下一个层次 are found at the next level。
“/system” 是所有由系统创建的顶级actor（如日志监听器或由配置指定在actor系统启动时自动部署的actor）的监管者。
“/deadLetters” 是死信actor，所有发往已经终止或不存在的actor的消息会被送到这里。
“/temp”是所有系统创建的短时actor(i.e.那些用在ActorRef.ask的实现中的actor)的监管者。
“/remote” 是一个人造的路径，用来存放所有其监管者是远程actor引用的actor。
附录-Actor模型概述：&lt;/p&gt;

&lt;p&gt;Actor模型为编写并发和分布式系统提供了一种更高的抽象级别。它将开发人员从显式地处理锁和线程管理的工作中解脱出来，使编写并发和并行系统更加容易。Actor模型是在1973年Carl Hewitt的论文中提的，但是被Erlang语言采用后才变得流行起来，一个成功案例是爱立信使用Erlang非常成功地创建了高并发的可靠的电信系统。&lt;/p&gt;

&lt;p&gt;Actor的树形结构&lt;/p&gt;

&lt;p&gt;像一个商业组织一样，actor自然会形成树形结构。程序中负责某一个功能的actor可能需要把它的任务分拆成更小的、更易管理的部分。为此它启动子Actor并监管它们。要知道每个actor有且仅有一个监管者，就是创建它的那个actor。&lt;/p&gt;

&lt;p&gt;Actor系统的精髓在于任务被分拆开来并进行委托，直到任务小到可以被完整地进行处理。 这样做不仅使任务本身被清晰地划分出结构，而且最终的actor也能按照它们“应该处理的消息类型”，“如何完成正常流程的处理”以及“失败流程应如何处理”来进行解析。如果一个actor对某种状况无法进行处理，它会发送相应的失败消息给它的监管者请求帮助。这样的递归结构使得失败能够在正确的层次进行处理。&lt;/p&gt;

&lt;p&gt;可以将这与分层的设计方法进行比较。分层的设计方法最终很容易形成防御性编程，以防止任何失败被泄露出来。把问题交由正确的人处理会是比将所有的事情“藏在深处”更好的解决方案。&lt;/p&gt;

&lt;p&gt;现在，设计这种系统的难度在于如何决定谁应该监管什么。这当然没有一个唯一的最佳方案，但是有一些可能会有帮助的原则：&lt;/p&gt;

&lt;p&gt;如果一个actort管理另一个actor所做的工作，如分配一个子任务，那么父actor应该监督子actor，原因是父actor知道可能会出现哪些失败情况，知道如何处理它们。
如果一个actor携带着重要数据（i.e. 它的状态要尽可能地不被丢失），这个actor应该将任何可能的危险子任务分配给它所监管的子actor，并酌情处理子任务的失败。视请求的性质，可能最好是为每一个请求创建一个子actor，这样能简化收集回应时的状态管理。这在Erlang中被称为“Error Kernel Pattern”。
如果actor A需要依赖actor B才能完成它的任务，A应该观测B的存活状态并对收到B的终止提醒消息进行响应。这与监管机制不同，因为观测方对监管机制没有影响，需要指出的是，仅仅是功能上的依赖并不足以用来决定是否在树形监管体系中添加子actor。
Actor实体&lt;/p&gt;

&lt;p&gt;一个Actor是一个容器，它包含了 状态，行为，一个邮箱，子Actor和一个监管策略。所有这些包含在一个Actor引用里。&lt;/p&gt;

&lt;p&gt;状态&lt;/p&gt;

&lt;p&gt;Actor对象通常包含一些变量来反映actor所处的可能状态。这可能是一个明确的状态机，或是一个计数器，一组监听器，待处理的请求，等等。这些数据使得actor有价值，并且必须将这些数据保护起来不被其它的actor所破坏。&lt;/p&gt;

&lt;p&gt;好消息是在概念上每个Akka actor都有它自己的轻量线程，这个线程是完全与系统其它部分隔离的。这意味着你不需要使用锁来进行资源同步，可以完全不必担心并发性地来编写你的actor代码。&lt;/p&gt;

&lt;p&gt;在幕后，Akka会在一组线程上运行一组Actor，通常是很多actor共享一个线程，对某一个actor的调用可能会在不同的线程上进行处理。Akka保证这个实现细节不影响处理actor状态的单线程性。
由于内部状态对于actor的操作是至关重要的，所以状态不一致是致命的。当actor失败并由其监管者重新启动，状态会进行重新创建，就象第一次创建这个actor一样。这是为了实现系统的“自愈合”。&lt;/p&gt;

&lt;p&gt;行为&lt;/p&gt;

&lt;p&gt;每次当一个消息被处理时，消息会与actor的当前的行为进行匹配。行为是一个函数，它定义了处理当前消息所要采取的动作，例如如果客户已经授权过了，那么就对请求进行处理，否则拒绝请求。&lt;/p&gt;

&lt;p&gt;邮箱&lt;/p&gt;

&lt;p&gt;Actor的用途是处理消息，这些消息是从其它的actor（或者从actor系统外部）发送过来的。连接发送者与接收者的纽带是actor的邮箱：每个actor有且仅有一个邮箱，所有的发来的消息都在邮箱里排队。排队按照发送操作的时间顺序来进行，这意味着从不同的actor发来的消息在运行时没有一个固定的顺序，这是由于actor分布在不同的线程中。从另一个角度讲，从同一个actor发送多个消息到相同的actor，则消息会按发送的顺序排队。&lt;/p&gt;

&lt;p&gt;可以有不同的邮箱实现供选择，缺省的是FIFO：actor处理消息的顺序与消息入队列的顺序一致。这通常是一个好的选择，但是应用可能需要对某些消息进行优先处理。在这种情况下，可以使用优先邮箱来根据消息优先级将消息放在某个指定的位置，甚至可能是队列头，而不是队列末尾。如果使用这样的队列，消息的处理顺序是由队列的算法决定的，而不是FIFO。&lt;/p&gt;

&lt;p&gt;Akka与其它actor模型实现的一个重要差别在于当前的行为必须处理下一个从队列中取出的消息，Akka不会去扫描邮箱来找到下一个匹配的消息。无法处理某个消息通常是作为失败情况进行处理，除非actor覆盖了这个行为。&lt;/p&gt;

&lt;p&gt;子Actor&lt;/p&gt;

&lt;p&gt;每个actor都是一个潜在的监管者：如果它创建了子actor来委托处理子任务，它会自动地监管它们。子actor列表维护在actor的上下文中，actor可以访问它。对列表的更改是通过context.actorOf(…)创建或者context.stop(child)停止子actor来实现，并且这些更改会立刻生效。实际的创建和停止操作在幕后以异步的方式完成，这样它们就不会“阻塞”其监管者。&lt;/p&gt;

&lt;p&gt;监督策略&lt;/p&gt;

&lt;p&gt;Actor的最后一部分是它用来处理其子actor错误状况的机制。错误处理是由Akka透明地进行处理的。由于策略是actor系统组织结构的基础，所以一旦actor被创建了它就不能被修改。&lt;/p&gt;

&lt;p&gt;考虑对每个actor只有唯一的策略，这意味着如果一个actor的子actor们应用了不同的策略，这些子actor应该按照相同的策略来进行分组，生成中间的监管者，又一次倾向于根据任务到子任务的划分来组织actor系统的结构。&lt;/p&gt;
</description>
        <pubDate>Fri, 13 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/13/actor_detail.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/13/actor_detail.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Actor模型和CSP模型的区别</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;　Akka/Erlang的actor模型与Go语言的协程Goroutine与通道Channel代表的CSP(Communicating Sequential Processes)模型有什么区别呢？&lt;/p&gt;

&lt;p&gt;　　首先这两者都是并发模型的解决方案，我们看看Actor和Channel这两个方案的不同：&lt;/p&gt;

&lt;p&gt;Actor模型&lt;/p&gt;

&lt;p&gt;　　在Actor模型中，主角是Actor，类似一种worker，Actor彼此之间直接发送消息，不需要经过什么中介，消息是异步发送和处理的：
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/actors1.png&quot; alt=&quot;jupyter_use&quot; /&gt;
actor模型&lt;/p&gt;

&lt;p&gt;　　Actor模型描述了一组为了避免并发编程的常见问题的公理:&lt;/p&gt;

&lt;p&gt;　　1.所有Actor状态是Actor本地的，外部无法访问。
　　2.Actor必须只有通过消息传递进行通信。　　
　　3.一个Actor可以响应消息:推出新Actor,改变其内部状态,或将消息发送到一个或多个其他参与者。
　　4.Actor可能会堵塞自己,但Actor不应该堵塞它运行的线程。&lt;/p&gt;

&lt;p&gt;　　更多可见Actor模型专题&lt;/p&gt;

&lt;p&gt;Channel模型&lt;/p&gt;

&lt;p&gt;　　Channel模型中，worker之间不直接彼此联系，而是通过不同channel进行消息发布和侦听。消息的发送者和接收者之间通过Channel松耦合，发送者不知道自己消息被哪个接收者消费了，接收者也不知道是哪个发送者发送的消息。&lt;/p&gt;

&lt;p&gt;channel模型
&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/channel1.png&quot; alt=&quot;jupyter_use&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　Go语言的CSP模型是由协程Goroutine与通道Channel实现：&lt;/p&gt;

&lt;p&gt;Go协程goroutine: 是一种轻量线程，它不是操作系统的线程，而是将一个操作系统线程分段使用，通过调度器实现协作式调度。是一种绿色线程，微线程，它与Coroutine协程也有区别，能够在发现堵塞后启动新的微线程。
通道channel: 类似Unix的Pipe，用于协程之间通讯和同步。协程之间虽然解耦，但是它们和Channel有着耦合。&lt;/p&gt;

&lt;p&gt;Actor模型和CSP区别&lt;/p&gt;

&lt;p&gt;　　Actor模型和CSP区别图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://xiazemin.github.io/MyBlog/img/csp1.png&quot; alt=&quot;jupyter_use&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　Actor之间直接通讯，而CSP是通过Channel通讯，在耦合度上两者是有区别的，后者更加松耦合。&lt;/p&gt;

&lt;p&gt;　　同时，它们都是描述独立的进程通过消息传递进行通信。主要的区别在于：在CSP消息交换是同步的(即两个进程的执行”接触点”的，在此他们交换消息)，而Actor模型是完全解耦的，可以在任意的时间将消息发送给任何未经证实的接受者。由于Actor享有更大的相互独立,因为他可以根据自己的状态选择处理哪个传入消息。自主性更大些。&lt;/p&gt;

&lt;p&gt;　　在Go语言中为了不堵塞进程，程序员必须检查不同的传入消息，以便预见确保正确的顺序。CSP好处是Channel不需要缓冲消息，而Actor理论上需要一个无限大小的邮箱作为消息缓冲。&lt;/p&gt;

</description>
        <pubDate>Fri, 13 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/13/actor_csp.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/13/actor_csp.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>java_scala</title>
        <description>&lt;!-- more --&gt;

&lt;p&gt;Java似乎可以无缝操纵Scala语言中定义的类，在trait那一节中我们提到，如果trait中全部是抽象成员，则它与java中的interface是等同的，这时候java可以把它当作接口来使用，但如果trait中定义了具体成员，则它有着自己的内部实现，此时在java中使用的时候需要作相应的调整。&lt;/p&gt;

&lt;p&gt;Scala可以直接调用Java实现的任何类，只要符合scala语法就可以，不过某些方法在JAVA类中不存在，此时只要引入scala.collection.JavaConversions._包就可以了，它会我们自动地进行隐式转换，从而可以使用scala中的一些非常方便的高阶函数，如foreach方法,还可以显式地进行转换&lt;/p&gt;

&lt;p&gt;Java中的泛型可以直接转换成Scala中的泛型，在前面的课程中我们已经有所涉及，例如Java中的Comparator&lt;T&gt; 可以直接转换成 Scala中的Comparator[T] 使用方法完全一样，不同的只是语法上的。&lt;/T&gt;&lt;/p&gt;

&lt;p&gt;Scala中的异常处理是通过模式匹配来实现的&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/12/java_scala.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/12/java_scala.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Scala_collection</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Scala 集合分为可变的和不可变的集合。&lt;/p&gt;

&lt;p&gt;1	Scala List(列表)
List的特征是其元素以线性方式存储，集合中可以存放重复对象。
参考 API文档
2	Scala Set(集合)
Set是最简单的一种集合。集合中的对象不按特定的方式排序，并且没有重复对象。
参考 API文档
3	Scala Map(映射)
Map 是一种把键对象和值对象映射的集合，它的每一个元素都包含一对键对象和值对象。
参考 API文档
4	Scala 元组
元组是不同类型的值的集合
5	Scala Option
Option[T] 表示有可能包含值的容器，也可能不包含值。
6	Scala Iterator（迭代器）
迭代器不是一个容器，更确切的说是逐一访问容器内元素的方法。&lt;/p&gt;

&lt;p&gt;　Scala中的三种集合类型包含:Array,List,Tuple．那么究竟这三种有哪些异同呢？说实话，我之前一直没弄明确，所以今天特意花了点时间学习了一下．&lt;/p&gt;

&lt;p&gt;　　　　同样点:
　　　　　1.长度都是固定的，不可变长
　　　　　２.早期的Scala版本号,Array、List都不能混合类型，仅仅有Tuple能够,2.8版本号以后,3者的元素都能够混合不同的类型（转化为Any类型）&lt;/p&gt;

&lt;p&gt;　　　　不同点:
　　　　　1.Array 中的元素值可变，List和Tuple中的元素值不可变
　　　　　２.Array通常是先确定长度，后赋值，而List和Tuple在声明的时候就须要赋值
　　　　　３.Array取单个元素的效率非常高。而List读取单个元素的效率是O(n)
　　　　　4.List和Array的声明不须要newkeyword。而Tuple声明无论有无new 都能够&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      val arrayTest = Array(1,2,3,4)   //正确
      val arrayTest = Array(1,2,3,4)   //错误&amp;lt;span style=&quot;font-family: Arial, Helvetica, sans-serif;&quot;&amp;gt;  &amp;lt;/span&amp;gt;
      val listTest = List(1,2,3,4)         //正确
      val listTest = new List(1,2,3,4)    //错误

      val tupleTest = Tuple(1,2,&quot;aaa&quot;)        //正确
      val tupleTest = new Tuple(1,2,&quot;aaa&quot;)    //正确
      val tupleTest = (1,2,&quot;aaa&quot;)             //正确 　　 　　　　　5.当使用混合类型时，Array和List会将元素类型转化为Any类型,而Tuple则保留每个元素的初始类型

                6.訪问方式不同。Array和List的下标从0開始，且使用小括号,而Tuple的下标从1開始，切使用点加下划线的方式訪问，如：arrayTest(0), listTest(0); Tuple訪问: tupleTest._1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;List 有个叫“ ::: ” 的方法实现叠加功能。你可以这么用：&lt;/p&gt;

&lt;p&gt;val oneTwo = List(1, 2)
val threeFour = List(3, 4)&lt;/p&gt;

&lt;p&gt;val oneTwoThreeFour = oneTwo ::: threeFour&lt;/p&gt;

&lt;p&gt;List 最常用的操作符是发音为“ cons ” 的‘ :: 。 Cons 把一个新元素组合到已有 List 的最前端，然后返回结果 List 。例如，若执行这个脚本：
val twoThree = list(2, 3)
val oneTwoThree = 1 :: twoThree&lt;/p&gt;

&lt;p&gt;表达式“ 1 :: twoThree ” 中， :: 是它右操作数，列表 twoThree ，的方法。你或许会疑惑 :: 方法的关联性上有什么东西搞错了，不过这只是一个简单的需记住的规则：如果一个方法被用作操作符标注，如 a * b ，那么方法被左操作数调用，就像 a.*(b) 除非方法名以冒号结尾。这种情况下，方法被右操作数调用。因此， 1 :: twoThree 里， :: 方法被 twoThree 调用，传入 1 ，像这样： twoThree.::(1) 。&lt;/p&gt;

</description>
        <pubDate>Thu, 12 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/12/Scala_collection.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/12/Scala_collection.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>scala_list</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;一、常用操作符（操作符其实也是函数）&lt;/p&gt;

&lt;p&gt;++ ++&lt;a href=&quot;that: GenTraversableOnce[B]&quot;&gt;B&lt;/a&gt;: List[B] 从列表的尾部添加另外一个列表&lt;/p&gt;

&lt;p&gt;++: ++:&lt;a href=&quot;that: collection.Traversable[B]&quot;&gt;B &amp;gt;: A, That&lt;/a&gt;(implicit bf: CanBuildFrom[List[A], B, That]): That 在列表的头部添加一个列表&lt;/p&gt;

&lt;p&gt;+: +:(elem: A): List[A] 在列表的头部添加一个元素&lt;/p&gt;

&lt;p&gt;:+ :+(elem: A): List[A] 在列表的尾部添加一个元素&lt;/p&gt;

&lt;p&gt;:: ::(x: A): List[A] 在列表的头部添加一个元素&lt;/p&gt;

&lt;p&gt;::: :::(prefix: List[A]): List[A] 在列表的头部添加另外一个列表&lt;/p&gt;

&lt;p&gt;:\ :&lt;a href=&quot;z: B&quot;&gt;B&lt;/a&gt;(op: (A, B) ⇒ B): B 与foldRight等价&lt;/p&gt;

&lt;p&gt;val left = List(1,2,3)
val right = List(4,5,6)&lt;/p&gt;

&lt;p&gt;//以下操作等价
left ++ right   // List(1,2,3,4,5,6)
left ++: right  // List(1,2,3,4,5,6)
right.++:(left)    // List(1,2,3,4,5,6)
right.:::(left)  // List(1,2,3,4,5,6)&lt;/p&gt;

&lt;p&gt;//以下操作等价
0 +: left    //List(0,1,2,3)
left.+:(0)   //List(0,1,2,3)&lt;/p&gt;

&lt;p&gt;//以下操作等价
left :+ 4    //List(1,2,3,4)
left.:+(4)   //List(1,2,3,4)&lt;/p&gt;

&lt;p&gt;//以下操作等价
0 :: left      //List(0,1,2,3)
left.::(0)     //List(0,1,2,3)
看到这里大家应该跟我一样有一点晕吧，怎么这么多奇怪的操作符，这里给大家一个提示，任何以冒号结果的操作符，都是右绑定的，即 0 :: List(1,2,3) = List(1,2,3).::(0) = List(0,1,2,3) 从这里可以看出操作::其实是右边List的操作符，而非左边Int类型的操作符&lt;/p&gt;

&lt;p&gt;二、常用变换操作&lt;/p&gt;

&lt;p&gt;1.map&lt;/p&gt;

&lt;p&gt;map&lt;a href=&quot;f: (A) ⇒ B&quot;&gt;B&lt;/a&gt;: List[B]&lt;/p&gt;

&lt;p&gt;定义一个变换,把该变换应用到列表的每个元素中,原列表不变，返回一个新的列表数据&lt;/p&gt;

&lt;p&gt;Example1 平方变换&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3)
val square = (x: Int) =&amp;gt; x&lt;em&gt;x &lt;br /&gt;
val squareNums1 = nums.map(num =&amp;gt; num&lt;/em&gt;num)    //List(1,4,9)
val squareNums2 = nums.map(math.pow(_,2))    //List(1,4,9)
val squareNums3 = nums.map(square)            //List(1,4,9)&lt;/p&gt;

&lt;p&gt;Example2 保存文本数据中的某几列&lt;/p&gt;

&lt;p&gt;val text = List(“Homeway,25,Male”,”XSDYM,23,Female”)
val usersList = text.map(_.split(“,”)(0))  &lt;br /&gt;
val usersWithAgeList = text.map(line =&amp;gt; {
    val fields = line.split(“,”)
    val user = fields(0)
    val age = fields(1).toInt
    (user,age)
})
2.flatMap, flatten&lt;/p&gt;

&lt;p&gt;flatten: flatten[B]: List[B] 对列表的列表进行平坦化操作 flatMap: flatMap&lt;a href=&quot;f: (A) ⇒ GenTraversableOnce[B]&quot;&gt;B&lt;/a&gt;: List[B] map之后对结果进行flatten&lt;/p&gt;

&lt;p&gt;定义一个变换f, 把f应用列表的每个元素中，每个f返回一个列表，最终把所有列表连结起来。&lt;/p&gt;

&lt;p&gt;val text = List(“A,B,C”,”D,E,F”)
val textMapped = text.map(&lt;em&gt;.split(“,”).toList) // List(List(“A”,”B”,”C”),List(“D”,”E”,”F”))
val textFlattened = textMapped.flatten          // List(“A”,”B”,”C”,”D”,”E”,”F”)
val textFlatMapped = text.flatMap(&lt;/em&gt;.split(“,”).toList) // List(“A”,”B”,”C”,”D”,”E”,”F”)&lt;/p&gt;

&lt;p&gt;3.reduce&lt;/p&gt;

&lt;p&gt;reduce&lt;a href=&quot;op: (A1, A1) ⇒ A1&quot;&gt;A1 &amp;gt;: A&lt;/a&gt;: A1&lt;/p&gt;

&lt;p&gt;定义一个变换f, f把两个列表的元素合成一个，遍历列表，最终把列表合并成单一元素&lt;/p&gt;

&lt;p&gt;Example 列表求和&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3)
val sum1 = nums.reduce((a,b) =&amp;gt; a+b)   //6
val sum2 = nums.reduce(&lt;em&gt;+&lt;/em&gt;)            //6
val sum3 = nums.sum                 //6&lt;/p&gt;

&lt;p&gt;4.reduceLeft,reduceRight&lt;/p&gt;

&lt;p&gt;reduceLeft: reduceLeft&lt;a href=&quot;f: (B, A) ⇒ B&quot;&gt;B &amp;gt;: A&lt;/a&gt;: B&lt;/p&gt;

&lt;p&gt;reduceRight: reduceRight&lt;a href=&quot;op: (A, B) ⇒ B&quot;&gt;B &amp;gt;: A&lt;/a&gt;: B&lt;/p&gt;

&lt;p&gt;reduceLeft从列表的左边往右边应用reduce函数，reduceRight从列表的右边往左边应用reduce函数&lt;/p&gt;

&lt;p&gt;Example&lt;/p&gt;

&lt;p&gt;val nums = List(2.0,2.0,3.0)
val resultLeftReduce = nums.reduceLeft(math.pow)  // = pow( pow(2.0,2.0) , 3.0) = 64.0
val resultRightReduce = nums.reduceRight(math.pow) // = pow(2.0, pow(2.0,3.0)) = 256.0&lt;/p&gt;

&lt;p&gt;5.fold,foldLeft,foldRight&lt;/p&gt;

&lt;p&gt;fold: fold&lt;a href=&quot;z: A1&quot;&gt;A1 &amp;gt;: A&lt;/a&gt;(op: (A1, A1) ⇒ A1): A1 带有初始值的reduce,从一个初始值开始，从左向右将两个元素合并成一个，最终把列表合并成单一元素。&lt;/p&gt;

&lt;p&gt;foldLeft: foldLeft&lt;a href=&quot;z: B&quot;&gt;B&lt;/a&gt;(f: (B, A) ⇒ B): B 带有初始值的reduceLeft&lt;/p&gt;

&lt;p&gt;foldRight: foldRight&lt;a href=&quot;z: B&quot;&gt;B&lt;/a&gt;(op: (A, B) ⇒ B): B 带有初始值的reduceRight&lt;/p&gt;

&lt;p&gt;val nums = List(2,3,4)
val sum = nums.fold(1)(&lt;em&gt;+&lt;/em&gt;)  // = 1+2+3+4 = 9&lt;/p&gt;

&lt;p&gt;val nums = List(2.0,3.0)
val result1 = nums.foldLeft(4.0)(math.pow) // = pow(pow(4.0,2.0),3.0) = 4096
val result2 = nums.foldRight(1.0)(math.pow) // = pow(1.0,pow(2.0,3.0)) = 8.0&lt;/p&gt;

&lt;p&gt;6.sortBy,sortWith,sorted&lt;/p&gt;

&lt;p&gt;sortBy: sortBy&lt;a href=&quot;f: (A) ⇒ B&quot;&gt;B&lt;/a&gt;(implicit ord: math.Ordering[B]): List[A] 按照应用函数f之后产生的元素进行排序&lt;/p&gt;

&lt;p&gt;sorted： sorted&lt;a href=&quot;implicit ord: math.Ordering[B]&quot;&gt;B &amp;gt;: A&lt;/a&gt;: List[A] 按照元素自身进行排序&lt;/p&gt;

&lt;p&gt;sortWith： sortWith(lt: (A, A) ⇒ Boolean): List[A] 使用自定义的比较函数进行排序&lt;/p&gt;

&lt;p&gt;val nums = List(1,3,2,4)
val sorted = nums.sorted  //List(1,2,3,4)&lt;/p&gt;

&lt;p&gt;val users = List((“HomeWay”,25),(“XSDYM”,23))
val sortedByAge = users.sortBy{case(user,age) =&amp;gt; age}  //List((“XSDYM”,23),(“HomeWay”,25))
val sortedWith = users.sortWith{case(user1,user2) =&amp;gt; user1._2 &amp;lt; user2._2} //List((“XSDYM”,23),(“HomeWay”,25))&lt;/p&gt;

&lt;p&gt;7.filter, filterNot&lt;/p&gt;

&lt;p&gt;filter: filter(p: (A) ⇒ Boolean): List[A]&lt;/p&gt;

&lt;p&gt;filterNot: filterNot(p: (A) ⇒ Boolean): List[A]&lt;/p&gt;

&lt;p&gt;filter 保留列表中符合条件p的列表元素 ， filterNot，保留列表中不符合条件p的列表元素&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3,4)
val odd = nums.filter( _ % 2 != 0) // List(1,3)
val even = nums.filterNot( _ % 2 != 0) // List(2,4)&lt;/p&gt;

&lt;p&gt;8.count&lt;/p&gt;

&lt;p&gt;count(p: (A) ⇒ Boolean): Int&lt;/p&gt;

&lt;p&gt;计算列表中所有满足条件p的元素的个数，等价于 filter(p).length&lt;/p&gt;

&lt;p&gt;val nums = List(-1,-2,0,1,2) val plusCnt1 = nums.count( &amp;gt; 0) val plusCnt2 = nums.filter( &amp;gt; 0).length&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;diff, union, intersect&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;diff:diff(that: collection.Seq[A]): List[A] 保存列表中那些不在另外一个列表中的元素，即从集合中减去与另外一个集合的交集&lt;/p&gt;

&lt;p&gt;union : union(that: collection.Seq[A]): List[A] 与另外一个列表进行连结&lt;/p&gt;

&lt;p&gt;intersect: intersect(that: collection.Seq[A]): List[A] 与另外一个集合的交集&lt;/p&gt;

&lt;p&gt;val nums1 = List(1,2,3)
val nums2 = List(2,3,4)
val diff1 = nums1 diff nums2   // List(1)
val diff2 = nums2.diff(num1)   // List(4)
val union1 = nums1 union nums2  // List(1,2,3,2,3,4)
val union2 = nums2 ++ nums1        // List(2,3,4,1,2,3)
val intersection = nums1 intersect nums2  //List(2,3)&lt;/p&gt;

&lt;p&gt;10.distinct&lt;/p&gt;

&lt;p&gt;distinct: List[A] 保留列表中非重复的元素，相同的元素只会被保留一次&lt;/p&gt;

&lt;p&gt;val list = List(“A”,”B”,”C”,”A”,”B”) val distincted = list.distinct // List(“A”,”B”,”C”)
1
11.groupBy, grouped&lt;/p&gt;

&lt;p&gt;groupBy : groupBy&lt;a href=&quot;f: (A) ⇒ K&quot;&gt;K&lt;/a&gt;: Map[K, List[A]] 将列表进行分组，分组的依据是应用f在元素上后产生的新元素 
grouped: grouped(size: Int): Iterator[List[A]] 按列表按照固定的大小进行分组&lt;/p&gt;

&lt;p&gt;val data = List((“HomeWay”,”Male”),(“XSDYM”,”Femail”),(“Mr.Wang”,”Male”))
val group1 = data.groupBy(_._2) // = Map(“Male” -&amp;gt; List((“HomeWay”,”Male”),(“Mr.Wang”,”Male”)),”Female” -&amp;gt; List((“XSDYM”,”Femail”)))
val group2 = data.groupBy{case (name,sex) =&amp;gt; sex} // = Map(“Male” -&amp;gt; List((“HomeWay”,”Male”),(“Mr.Wang”,”Male”)),”Female” -&amp;gt; List((“XSDYM”,”Femail”)))
val fixSizeGroup = data.grouped(2).toList // = Map(“Male” -&amp;gt; List((“HomeWay”,”Male”),(“XSDYM”,”Femail”)),”Female” -&amp;gt; List((“Mr.Wang”,”Male”)))&lt;/p&gt;

&lt;p&gt;12.scan&lt;/p&gt;

&lt;p&gt;scan&lt;a href=&quot;z: B&quot;&gt;B &amp;gt;: A, That&lt;/a&gt;(op: (B, B) ⇒ B)(implicit cbf: CanBuildFrom[List[A], B, That]): That&lt;/p&gt;

&lt;p&gt;由一个初始值开始，从左向右，进行积累的op操作，这个比较难解释，具体的看例子吧。&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3)
val result = nums.scan(10)(&lt;em&gt;+&lt;/em&gt;)   // List(10,10+1,10+1+2,10+1+2+3) = List(10,11,12,13)&lt;/p&gt;

&lt;p&gt;13.scanLeft,scanRight&lt;/p&gt;

&lt;p&gt;scanLeft: scanLeft&lt;a href=&quot;z: B&quot;&gt;B, That&lt;/a&gt;(op: (B, A) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That&lt;/p&gt;

&lt;p&gt;scanRight: scanRight&lt;a href=&quot;z: B&quot;&gt;B, That&lt;/a&gt;(op: (A, B) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That&lt;/p&gt;

&lt;p&gt;scanLeft: 从左向右进行scan函数的操作，scanRight：从右向左进行scan函数的操作&lt;/p&gt;

&lt;p&gt;val nums = List(1.0,2.0,3.0)
val result = nums.scanLeft(2.0)(math.pow)   // List(2.0,pow(2.0,1.0), pow(pow(2.0,1.0),2.0),pow(pow(pow(2.0,1.0),2.0),3.0) = List(2.0,2.0,4.0,64.0)
val result = nums.scanRight(2.0)(math.pow)  // List(2.0,pow(3.0,2.0), pow(2.0,pow(3.0,2.0)), pow(1.0,pow(2.0,pow(3.0,2.0))) = List(1.0,512.0,9.0,2.0)&lt;/p&gt;

&lt;p&gt;14.take,takeRight,takeWhile&lt;/p&gt;

&lt;p&gt;take : takeRight(n: Int): List[A] 提取列表的前n个元素 takeRight: takeRight(n: Int): List[A] 提取列表的最后n个元素 takeWhile: takeWhile(p: (A) ⇒ Boolean): List[A] 从左向右提取列表的元素，直到条件p不成立&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,1,1,4,4,4,4)
val left = nums.take(4)   // List(1,1,1,1)
val right = nums.takeRight(4) // List(4,4,4,4)
val headNums = nums.takeWhile( _ == nums.head)  // List(1,1,1,1)&lt;/p&gt;

&lt;p&gt;15.drop,dropRight,dropWhile&lt;/p&gt;

&lt;p&gt;drop: drop(n: Int): List[A] 丢弃前n个元素，返回剩下的元素 dropRight: dropRight(n: Int): List[A] 丢弃最后n个元素，返回剩下的元素 dropWhile: dropWhile(p: (A) ⇒ Boolean): List[A] 从左向右丢弃元素，直到条件p不成立&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,1,1,4,4,4,4)
val left = nums.drop(4)   // List(4,4,4,4)
val right = nums.dropRight(4) // List(1,1,1,1)
val tailNums = nums.dropWhile( _ == nums.head)  // List(4,4,4,4)&lt;/p&gt;

&lt;p&gt;16.span, splitAt, partition&lt;/p&gt;

&lt;p&gt;span : span(p: (A) ⇒ Boolean): (List[A], List[A]) 从左向右应用条件p进行判断，直到条件p不成立，此时将列表分为两个列表&lt;/p&gt;

&lt;p&gt;splitAt: splitAt(n: Int): (List[A], List[A]) 将列表分为前n个，与，剩下的部分&lt;/p&gt;

&lt;p&gt;partition: partition(p: (A) ⇒ Boolean): (List[A], List[A]) 将列表分为两部分，第一部分为满足条件p的元素，第二部分为不满足条件p的元素&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,1,2,3,2,1)
val (prefix,suffix) = nums.span( _ == 1) // prefix = List(1,1,1), suffix = List(2,3,2,1)
val (prefix,suffix) = nums.splitAt(3)  // prefix = List(1,1,1), suffix = List(2,3,2,1)
val (prefix,suffix) = nums.partition( _ == 1) // prefix = List(1,1,1,1), suffix = List(2,3,2)&lt;/p&gt;

&lt;p&gt;17.padTo&lt;/p&gt;

&lt;p&gt;padTo(len: Int, elem: A): List[A]&lt;/p&gt;

&lt;p&gt;将列表扩展到指定长度，长度不够的时候，使用elem进行填充，否则不做任何操作。&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,1)
 val padded = nums.padTo(6,2)   // List(1,1,1,2,2,2)&lt;/p&gt;

&lt;p&gt;18.combinations,permutations&lt;/p&gt;

&lt;p&gt;combinations: combinations(n: Int): Iterator[List[A]] 取列表中的n个元素进行组合，返回不重复的组合列表，结果一个迭代器&lt;/p&gt;

&lt;p&gt;permutations: permutations: Iterator[List[A]] 对列表中的元素进行排列，返回不重得的排列列表，结果是一个迭代器&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,3)
val combinations = nums.combinations(2).toList //List(List(1,1),List(1,3))
val permutations = nums.permutations.toList        // List(List(1,1,3),List(1,3,1),List(3,1,1))&lt;/p&gt;

&lt;p&gt;19.zip, zipAll, zipWithIndex, unzip,unzip3&lt;/p&gt;

&lt;p&gt;zip: zip&lt;a href=&quot;that: GenIterable[B]&quot;&gt;B&lt;/a&gt;: List[(A, B)] 与另外一个列表进行拉链操作，将对应位置的元素组成一个pair，返回的列表长度为两个列表中短的那个&lt;/p&gt;

&lt;p&gt;zipAll: zipAll&lt;a href=&quot;that: collection.Iterable[B], thisElem: A, thatElem: B&quot;&gt;B&lt;/a&gt;: List[(A, B)] 与另外一个列表进行拉链操作，将对应位置的元素组成一个pair，若列表长度不一致，自身列表比较短的话使用thisElem进行填充，对方列表较短的话使用thatElem进行填充&lt;/p&gt;

&lt;p&gt;zipWithIndex：zipWithIndex: List[(A, Int)] 将列表元素与其索引进行拉链操作，组成一个pair&lt;/p&gt;

&lt;p&gt;unzip: unzip&lt;a href=&quot;implicit asPair: (A) ⇒ (A1, A2)&quot;&gt;A1, A2&lt;/a&gt;: (List[A1], List[A2]) 解开拉链操作&lt;/p&gt;

&lt;p&gt;unzip3: unzip3&lt;a href=&quot;implicit asTriple: (A) ⇒ (A1, A2, A3)&quot;&gt;A1, A2, A3&lt;/a&gt;: (List[A1], List[A2], List[A3]) 3个元素的解拉链操作&lt;/p&gt;

&lt;p&gt;val alphabet = List(“A”,B”,”C”)
val nums = List(1,2)
val zipped = alphabet zip nums   // List((“A”,1),(“B”,2))
val zippedAll = alphabet.zipAll(nums,”*”,-1)   // List((“A”,1),(“B”,2),(“C”,-1))
val zippedIndex = alphabet.zipWithIndex  // List((“A”,0),(“B”,1),(“C”,3))
val (list1,list2) = zipped.unzip        // list1 = List(“A”,”B”), list2 = List(1,2)
val (l1,l2,l3) = List((1, “one”, ‘1’),(2, “two”, ‘2’),(3, “three”, ‘3’)).unzip3   // l1=List(1,2,3),l2=List(“one”,”two”,”three”),l3=List(‘1’,’2’,’3’)&lt;/p&gt;

&lt;p&gt;20.slice&lt;/p&gt;

&lt;p&gt;slice(from: Int, until: Int): List[A] 提取列表中从位置from到位置until(不含该位置)的元素列表&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3,4,5)
val sliced = nums.slice(2,4)  //List(3,4)&lt;/p&gt;

&lt;p&gt;21.sliding&lt;/p&gt;

&lt;p&gt;sliding(size: Int, step: Int): Iterator[List[A]] 将列表按照固定大小size进行分组，步进为step，step默认为1,返回结果为迭代器&lt;/p&gt;

&lt;p&gt;val nums = List(1,1,2,2,3,3,4,4)
val groupStep2 = nums.sliding(2,2).toList  //List(List(1,1),List(2,2),List(3,3),List(4,4))
val groupStep1 = nums.sliding(2).toList //List(List(1,1),List(1,2),List(2,2),List(2,3),List(3,3),List(3,4),List(4,4))&lt;/p&gt;

&lt;p&gt;22.updated&lt;/p&gt;

&lt;p&gt;updated(index: Int, elem: A): List[A] 对列表中的某个元素进行更新操作&lt;/p&gt;

&lt;p&gt;val nums = List(1,2,3,3)
val fixed = nums.updated(3,4)  // List(1,2,3,4)&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/11/scala_list.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/11/scala_list.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark-session-context</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;初始化Spark
一个Spark程序首先必须要做的是创建一个SparkContext对象，这个对象告诉Spark如何访问一个集群。为了创建一个SparkContext，你首先需要构建一个包含了关于你的应用的信息的SparkConf对象。&lt;/p&gt;

&lt;p&gt;只有一个SparkContext可以激活一个JVM。你必须在创建一个新的SparkContext之前stop()掉这个激活的SparkContext。&lt;/p&gt;

&lt;p&gt;val conf = new SparkConf().setAppName(appName).setMaster(master)
new SparkContext(conf)
参数appName是在集群UI上展示的你的应用的名字。master是一个Spark，Mesos或者YARN集群的URL，或者是表示以本地模式运行的一个特定的字符串”local”。在实践中，当运行在集群上时，你是不会想在程序中硬编码master的，而是通过spark-submit启动应用并指定master的。然而，对于本地测试和单元测试，你可以通过传递”local”来在进程内运行Spark。&lt;/p&gt;

&lt;p&gt;使用shell
在Spark shell中，一个特殊的SparkContext已经为你创建好了，变量名是sc。如果再创建你自己的SparkContext就不起作用了。你可以使用参数–master来设置context连接到哪个master，还可以通过给参数–jars传递逗号分隔的列表来给classpath增加JARs。你还可以通过给参数–packages提供逗号分隔的Maven坐标来给你的shell会话增加依赖（例如Spark包）。任何可能存在依赖的附加库（例如Sonatype）都可以被传递给参数–repositories&lt;/p&gt;

&lt;p&gt;　在Spark的早期版本，sparkContext是进入Spark的切入点。我们都知道RDD是Spark中重要的API，然而它的创建和操作得使用sparkContext提供的API；对于RDD之外的其他东西，我们需要使用其他的Context。比如对于流处理来说，我们得使用StreamingContext；对于SQL得使用sqlContext；而对于hive得使用HiveContext。然而DataSet和Dataframe提供的API逐渐称为新的标准API，我们需要一个切入点来构建它们，所以在 Spark 2.0中我们引入了一个新的切入点(entry point)：SparkSession&lt;/p&gt;

&lt;p&gt;　　SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。&lt;/p&gt;

&lt;p&gt;创建SparkSession&lt;/p&gt;

&lt;p&gt;　　SparkSession的设计遵循了工厂设计模式（factory design pattern），下面代码片段介绍如何创建SparkSession
[python] view plain copy
val sparkSession = SparkSession.builder.&lt;br /&gt;
      master(“local”)&lt;br /&gt;
      .appName(“spark session example”)&lt;br /&gt;
      .getOrCreate()&lt;br /&gt;
上面代码类似于创建一个SparkContext，master设置为local，然后创建了一个SQLContext封装它。如果你想创建hiveContext，可以使用下面的方法来创建SparkSession，以使得它支持Hive：
[python] view plain copy
val sparkSession = SparkSession.builder.&lt;br /&gt;
      master(“local”)&lt;br /&gt;
      .appName(“spark session example”)&lt;br /&gt;
      .enableHiveSupport()&lt;br /&gt;
      .getOrCreate()&lt;br /&gt;
enableHiveSupport 函数的调用使得SparkSession支持hive，类似于HiveContext。&lt;/p&gt;

&lt;p&gt;spark2.0 主要变化
1 更容易的SQL和Streamlined APIs&lt;/p&gt;

&lt;p&gt;Spark 2.0主要聚焦于两个方面：（1）、对标准的SQL支持（2）、统一DataFrame和Dataset API。&lt;/p&gt;

&lt;p&gt;　　在SQL方面，Spark 2.0已经显著地扩大了它的SQL功能，比如引进了一个新的ANSI SQL解析器和对子查询的支持。现在Spark 2.0已经可以运行TPC-DS所有的99个查询，这99个查询需要SQL 2003的许多特性。因为SQL是Spark应用程序的主要接口之一，Spark 2.0 SQL的扩展大幅减少了应用程序往Spark迁移的代价。&lt;/p&gt;

&lt;p&gt;　　在编程API方面，我们对API进行了精简。&lt;/p&gt;

&lt;p&gt;　　1、统一Scala和Java中DataFrames和Datasets的API：从Spark 2.0开始，DataFrame仅仅是Dataset的一个别名。有类型的方法(typed methods)（比如：map, filter, groupByKey）和无类型的方法(untyped methods)(比如：select, groupBy)目前在Dataset类上可用。同样，新的Dataset接口也在Structured Streaming中使用。因为编译时类型安全(compile-time type-safety)在Python和R中并不是语言特性，所以Dataset的概念并不在这些语言中提供相应的API。而DataFrame仍然作为这些语言的主要编程抽象。&lt;/p&gt;

&lt;p&gt;　　2、SparkSession：一个新的切入点，用于替代旧的SQLContext和HiveContext。对于那些使用DataFrame API的用户，一个常见的困惑就是我们正在使用哪个context？现在我们可以使用SparkSession了，其涵括了SQLContext和HiveContext，仅仅提供一个切入点。需要注意的是为了向后兼容，旧的SQLContext和HiveContext目前仍然可以使用。&lt;/p&gt;

&lt;p&gt;　　3、简单以及性能更好的Accumulator API：Spark 2.0中设计出一种新的Accumulator API，它拥有更加简洁的类型层次，而且支持基本类型。为了向后兼容，旧的Accumulator API仍然可以使用。&lt;/p&gt;

&lt;p&gt;　　4、基于DataFrame的Machine Learning API可以作为主要的ML API了：在Spark 2.0中， spark.ml包以其pipeline API将会作为主要的机器学习API了，而之前的spark.mllib仍然会保存，将来的开发会聚集在基于DataFrame的API上。&lt;/p&gt;

&lt;p&gt;　　5、Machine learning pipeline持久化：现在用户可以保存和加载Spark支持所有语言的Machine learning pipeline和models。&lt;/p&gt;

&lt;p&gt;　　6、R的分布式算法：在R语言中添加支持了Generalized Linear Models (GLM), Naive Bayes, Survival Regression, and K-Means。
　　
2 更快：Spark作为编译器&lt;/p&gt;

&lt;p&gt;Spark 2.0中附带了第二代Tungsten engine，这一代引擎是建立在现代编译器和MPP数据库的想法上，并且把它们应用于数据的处理过程中。主要想法是通过在运行期间优化那些拖慢整个查询的代码到一个单独的函数中，消除虚拟函数的调用以及利用CPU寄存器来存放那些中间数据。我们把这些技术称为”整段代码生成”(whole-stage code generation)。&lt;/p&gt;

&lt;p&gt;3 更加智能：Structured Streaming&lt;/p&gt;

</description>
        <pubDate>Fri, 06 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/10/06/spark-session-context.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/10/06/spark-session-context.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>scala_main_class</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;解决
错误: 找不到或无法加载主类 SparkTest.SparkSessionTest&lt;/p&gt;

&lt;p&gt;project-&amp;gt;buildin path-&amp;gt; configure build in path 
-&amp;gt;scala compiler -&amp;gt;use project settings -&amp;gt;scala installation
-&amp;gt; dixed scala installation 2.11.8 built in&lt;/p&gt;
</description>
        <pubDate>Fri, 29 Sep 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/09/29/scala_main_class.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/09/29/scala_main_class.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>alibaba_fast_json</title>
        <description>&lt;!-- more --&gt;
&lt;p&gt;Fastjson是一个Java语言编写的高性能功能完善的JSON库。将解析json的性能提升到极致，是目前Java语言中最快的JSON库。Fastjson接口简单易用，已经被广泛使用在缓存序列化、协议交互、Web输出、Android客户端等多种应用场景。&lt;/p&gt;

&lt;p&gt;GitHub下载地址: 
https://github.com/alibaba/fastjson&lt;/p&gt;

&lt;p&gt;最新发布版本jar包 1.2.23 下载地址: https://search.maven.org/remote_content?g=com.alibaba&amp;amp;a=fastjson&amp;amp;v=LATEST&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;lineno&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;com.alibaba.fastjson.JSON&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2 &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FastJsonExp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3 &lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]){&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4 &lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{\&amp;quot;name\&amp;quot;:\&amp;quot;chenggang\&amp;quot;,\&amp;quot;age\&amp;quot;:24}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5 &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//反序列化&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6 &lt;/span&gt; &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;userInfo&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parseObject&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;7 &lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;name:&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;, age:&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;8 &lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;9 &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
</description>
        <pubDate>Fri, 29 Sep 2017 00:00:00 +0800</pubDate>
        <link>https://xiazemin.github.io/MyBlog/spark/2017/09/29/alibaba_fast_json.html</link>
        <guid isPermaLink="true">https://xiazemin.github.io/MyBlog/spark/2017/09/29/alibaba_fast_json.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
  </channel>
</rss>
